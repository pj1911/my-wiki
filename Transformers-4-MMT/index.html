
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../Transformers-3-LLMs/">
      
      
        <link rel="next" href="../Fuzzy%20Inference%20Systems/">
      
      
        
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.1">
    
    
      
        <title>4 – Multimodal Transformers - Prajwal's  Wiki</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.484c7ddc.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../css/justify.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#multimodal-transformers" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Prajwal&#39;s  Wiki" class="md-header__button md-logo" aria-label="Prajwal's  Wiki" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Prajwal's  Wiki
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              4 – Multimodal Transformers
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Prajwal&#39;s  Wiki" class="md-nav__button md-logo" aria-label="Prajwal's  Wiki" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Prajwal's  Wiki
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Home
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Machine Learning
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Machine Learning
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../linearRegression/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Linear Regression
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Generative%20Adversarial%20Networks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Generative Adversarial Networks
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_3" checked>
        
          
          <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Transformers
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Transformers
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Transformers-1-Introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    1 – Introduction
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Transformers-2-NLP/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    2 – Natural Language Processing
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Transformers-3-LLMs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    3 – Transformer Language Models
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    4 – Multimodal Transformers
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    4 – Multimodal Transformers
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    <span class="md-ellipsis">
      
        Introduction
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vision-transformers" class="md-nav__link">
    <span class="md-ellipsis">
      
        Vision transformers
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#generative-image-transformers" class="md-nav__link">
    <span class="md-ellipsis">
      
        Generative image transformers
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Generative image transformers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vector-quantization" class="md-nav__link">
    <span class="md-ellipsis">
      
        Vector quantization
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#audio-data" class="md-nav__link">
    <span class="md-ellipsis">
      
        Audio data
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#text-to-speech" class="md-nav__link">
    <span class="md-ellipsis">
      
        Text-to-speech
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vision-and-language-transformers" class="md-nav__link">
    <span class="md-ellipsis">
      
        Vision and language transformers
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    <span class="md-ellipsis">
      
        References
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Mathematics
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Mathematics
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Fuzzy%20Inference%20Systems/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Fuzzy Inference Systems
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Neural network methods for partial differential equations
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Neural network methods for partial differential equations
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Hamilton-Jacobi%20equations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    1 - Hamilton Jacobi Equation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Hyperbolic%20PDEs-1-weak%20solutions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    2 - Weak solutions
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Hyperbolic%20PDEs-2-Conservation%20laws%20and%20entropy%20form/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    3 - Entropy Stable and Hyperbolic Solutions
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Reinforcement Learning
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    Reinforcement Learning
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../RL-1-Introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    1 - Introduction
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../RL-2-Markov%20Decision%20Processes/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    2 - Markov Decision Processes
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../RL-3-Partially%20Observable%20MDP/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    3 - Partially Observable MDP
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../RL-4-Planning%20with%20Dynamic%20Programming/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    4 - Planning with Dynamic Programming
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../RL-5-Model%20free%20prediction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    5 - Model Free Prediction
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../RL-6-Model%20free%20control/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    6 - Model Free Control
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../RL-7-Value%20function%20approximation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    7 - Value Function Approximation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    <span class="md-ellipsis">
      
        Introduction
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vision-transformers" class="md-nav__link">
    <span class="md-ellipsis">
      
        Vision transformers
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#generative-image-transformers" class="md-nav__link">
    <span class="md-ellipsis">
      
        Generative image transformers
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Generative image transformers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vector-quantization" class="md-nav__link">
    <span class="md-ellipsis">
      
        Vector quantization
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#audio-data" class="md-nav__link">
    <span class="md-ellipsis">
      
        Audio data
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#text-to-speech" class="md-nav__link">
    <span class="md-ellipsis">
      
        Text-to-speech
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vision-and-language-transformers" class="md-nav__link">
    <span class="md-ellipsis">
      
        Vision and language transformers
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    <span class="md-ellipsis">
      
        References
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="multimodal-transformers">Multimodal Transformers</h1>
<h2 id="introduction">Introduction</h2>
<p>Transformers were first introduced as an alternative to recurrent networks for handling sequential language data. Today, they are used across almost all areas of deep learning.</p>
<p>A key reason is that transformers are very general—they make only weak assumptions about the structure of the input. This contrasts with convolutional networks, which strongly assume that important patterns are local and behave similarly when shifted across the input (equivariance and locality).</p>
<ul>
<li><strong>Locality</strong> means that each neuron in a convolutional layer only looks at a small neighborhood (a local patch) of the input, not the whole image or signal at once. We are building in the belief that useful features like edges, corners, or textures can be detected from nearby pixels.</li>
<li><strong>Equivariance</strong> means that if we shift the input (for example, move an object a few pixels to the left), the feature maps produced by the convolution also shift in the same way. The pattern is recognized no matter where it appears, and the response simply moves along with it. This is a very strong built-in assumption about how patterns behave across space, and it is one of the reasons convolutions work so well on images but are less general than transformers.</li>
</ul>
<p>Because of this generality, transformers have become state-of-the-art on many data types, including text, images, video, point clouds, and audio. Within each of these domains, they are used for both discriminative tasks (such as classification) and generative tasks (such as synthesis). Interestingly, the basic transformer layer architecture has stayed almost the same over time and across applications. Most of the innovation needed to move from pure language to other domains has focused instead on how we <em>represent and encode</em> the inputs and outputs so that the transformer can work with them.</p>
<p>Having a single architecture that can process many kinds of data also makes <em>multimodal</em> applications much easier. Here, “multimodal” means we combine two or more types of data in the inputs, the outputs, or both. For example, we can generate an image from a text prompt, or we can build a robot that fuses information from cameras, radar, and microphones.</p>
<p>The key takeaway is simple: if we can tokenize the inputs and later decode the output tokens back into a useful form, there is a good chance that a transformer can be applied.</p>
<h2 id="vision-transformers">Vision transformers</h2>
<p>Transformers also work very well for vision and now reach state-of-the-art results on many image tasks. When we use a standard transformer encoder on images, we call it a <em>vision transformer</em> (ViT). To use a transformer, we must first turn an image into a sequence of tokens. The simplest idea is to treat each pixel as one token after a linear projection, but this is usually impossible in practice because in this case the memory cost of a transformer grows roughly with the <em>square</em> of the number of tokens/pixels. Instead, ViTs almost always use <em>patch tokens</em>. Assume an image</p>
<div class="arithmatex">\[
\mathbf{x} \in \mathbb{R}^{H \times W \times C},
\]</div>
<p>where <span class="arithmatex">\(H\)</span> and <span class="arithmatex">\(W\)</span> are height and width in pixels and <span class="arithmatex">\(C\)</span> is the number of channels (typically <span class="arithmatex">\(C=3\)</span> for RGB). In a ViT we make a different design choice: <em>one token = one image patch</em>. So pixels are no longer tokens. To achieve this we cut the image into non-overlapping <span class="arithmatex">\(P \times P\)</span> patches (e.g. <span class="arithmatex">\(P = 16\)</span>). Each patch contains <span class="arithmatex">\(P \times P \times C\)</span> pixel values, which we reshape into a vector of length <span class="arithmatex">\(P^{2}C\)</span>. This raw patch vector is then mapped, via a linear layer, to a <span class="arithmatex">\(D\)</span>-dimensional embedding, that <span class="arithmatex">\(D\)</span>-dimensional vector is <em>one token</em>. Doing this for all patches gives</p>
<div class="arithmatex">\[
\mathbf{x}_p \in \mathbb{R}^{N \times (P^{2} C)},
\]</div>
<p>before the linear projection, where</p>
<div class="arithmatex">\[
N = \frac{HWC}{P^{2}C}
= \frac{HW}{P^{2}}
\]</div>
<p>is the number of patches, and therefore the number of tokens.
Each patch (and thus each token) contains <span class="arithmatex">\(P^{2}\)</span> pixels in space and <span class="arithmatex">\(P^{2}C\)</span> pixel values in total (because there are <span class="arithmatex">\(C\)</span> channels). After projection, each token is a <span class="arithmatex">\(D\)</span>-dimensional vector fed into the transformer.</p>
<p>Another way to create tokens is to first pass the image through a small convolutional neural network (CNN). The CNN down-samples the spatial resolution, and we then treat each spatial location of the final feature map as one token. For example, a typical ResNet18 encoder reduces height and width each by a factor of <span class="arithmatex">\(8\)</span>, so we obtain <span class="arithmatex">\(64\)</span> times fewer tokens than raw pixels.</p>
<p><strong>Positional embeddings.</strong> We also need to encode where each patch comes from. One option is to build explicit 2D positional embeddings that represent the <span class="arithmatex">\((x,y)\)</span> location of each patch. In practice, this rarely helps compared to simply learning a positional embedding vector per token index, so learned positional embeddings are more common. Unlike NLP transformers, vision transformers usually assume a fixed number of tokens (for example, always <span class="arithmatex">\(14 \times 14\)</span> patches), so these learned embeddings do not need to generalize to different input lengths or image sizes.</p>
<p>Architecturally, a ViT is very different from a CNN. CNNs have strong built-in inductive biases: weight sharing, locality, and approximate translation equivariance. In a ViT, the only real inductive bias is the decision to slice the image into patches, everything else about image geometry must be learned from data. As a result, ViTs typically need more training data than comparable CNNs. The upside is that, because they do not hard-code many assumptions about the input structure, transformers can often reach higher accuracy once enough data and compute are available. This nicely illustrates the trade-off between strong inductive biases and the amount of training data.</p>
<h2 id="generative-image-transformers">Generative image transformers</h2>
<p>In language, transformers shine when used as <em>autoregressive</em> generators: they
predict the next token given all previous ones and can synthesize long texts. A
natural question is whether we can do the same for images. Language is intrinsically sequential, so an autoregressive ordering is obvious.
Images, in contrast, have no natural pixel order. Mathematically, however, any
joint distribution over variables <span class="arithmatex">\(\mathbf{x}_1,\dots,\mathbf{x}_N\)</span> can be written
as a product of conditionals once we pick <em>some</em> ordering:</p>
<div class="arithmatex">\[
p(\mathbf{x}_1,\dots,\mathbf{x}_N)
= \prod_{n=1}^N p(\mathbf{x}_n \mid \mathbf{x}_1,\dots,\mathbf{x}_{n-1})
\]</div>
<p>This factorization is completely general and does not restrict the form of the
conditionals <span class="arithmatex">\(p(\mathbf{x}_n \mid \mathbf{x}_1,\dots,\mathbf{x}_{n-1})\)</span>.</p>
<p>For images, lets assume we can choose <span class="arithmatex">\(\mathbf{x}_n\)</span> to be the RGB vector of the <span class="arithmatex">\(n\)</span>-th pixel.
We then need an ordering of pixels. A common choice is a <em>raster scan</em>
(left-to-right, top-to-bottom). Generating an image autoregressively means
sampling each pixel in this raster scan order using
the above equation. Autoregressive image models existed well before transformers. PixelCNN and
PixelRNN, for example, used specially masked convolutions so that each pixel only
depends on earlier pixels in the raster order.</p>
<p>Real-valued (continuous) image representations work very well for <em>discriminative</em> tasks such as
classification: a CNN can map real-valued pixels to real-valued features and then to class scores with no
problem. For <em>generation</em>, however, if we model the conditionals with continuous distributions such as
Gaussians and train by maximum likelihood, the model is encouraged to predict the <em>average</em> of all
plausible pixel values, which often leads to smooth, blurry images. Discrete representations avoid this by
treating each pixel or patch as choosing from a finite set of codes and modelling a categorical distribution
(over these codes) with a softmax. In this case a conditional
<span class="arithmatex">\(p(\mathbf{x}_n \mid \mathbf{x}_1,\dots,\mathbf{x}_{n-1})\)</span> can assign high probability to both “black” and
“white” for a pixel, rather than collapsing to “grey”, so sharp, multimodal structure is captured much
more naturally and samples are typically higher quality.</p>
<p>However, working directly with <em>discrete pixels</em> is still difficult. A single
colour pixel has 8 bits for each of the three RGB channels, so each channel can
take <span class="arithmatex">\(2^8 = 256\)</span> values. The total number of possible colours per pixel is
<span class="arithmatex">\(256^3 = 2^{24} \approx 16\text{M}\)</span>, so using a separate softmax over all options for
every pixel is computationally impractical.
A popular fix is
<em>vector quantization</em>, which we can view as learned compression.</p>
<h3 id="vector-quantization">Vector quantization</h3>
<p>Assume our dataset can be written as a matrix
<span class="arithmatex">\(\mathbf{X} \in \mathbb{R}^{N \times D}\)</span>, where each row is a data vector
<span class="arithmatex">\(\mathbf{x}_1,\dots,\mathbf{x}_N \in \mathbb{R}^D\)</span> (e.g. pixels). We also have
a set of <span class="arithmatex">\(K\)</span> codebook vectors
<span class="arithmatex">\(\mathcal{C} = \{\mathbf{c}_1,\dots,\mathbf{c}_K\} \subset \mathbb{R}^D\)</span>,
with <span class="arithmatex">\(K \ll D\)</span>. We approximate each data vector by its nearest codebook vector,
usually in Euclidean distance:</p>
<div class="arithmatex">\[
\mathbf{x}_n \;\rightarrow\;
\arg\min_{\mathbf{c}_k \in \mathcal{C}} \|\mathbf{x}_n - \mathbf{c}_k\|^2
\]</div>
<p>Because there are only <span class="arithmatex">\(K\)</span> codebook vectors, we can represent each
<span class="arithmatex">\(\mathbf{x}_n\)</span> by a <span class="arithmatex">\(K\)</span>-dimensional one-hot code, so the whole dataset becomes
a matrix of codes <span class="arithmatex">\(\mathbf{Z} \in \{0,1\}^{N \times K}\)</span> (or, equivalently, an
index vector in <span class="arithmatex">\(\{1,\dots,K\}^N\)</span>). By choosing <span class="arithmatex">\(K\)</span>, we control a trade-off:
larger <span class="arithmatex">\(K\)</span> gives a more faithful representation, smaller <span class="arithmatex">\(K\)</span> gives stronger
compression. We can now map all pixels into this lower-dimensional codebook
space, train an autoregressive transformer to generate sequences of code
indices, and finally map these indices back to image pixels by replacing index
<span class="arithmatex">\(k\)</span> with its codebook vector <span class="arithmatex">\(\mathbf{c}_k\)</span>. This reconstruction is only
approximate: we generally cannot recover the exact original <span class="arithmatex">\(\mathbf{x}_n\)</span>, only
its nearest codebook vector <span class="arithmatex">\(\mathbf{c}_k\)</span>. In practice, we pick <span class="arithmatex">\(K\)</span> (and learn
the codebook) so that this quantization error is small enough that the generated
images still look sharp and realistic.</p>
<p><strong>ImageGPT</strong> was one of the first autoregressive transformers for images. It clusters
the colour space with <span class="arithmatex">\(K\)</span>-means and treats each pixel as belonging to one of the
resulting <span class="arithmatex">\(K\)</span> RGB codebook vectors. The one-hot codes act as discrete tokens,
just like words in language models, and the transformer is trained with a
next-token prediction loss. This objective gives strong image representations
that can be fine-tuned for downstream tasks, mirroring language modelling.</p>
<p>As in vision transformers, it is more efficient to use <em>patches</em> as tokens.
Fewer tokens make high-resolution images feasible. We still want discrete tokens to capture multimodal conditionals, but now the
<em>dimensionality explodes</em>: the number of possible patches grows
exponentially with the number of pixels in a patch. If we take <span class="arithmatex">\(16\times 16\)</span>
patches with just two pixel values (black/white), there are</p>
<div class="arithmatex">\[
16 \times 16 = 256 \quad\Rightarrow\quad \text{number of patches}
= 2^{256} \approx 1.16 \times 10^{77},
\]</div>
<p>since each of the <span class="arithmatex">\(256\)</span> pixels can independently be black or white.</p>
<p>So we again turn to vector quantization, now applied to patches. We learn a
codebook of patch vectors from data, using methods like <span class="arithmatex">\(K\)</span>-means, fully
convolutional networks, or even vision transformers. A difficulty is that the
quantization step (the nearest-codebook lookup) is not differentiable. In
practice, we use the <em>straight-through estimator</em>: during backpropagation we
simply copy gradients through the non-differentiable step as if it were the
identity function. Finally, the same idea extends naturally from images to videos. We treat a video
as one long sequence of vector-quantized tokens (over space and time) and train
an autoregressive transformer over this sequence to generate videos frame by
frame.</p>
<h2 id="audio-data">Audio data</h2>
<p>Transformers can also process audio. Raw sound is usually stored as a
<em>waveform</em>: a sequence of air-pressure samples over time. Instead of using
this directly, we usually convert it to a <em>mel spectrogram</em>, a matrix whose
columns are time steps and rows are frequency bands on the mel scale, designed
so equal steps roughly match equal perceived pitch changes.</p>
<p>A core task is audio classification, where short clips are assigned labels such
as <em>car</em>, <em>animal</em>, or <em>laughter</em>. A common benchmark is the
<em>AudioSet</em> dataset. Before transformers, the best systems treated mel
spectrograms as images and used CNNs. CNNs capture local patterns well but
struggle with long-range temporal dependencies, which often matter for audio.</p>
<p>Now, transformers are increasingly used instead. A transformer encoder with the
same architecture as in language or vision can classify audio. We view the mel
spectrogram as an image, split it into patches (optionally overlapping), and
flatten each patch into a 1D vector. Each patch
becomes a token, we add positional encodings, prepend a special <code>&lt;class&gt;</code>
token, and feed all tokens to the transformer encoder. The final output at the
<code>&lt;class&gt;</code> position goes through a linear layer and a softmax, and the whole
model is trained end-to-end with a cross-entropy loss.</p>
<h2 id="text-to-speech">Text-to-speech</h2>
<p>Classification is only one success story for transformers in audio. Another is
<em>text-to-speech</em> (TTS): generating spoken audio that follows a given text,
often in the voice of a specific speaker. In a traditional TTS system, we record many examples of a single speaker and
train a supervised regression model to map text to a low-level representation of
speech, such as a mel spectrogram. At inference time, we feed in new text, get a
predicted spectrogram, and convert it deterministically back to a waveform.</p>
<p>This setup has several drawbacks. If we predict very low-level units (for
example, phonemes), the model must handle long-range context to make sentences
sound natural. If we instead predict longer segments, the input space becomes
huge and requires impractically large datasets. The approach also does not share
knowledge across speakers, so each new voice needs a lot of data. Finally, TTS
is inherently a generative problem: many different speech signals are valid for
the same text and speaker, while regression tends to average them into bland,
less expressive outputs.</p>
<p>A more modern view treats speech like language and frames TTS as
<em>conditional language modelling</em>. We still use transformers, but now the
model predicts the next audio token given previous audio tokens and the input
text. The main design questions become: (1) how to tokenize speech so we can
decode predictions back to audio, and (2) how to condition the model on the
desired speaker’s voice.</p>
<p>First, speech is converted into a sequence of <em>speech tokens</em> using vector
quantization. We learn a codebook (dictionary) of audio embeddings, split a
waveform into short frames, and replace each frame by the index of its nearest
codebook vector. During training, the transformer input is a single sequence
built by concatenating (i) the text tokens for the sentence we want to speak and
(ii) a short run of speech tokens taken from a separate sample of the same
speaker. The model is trained to output the speech tokens that correspond to the
full spoken version of the input text. Intuitively, the text tokens tell the
model <em>what</em> to say, while the conditioning speech tokens tell it <em>in
which voice</em> to say it.</p>
<p>At test time, we provide new text plus a brief speech sample from a new
speaker. The model generates speech tokens conditioned on both the text and this
speaker snippet. Finally, we map the tokens back through the same codebook to
produce a waveform. This lets the system read out arbitrary text in the voice of
a speaker it has only heard for a few seconds.</p>
<h2 id="vision-and-language-transformers">Vision and language transformers</h2>
<p>So far we have seen how to build discrete tokens for text, audio, and images.
A natural next step is to mix modalities: let the input tokens come from one
modality and the output tokens from another, or even use several modalities on
both sides. In practice, the most studied case is text + vision, but the same
ideas extend to other combinations.</p>
<p>The first ingredient is a large multimodal dataset. For text–image work, the
LAION-400M dataset has played a role similar to ImageNet for image
classification, enabling rapid progress in both text-to-image generation and
image captioning. Text-to-image generation is very close to unconditional image
generation, except that we now <em>condition</em> on a text prompt. With transformers, conditioning on text is straightforward. We first encode the
prompt into text tokens and keep them in the context, at every step when the
model predicts the next image token, its attention layers can look back at both
the previously generated image tokens <em>and</em> these fixed text tokens, so the
visual details follow the words in the prompt.</p>
<p>We can also view text-to-image as a standard sequence-to-sequence problem, like
machine translation, but with discrete image tokens as the target sequence
instead of words. This motivates using a full encoder–decoder transformer: the
encoder reads the text tokens <span class="arithmatex">\(X\)</span>, and the decoder outputs the image tokens <span class="arithmatex">\(Y\)</span>.
Models such as Parti follow this pattern and scale the transformer to tens of
billions of parameters, with performance improving as the model size increases.</p>
<p>Another research line starts from large pre-trained language models and
adapts them so they can also accept visual inputs. These systems usually use
custom modules that map images to continuous feature vectors, which are then
injected into the language model. Because the visual representation is tied to a
specific encoder and feature format, it is awkward to plug in new kinds of data,
such as audio or video, without redesigning this interface. This also makes it
harder to apply the same model to <em>generate</em> images, since the model never
sees discrete image tokens that could be decoded back into pixels in a unified
way. Ideally, we would like a single model that can consume and
produce both text and image tokens (and possibly more). The simplest recipe is
to treat everything as one long token sequence and define a joint vocabulary
that is just the union of the text token dictionary and the image token
codebook. The key point is that <em>all</em> modalities now share this one
vocabulary, so a single transformer can read and generate mixed streams of
tokens (text, image, audio, …) without any modality-specific heads or
separate architectures.</p>
<p>Models such as CM3 and CM3Leon follow this language-modelling view. They are
trained on HTML pages from the web that contain both text and images, using a
variant of next-token prediction over the mixed token stream. With enough
training data and a scalable architecture, these models become very powerful and
flexible: they can do text-to-image generation, image captioning, image editing,
text completion, and essentially any task a regular language model can handle,
all within a single multimodal transformer.</p>
<h2 id="references">References</h2>
<ul>
<li>Bishop, C. M., &amp; Bishop, H. (2023). Transformers. In Deep Learning: Foundations and Concepts (pp. 357-406). Cham: Springer International Publishing.</li>
</ul>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "..", "features": [], "search": "../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.79ae519e.min.js"></script>
      
        <script src="../js/mathjax-config.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>