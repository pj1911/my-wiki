
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../Generative%20Adversarial%20Networks/">
      
      
        <link rel="next" href="../Transformers-2-NLP/">
      
      
        
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.1">
    
    
      
        <title>1 – Introduction - Prajwal's  Wiki</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.484c7ddc.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../css/justify.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#transformers-introduction" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Prajwal&#39;s  Wiki" class="md-header__button md-logo" aria-label="Prajwal's  Wiki" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Prajwal's  Wiki
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              1 – Introduction
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Prajwal&#39;s  Wiki" class="md-nav__button md-logo" aria-label="Prajwal's  Wiki" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Prajwal's  Wiki
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Home
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Machine Learning
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Machine Learning
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../linearRegression/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Linear Regression
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Generative%20Adversarial%20Networks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Generative Adversarial Networks
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_3" checked>
        
          
          <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Transformers
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Transformers
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    1 – Introduction
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    1 – Introduction
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    <span class="md-ellipsis">
      
        Introduction
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#attention" class="md-nav__link">
    <span class="md-ellipsis">
      
        Attention
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Attention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformer-processing" class="md-nav__link">
    <span class="md-ellipsis">
      
        Transformer processing
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#notations" class="md-nav__link">
    <span class="md-ellipsis">
      
        Notations
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#attention-coefficients" class="md-nav__link">
    <span class="md-ellipsis">
      
        Attention coefficients
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#self-attention" class="md-nav__link">
    <span class="md-ellipsis">
      
        Self attention
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#network-parameters" class="md-nav__link">
    <span class="md-ellipsis">
      
        Network parameters
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scaled-self-attention" class="md-nav__link">
    <span class="md-ellipsis">
      
        Scaled self attention
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multi-head-attention" class="md-nav__link">
    <span class="md-ellipsis">
      
        Multi head attention
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformer-layers" class="md-nav__link">
    <span class="md-ellipsis">
      
        Transformer layers
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#computational-complexity" class="md-nav__link">
    <span class="md-ellipsis">
      
        Computational complexity
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#positional-encoding" class="md-nav__link">
    <span class="md-ellipsis">
      
        Positional encoding
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    <span class="md-ellipsis">
      
        References
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Transformers-2-NLP/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    2 – Natural Language Processing
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Transformers-3-LLMs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    3 – Transformer Language Models
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Transformers-4-MMT/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    4 – Multimodal Transformers
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Mathematics
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Mathematics
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Fuzzy%20Inference%20Systems/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Fuzzy Inference Systems
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Neural network methods for partial differential equations
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Neural network methods for partial differential equations
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Hamilton-Jacobi%20equations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    1 - Hamilton Jacobi Equation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Hyperbolic%20PDEs-1-weak%20solutions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    2 - Weak solutions
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Hyperbolic%20PDEs-2-Conservation%20laws%20and%20entropy%20form/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    3 - Entropy Stable and Hyperbolic Solutions
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Reinforcement Learning
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    Reinforcement Learning
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../RL-1-Introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    1 - Introduction
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../RL-2-Markov%20Decision%20Processes/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    2 - Markov Decision Processes
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../RL-3-Partially%20Observable%20MDP/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    3 - Partially Observable MDP
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../RL-4-Planning%20with%20Dynamic%20Programming/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    4 - Planning with Dynamic Programming
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../RL-5-Model%20free%20prediction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    5 - Model Free Prediction
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../RL-6-Model%20free%20control/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    6 - Model Free Control
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../RL-7-Value%20function%20approximation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    7 - Value Function Approximation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    <span class="md-ellipsis">
      
        Introduction
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#attention" class="md-nav__link">
    <span class="md-ellipsis">
      
        Attention
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Attention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformer-processing" class="md-nav__link">
    <span class="md-ellipsis">
      
        Transformer processing
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#notations" class="md-nav__link">
    <span class="md-ellipsis">
      
        Notations
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#attention-coefficients" class="md-nav__link">
    <span class="md-ellipsis">
      
        Attention coefficients
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#self-attention" class="md-nav__link">
    <span class="md-ellipsis">
      
        Self attention
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#network-parameters" class="md-nav__link">
    <span class="md-ellipsis">
      
        Network parameters
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scaled-self-attention" class="md-nav__link">
    <span class="md-ellipsis">
      
        Scaled self attention
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multi-head-attention" class="md-nav__link">
    <span class="md-ellipsis">
      
        Multi head attention
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformer-layers" class="md-nav__link">
    <span class="md-ellipsis">
      
        Transformer layers
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#computational-complexity" class="md-nav__link">
    <span class="md-ellipsis">
      
        Computational complexity
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#positional-encoding" class="md-nav__link">
    <span class="md-ellipsis">
      
        Positional encoding
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    <span class="md-ellipsis">
      
        References
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="transformers-introduction">Transformers - Introduction</h1>
<h2 id="introduction">Introduction</h2>
<p>Transformers are a major breakthrough in deep learning. They use attention as a way for the network to give different weights to different inputs, where those weights are determined by the inputs themselves. This lets transformers naturally capture useful patterns in sequences and other kinds of data. They’re called transformers because they take a set of vectors in one space and turn them into a new set of vectors (with the same size) in another space. This new space is designed such that it holds a richer internal representation that makes it easier to solve downstream tasks.</p>
<p>One big strength of transformers is that transfer learning works really well with them. We can first train a transformer on a huge amount of data, then fine-tune that same model for many different tasks. When a large model like this can be adapted to lots of tasks, we call it a foundation model.</p>
<p>Transformers can also be trained in a self-supervised way on unlabeled data, which is perfect for language, since there’s so much text available on the internet and elsewhere. The scaling hypothesis says that if we just make the model bigger (more parameters) and train it on more data, we get much better performance even without changing the architecture. Transformers also run very efficiently on GPUs, which support massive parallel processing, so we can train huge language models with around a trillion parameters in a reasonable time. Now we will go through the different components of the Transformer architecture:</p>
<h2 id="attention">Attention</h2>
<p>The core idea behind transformers is attention. It was first created to improve RNNs for machine translation but later, it was shown that we could remove recurrence entirely and rely only on attention, getting much better results. Today, attention-based transformers have largely replaced RNNs in almost all applications. For example, consider the following sentences:</p>
<ul>
<li>The baseball player gripped the bat.</li>
<li>A small animal that hangs upside-down might be a bat.</li>
</ul>
<p>Here the word “bat” has different meanings in the two sentences. However, this can be detected only by looking at the context provided by the other words in the sequence. We also see that some words are more important than others in determining the interpretation of “bat.” In the first sentence, the words “baseball player” and “gripped” strongly indicate that “bat” refers to a piece of sports equipment, whereas in the second sentence, the words “small animal” and “hangs upside-down” indicate that “bat” refers to a flying mammal. Thus, to determine the appropriate interpretation of “bat,” a neural network processing such a sentence should attend to—i.e., rely more heavily on these specific words from the rest of the sequence. </p>
<p>In a standard neural network, each input affects the output based on its weight, and once the network is trained, these weights stay fixed. But based on the above examples, we want the model to focus on different words in different positions for each new input. Attention makes this possible by using weights that change depending on the specific input data.</p>
<p>In natural language processing we will see that word embeddings map each word to a vector in an embedding space. These vectors are then used as inputs to neural networks. The embeddings capture basic meaning: words with similar meanings end up close together in this space. A key point is that each word always maps to the same vector (for example, “bat” always has one fixed embedding even when the context is different, like in our example above). Meanwhile, a transformer can be seen as a more powerful kind of embedding. It maps each word’s vector to a new vector that depends on the other words in the sequence. That means the word “bat” can end up in different places depending on the sentence: near “baseball” or near “animal”.</p>
<h3 id="transformer-processing">Transformer processing</h3>
<p>In a transformer, the input is a set of vectors <span class="arithmatex">\(\{x_n\}\)</span> of dimensionality <span class="arithmatex">\(D\)</span>,
for <span class="arithmatex">\(n = 1, \dots, N\)</span>. Each vector is called a <em>token</em>. A token might
correspond to a word in a sentence or a patch in an image.</p>
<p>The individual components <span class="arithmatex">\(x_{ni}\)</span> of each token are called <em>features</em>. A key advantage of transformers is that we do not need to design a different
neural network architecture for each data type. Instead, we simply convert the
different kinds of data into a shared set of tokens and feed them into the same
model.</p>
<h3 id="notations">Notations</h3>
<p>We stack the token vectors for one sequence into a data matrix <span class="arithmatex">\(\mathbf{X}\)</span> of shape <span class="arithmatex">\(\mathbf{N} \times \mathbf{D}\)</span>, where each row (<span class="arithmatex">\(x_n^T\)</span>) is a token with <span class="arithmatex">\(\mathbf{D}\)</span> columns or features. In real tasks, we have many such sequences (for example, many text passages, 
with each word represented as one token). </p>
<p>A fundamental building block of the transformer is the <em>transformer layer</em>, a function that takes <span class="arithmatex">\(\mathbf{X}\)</span> as input 
and outputs a new matrix <span class="arithmatex">\(\tilde{\mathbf{X}}\)</span> of the same size:</p>
<div class="arithmatex">\[
\tilde{\mathbf{X}} = \mathrm{TransformerLayer}[\mathbf{X}] \, .
\]</div>
<p>By stacking several transformer layers, we obtain a deep network that can learn 
rich internal representations. Each transformer layer has its own weights and 
biases, which are learned using gradient descent with an appropriate cost function.</p>
<p>A single transformer layer has two stages. The first stage implements the attention mechanism, which, for each feature column, forms a weighted sum over all tokens (rows), thereby mixing information between the token vectors. The second stage then acts on each row 
independently and further transforms the features within each token vector.</p>
<h3 id="attention-coefficients">Attention coefficients</h3>
<p>Suppose we have a set of input token vectors (rows)</p>
<div class="arithmatex">\[
\mathbf{x}_1, \dots, \mathbf{x}_N
\]</div>
<p>and we want to map them to a new set of output vectors</p>
<div class="arithmatex">\[
\mathbf{y}_1, \dots, \mathbf{y}_N
\]</div>
<p>in a new embedding space that captures richer semantic structure.</p>
<p>For any particular output vector <span class="arithmatex">\(\mathbf{y}_n\)</span>, we want it to depend not only on
its corresponding input <span class="arithmatex">\(\mathbf{x}_n\)</span> but on all input token vectors (rows)
<span class="arithmatex">\(\mathbf{x}_1, \dots, \mathbf{x}_N\)</span>. A simple way to achieve this is to define
<span class="arithmatex">\(\mathbf{y}_n\)</span> as a weighted sum of the inputs:</p>
<div class="arithmatex">\[
\mathbf{y}_n = \sum_{m=1}^{N} a_{nm}\,\mathbf{x}_m,
\]</div>
<p>where the coefficients <span class="arithmatex">\(a_{nm}\)</span> are called <em>attention weights</em>.</p>
<p>We require these coefficients to satisfy</p>
<div class="arithmatex">\[
a_{nm} \ge 0 \quad \text{for all } m
\]</div>
<p>and</p>
<div class="arithmatex">\[
\sum_{m=1}^{N} a_{nm} = 1.
\]</div>
<p>These constraints ensure that the weights form a partition of unity, so that
each coefficient lies in the range <span class="arithmatex">\(0 \le a_{nm} \le 1\)</span>. Thus, each output
vector <span class="arithmatex">\(\mathbf{y}_n\)</span> is a convex combination (a weighted average) of the input
vectors, with some inputs receiving larger weights than others as we wanted.</p>
<p>Note that we have a different set of coefficients <span class="arithmatex">\(\{a_{n1}, \dots, a_{nN}\}\)</span>
for each output index <span class="arithmatex">\(n\)</span>, and the above constraints apply separately for each
<span class="arithmatex">\(n\)</span>. The coefficients <span class="arithmatex">\(a_{nm}\)</span> depend on the input data, later we will see how
we compute them in practice.</p>
<h3 id="self-attention">Self attention</h3>
<p>We wish to determine the coefficients <span class="arithmatex">\(a_{nm}\)</span> used in</p>
<div class="arithmatex">\[
\mathbf{y}_n = \sum_{m=1}^{N} a_{nm}\,\mathbf{x}_m.
\]</div>
<p><strong>Query, Key and Value analogy.</strong><br />
In information retrieval (e.g. a movie streaming service), each movie is
described by an attribute vector called a <em>key</em>, while the movie file
itself is a <em>value</em>.<br />
A user specifies their preferences as a <em>query</em> vector.<br />
The system compares the query with all keys, finds the best match, and returns
the corresponding value.<br />
Focusing on a single best-matching movie would be called <em>hard attention</em>.</p>
<p>In transformers we use <em>soft attention</em>: instead of returning a single
value, we compute continuous weights that tell us how strongly each value
should influence the output. This keeps the whole mapping differentiable, so it
can be trained by gradient descent.</p>
<p><strong>Applying this to tokens.</strong><br />
For each input token we start from its embedding <span class="arithmatex">\(\mathbf{x}_n\)</span>, but we use
<em>three</em> conceptually different copies of it (in this simple version we set
<span class="arithmatex">\(\mathbf{q}_n = \mathbf{k}_n = \mathbf{v}_n = \mathbf{x}_n\)</span> for all <span class="arithmatex">\(n\)</span>).</p>
<ul>
<li><strong>Value</strong> <span class="arithmatex">\(\mathbf{v}_n\)</span> <em>(movie file)</em>: the actual content to return or mix into the output.</li>
<li><strong>Key</strong> <span class="arithmatex">\(\mathbf{k}_n\)</span> <em>(movie's attribute profile)</em>: a summary describing that movie (genre, actors, length) used for matching.</li>
<li><strong>Query</strong> <span class="arithmatex">\(\mathbf{q}_n\)</span> <em>(user's wish list of attributes)</em>: what the output position is looking for and this is compared against all keys.</li>
</ul>
<p>To decide how much the output at position <span class="arithmatex">\(n\)</span> should attend to token <span class="arithmatex">\(m\)</span>, we
measure the similarity between the corresponding query and key vectors.  A
simple similarity measure is the dot product</p>
<div class="arithmatex">\[
\mathbf{x}_n^\top \mathbf{x}_m.
\]</div>
<p><strong>Attention weights via softmax.</strong><br />
To enforce the constraints</p>
<div class="arithmatex">\[
a_{nm} \ge 0, \qquad
\sum_{m=1}^{N} a_{nm} = 1 \quad \text{for each fixed } n,
\]</div>
<p>we define the attention weights by a softmax over <span class="arithmatex">\(m\)</span>:</p>
<div class="arithmatex">\[
a_{nm}
= \frac{\exp\big(\mathbf{x}_n^\top \mathbf{x}_m\big)}
       {\sum_{m'=1}^{N} \exp\big(\mathbf{x}_n^\top \mathbf{x}_{m'}\big)}.
\]</div>
<p>Thus, for each <span class="arithmatex">\(n\)</span>, the row <span class="arithmatex">\((a_{n1},\dots,a_{nN})\)</span> forms a set of
non-negative coefficients that sum to one, assigning larger weights to inputs
whose keys are more similar to the query.</p>
<p><strong>Matrix form and self-attention.</strong><br />
Let <span class="arithmatex">\(\mathbf{X} \in \mathbb{R}^{N \times D}\)</span> be the input matrix whose <span class="arithmatex">\(n\)</span>-th
row is <span class="arithmatex">\(\mathbf{x}_n\)</span>, and let <span class="arithmatex">\(\mathbf{Y} \in \mathbb{R}^{N \times D}\)</span> be the
output matrix whose <span class="arithmatex">\(n\)</span>-th row is <span class="arithmatex">\(\mathbf{y}_n\)</span>.<br />
We first compute all pairwise dot products:</p>
<div class="arithmatex">\[
\mathbf{L} = \mathbf{X}\mathbf{X}^\top \in \mathbb{R}^{N \times N}.
\]</div>
<p>We then apply the softmax row-wise to obtain the attention matrix</p>
<div class="arithmatex">\[
\mathbf{A} = \mathrm{Softmax}[\mathbf{X}\mathbf{X}^\top],
\]</div>
<p>and finally compute the outputs as</p>
<div class="arithmatex">\[
\mathbf{Y} = \mathbf{A}\mathbf{X}
           = \mathrm{Softmax}[\mathbf{X}\mathbf{X}^\top]\,\mathbf{X}.
\]</div>
<p>Because the queries, keys, and values are all derived from the same sequence
<span class="arithmatex">\(\mathbf{X}\)</span>, this mechanism is known as <em>self-attention</em>, and since the
similarity is given by a dot product, it is specifically <em>dot-product
self-attention</em>.</p>
<h3 id="network-parameters">Network parameters</h3>
<p>We currently have two problems:
- Vanilla self-attention contains no trainable parameters
- Treat all feature values within a token equally in determining the attention coefficients.</p>
<p>Introducing a single projection (learnable weight matrix) <span class="arithmatex">\(\mathbf{U}\in\mathbb{R}^{D\times D}\)</span>,</p>
<div class="arithmatex">\[
\tilde{\mathbf{X}}=\mathbf{X}\mathbf{U},\qquad
\mathbf{Y}=\mathrm{Softmax}\!\big[\mathbf{X}\mathbf{U}\mathbf{U}^\top\mathbf{X}^\top\big]\;\mathbf{X}\mathbf{U},
\]</div>
<p>adds learnability but yields a symmetric score matrix
<span class="arithmatex">\(\mathbf{X}\mathbf{U}\mathbf{U}^\top\mathbf{X}^\top\)</span> and ties the value and
similarity parameters.</p>
<p>To obtain a more flexible, asymmetric mechanism (for example, bat should be strongly associated with animal but animal should not be strongly associated with bat as there are different kinds of animals), therefore, we use independent projections
for queries, keys, and values:</p>
<div class="arithmatex">\[
\mathbf{Q}=\mathbf{X}\mathbf{W}^{(q)},\quad
\mathbf{K}=\mathbf{X}\mathbf{W}^{(k)},\quad
\mathbf{V}=\mathbf{X}\mathbf{W}^{(v)},
\]</div>
<p>with linear trasnformations <span class="arithmatex">\(\mathbf{W}^{(q)},\mathbf{W}^{(k)}\in\mathbb{R}^{D\times D_k}\)</span> and
<span class="arithmatex">\(\mathbf{W}^{(v)}\in\mathbb{R}^{D\times D_v}\)</span> (typically <span class="arithmatex">\(D_k=D\)</span>, <span class="arithmatex">\(D_v=D\)</span> as this helps to stack multiple Transformer layers on top of each other).
The resulting dot-product self-attention is</p>
<div class="arithmatex">\[
\mathbf{Y}=\mathrm{Softmax}\!\big[\mathbf{Q}\mathbf{K}^\top\big]\;\mathbf{V},
\]</div>
<p>with <span class="arithmatex">\(\mathbf{Q}\mathbf{K}^{T}\in\mathbb{R}^{N\times N}\)</span> and <span class="arithmatex">\(\mathbf{Y}\in\mathbb{R}^{N\times D_v}\)</span>, which is trainable, reweighs features, and supports asymmetric token
relationships.</p>
<p><strong>Bias absorption.</strong><br />
Add a column of ones to the data and a row for biases to the weights, so that</p>
<div class="arithmatex">\[
XW + \mathbf{1} b^\top
= \underbrace{\big[\,X\;\;\mathbf{1}\,\big]}_{X_{\text{aug}}}
\underbrace{\begin{bmatrix} W \\ b^\top \end{bmatrix}}_{W_{\text{aug}}}.
\]</div>
<p>Hence, biases can be treated as implicit via augmentation.</p>
<p><strong>Fixed vs data dependent weights - Simple NN vs Transformer.</strong><br />
For a simple NN let a single input vector be</p>
<div class="arithmatex">\[
\mathbf{x}\in\mathbb{R}^{D_{\text{in}}}
\quad\text{and the layer have } D_{\text{out}} \text{ output units.}
\]</div>
<p>The layer has a weight matrix <span class="arithmatex">\(W\in\mathbb{R}^{D_{\text{in}}\times D_{\text{out}}}\)</span>.
The output is</p>
<div class="arithmatex">\[
\mathbf{y}=\mathbf{x}W \quad(\text{or } \mathbf{y}=\mathbf{x}W).
\]</div>
<p>Component-wise, for each output unit <span class="arithmatex">\(n\in\{1,\dots,D_{\text{out}}\}\)</span>,</p>
<div class="arithmatex">\[
y_n=\sum_{m=1}^{D_{\text{in}}} x_m\,W_{m n}.
\]</div>
<p>So each output unit is a <em>weighted sum</em> of <em>all</em> input features. But these weights <span class="arithmatex">\(W_{mn}\)</span> are fix for all inputs.</p>
<p><strong>Transformers data dependent weights.</strong><br />
In the standard layer above, once training is done, the weights <span class="arithmatex">\(W_{mn}\)</span> are
<em>fixed</em>. For any new input <span class="arithmatex">\(\mathbf{x}\)</span>, the contribution of input feature
<span class="arithmatex">\(m\)</span> to output <span class="arithmatex">\(n\)</span> is always <span class="arithmatex">\(x_m\,W_{mn}\)</span>.</p>
<p>In <em>attention</em>, by contrast, the mixing coefficients are computed from the
<em>current input</em>. With queries, keys, and values
<span class="arithmatex">\(\,Q=XW^{(q)},\,K=XW^{(k)},\,V=XW^{(v)}\)</span>. Therefore, Q,K and V will be different for different inputs.</p>
<div class="arithmatex">\[
Y = \underbrace{\mathrm{Softmax}\!\big(QK^\top\big)}_{\displaystyle A(X)}
\,V,
\qquad
\mathbf{y}_n=\sum_{m=1}^{N} a_{nm}(X)\,\mathbf{v}_m,
\]</div>
<p>and the “weights” <span class="arithmatex">\(a_{nm}(X)\)</span> depend on the present data (via a softmax over
dot products).Thus, the contribution from token <span class="arithmatex">\(m\)</span> to output <span class="arithmatex">\(n\)</span> can be nearly zero for one input and large for another a behavior that a standard fixed-weight layer cannot achieve.</p>
<h3 id="scaled-self-attention">Scaled self attention</h3>
<p><strong>Issue.</strong> Softmax gradients shrink when its inputs (logits) are large in
magnitude (saturation). In dot-product attention the logits are
<span class="arithmatex">\(\ell_{ij}=\mathbf{q}_i^\top\mathbf{k}_j\)</span>, which can grow with vector
dimension.</p>
<p><strong>Why do they grow?</strong> Assume (as a scale reference) that query/key
components are independent with mean <span class="arithmatex">\(0\)</span> and variance <span class="arithmatex">\(1\)</span>:
<span class="arithmatex">\(\mathbf{q}=(q_1,\dots,q_{D_k})\)</span>, <span class="arithmatex">\(\mathbf{k}=(k_1,\dots,k_{D_k})\)</span>.
Then</p>
<div class="arithmatex">\[
\mathbf{q}^\top\mathbf{k}=\sum_{t=1}^{D_k} q_t k_t,
\qquad
\mathbb{E}[q_t k_t]=0,\quad
\mathrm{Var}(q_t k_t)=\mathbb{E}[q_t^2]\mathbb{E}[k_t^2]=1\cdot 1=1,
\]</div>
<p>so by independence,</p>
<div class="arithmatex">\[
\mathrm{Var}(\mathbf{q}^\top\mathbf{k})
=\sum_{t=1}^{D_k}\mathrm{Var}(q_t k_t)=D_k,
\]</div>
<p>and the typical magnitude (std. dev.) is <span class="arithmatex">\(\sqrt{D_k}\)</span>. Larger <span class="arithmatex">\(D_k\)</span> therefore
pushes logits to larger scales, sharpening the softmax and shrinking gradients.</p>
<p><strong>Fix.</strong> Normalize logits by their standard deviation:</p>
<div class="arithmatex">\[
\tilde{\ell}_{ij}=\frac{\mathbf{q}_i^\top\mathbf{k}_j}{\sqrt{D_k}},
\]</div>
<p>which makes <span class="arithmatex">\(\mathrm{Var}(\tilde{\ell}_{ij})\approx 1\)</span> under the reference
assumption, keeping softmax in a stable range.</p>
<p><strong>Result.</strong> The attention layer is</p>
<div class="arithmatex">\[
\mathbf{Y} = Attention(\mathbf{Q},\mathbf{K},\mathbf{V})
=\mathrm{Softmax}\!\left(\frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{D_k}}\right)\mathbf{V}.
\]</div>
<p>This is <em>scaled</em> dot-product self-attention. (Even when the independence/variance
assumptions are only approximate, this scaling acts like a principled temperature
that stabilizes training.)</p>
<h3 id="multi-head-attention">Multi head attention</h3>
<p>A single head can average out distinct patterns. Therefore, we use <span class="arithmatex">\(H\)</span> parallel heads with separate
parameters to attend to different patterns for example in NLP it can be tenses, vocabulary, etc.</p>
<p><strong>Setup.</strong> Let <span class="arithmatex">\(X\!\in\!\mathbb{R}^{N\times D}\)</span>.
Each head <span class="arithmatex">\(h\in\{1,\dots,H\}\)</span> has its own:</p>
<div class="arithmatex">\[
Q_h = X W^{(q)}_h,\qquad
K_h = X W^{(k)}_h,\qquad
V_h = X W^{(v)}_h,
\]</div>
<p>with <span class="arithmatex">\(W^{(q)}_h,W^{(k)}_h\!\in\!\mathbb{R}^{D\times D_k}\)</span> and
<span class="arithmatex">\(W^{(v)}_h\!\in\!\mathbb{R}^{D\times D_v}\)</span>.</p>
<p><strong>Per-head attention (scaled).</strong></p>
<div class="arithmatex">\[
\mathbf{H}_h \;=\; Attention(\mathbf{Q_h}, \mathbf{K_h},\mathbf{V_h}) \;=\; \mathrm{Softmax}\!\left(\frac{Q_h K_h^\top}{\sqrt{D_k}}\right) V_h
\;\in\;\mathbb{R}^{N\times D_v}.
\]</div>
<p><strong>Combine heads.</strong> Concatenate along features to get shape <span class="arithmatex">\((N \times H D_v)\)</span> and project:</p>
<div class="arithmatex">\[
Y(X) \;=\; \mathrm{Concat}\,[H_1,\dots,H_H]\; W^{(o)},\qquad
W^{(o)} \in \mathbb{R}^{H D_v \times D }\text{ is a trainable linear matrix: },
\]</div>
<p>so <span class="arithmatex">\(Y\in\mathbb{R}^{N\times D}\)</span> matches the input width.
A common choice is <span class="arithmatex">\(D_k=D_v=D/H\)</span>, making the concatenated matrix <span class="arithmatex">\(N\times D\)</span>.</p>
<p><strong>Redundancy observed.</strong>
It is due to the reparameterization on the value path.
We know for each head,</p>
<div class="arithmatex">\[
H_h=\mathrm{Softmax}\!\Big(\tfrac{Q_h K_h^\top}{\sqrt{D_k}}\Big)\;V_h,
\qquad
V_h = X W^{(v)}_h,
\]</div>
<p>and the final combine is</p>
<div class="arithmatex">\[
Y=\mathrm{Concat}[H_1,\dots,H_H]\;W^{(o)}.
\]</div>
<p>We can write <span class="arithmatex">\(W^{(o)}=\big[(W^{(o)}_1)^\top\;\cdots\;(W^{(o)}_H)^\top\big]^\top\)</span>.
Then</p>
<div class="arithmatex">\[
Y=\sum_{h=1}^H H_h\,W^{(o)}_h
  =\sum_{h=1}^H \mathrm{Softmax}\!\Big(\tfrac{Q_h K_h^\top}{\sqrt{D_k}}\Big)\;\underbrace{(X W^{(v)}_h W^{(o)}_h)}_{X\,\tilde W^{(v)}_h}.
\]</div>
<p>Then we can define <span class="arithmatex">\(\tilde W^{(v)}_h := W^{(v)}_h W^{(o)}_h\)</span>. The layer becomes</p>
<div class="arithmatex">\[
Y=\sum_{h=1}^H \mathrm{Softmax}\!\Big(\tfrac{Q_h K_h^\top}{\sqrt{D_k}}\Big)\;X\,\tilde W^{(v)}_h,
\]</div>
<p>with no explicit <span class="arithmatex">\(W^{(o)}\)</span>.</p>
<p>Therefore the two consecutive linear maps on <span class="arithmatex">\(V\)</span> (first <span class="arithmatex">\(W^{(v)}_h\)</span>,
then the head’s block <span class="arithmatex">\(W^{(o)}_h\)</span>) can always be merged into a single matrix
<span class="arithmatex">\(\tilde W^{(v)}_h\)</span>. Since the attention weights use only <span class="arithmatex">\(Q\)</span> and <span class="arithmatex">\(K\)</span>, the value path is purely
linear per head: <span class="arithmatex">\(V_h W^{(o)}_h = X\,W^{(v)}_h W^{(o)}_h\)</span>.
Thus we can collapse the two matrices into one
<span class="arithmatex">\(\tilde W^{(v)}_h := W^{(v)}_h W^{(o)}_h\)</span>.
This gives two equivalent parameterizations:</p>
<div class="arithmatex">\[
\text{(separate)}\;\; (W^{(v)}_h,\,W^{(o)}_h)
\quad\longleftrightarrow\quad
\text{(collapsed)}\;\; \tilde W^{(v)}_h.
\]</div>
<p><strong>This non-uniqueness is the redundancy.</strong> we keep two matrices even
though one combined matrix suffices to represent exactly the same function.</p>
<p>Why keep <span class="arithmatex">\(W^{(o)}\)</span> in practice? It standardizes the output width <span class="arithmatex">\(D\)</span>,
keeps per-head value sizes <span class="arithmatex">\(D_v\)</span> small, and matches common implementations, 
but representationally, only the product <span class="arithmatex">\(W^{(v)}_h W^{(o)}_h\)</span> matters.</p>
<h3 id="transformer-layers">Transformer layers</h3>
<p>The input tokens have the shape:  <span class="arithmatex">\((X\in\mathbb{R}^{N\times D})\)</span> (where rows are the tokens).</p>
<p><strong>Multi-head attention (MHA).</strong> For heads <span class="arithmatex">\((h=1,\dots,H)\)</span>,</p>
<div class="arithmatex">\[
Q_h=XW^{(q)}_h,\quad K_h=XW^{(k)}_h,\quad V_h=XW^{(v)}_h,
\]</div>
<div class="arithmatex">\[
H_h=\mathrm{Softmax}\!\Big(\tfrac{Q_h K_h^\top}{\sqrt{D_k}}\Big)\,V_h\in\mathbb{R}^{N\times D_v}.
\]</div>
<p>Concatenate and project:</p>
<div class="arithmatex">\[
\mathrm{MHA}(X)=\mathrm{Concat}[H_1,\dots,H_H]\;W^{(o)},\qquad
W^{(o)}\in\mathbb{R}^{H D_v\times D}.
\]</div>
<p>The output from MHA layer has the same shape as its input of <span class="arithmatex">\((N \times D)\)</span> enabling residuals. MHA gives data-dependent mixing between tokens and helps
learn different relations (e.g., syntax vs. semantics) in parallel.</p>
<p><strong>Residual + LayerNorm (two variants).</strong>
Residuals preserve the input signal and enable deep stacks. In addition to this, pre/post
norm improve optimization stability (pre-norm is common for very deep models).</p>
<p><em>Post-norm:</em></p>
<div class="arithmatex">\[
Z=\mathrm{LayerNorm}\big(\mathrm{MHA}(X)+X\big).
\]</div>
<p><em>Pre-norm:</em></p>
<div class="arithmatex">\[
Z=X+\mathrm{MHA}(\mathrm{LayerNorm}(X)).
\]</div>
<p><strong>Position-wise MLP (shared across tokens).</strong>
MHA outputs lie in the span of inputs due to linear mixing, the MLP adds nonlinearity and feature-wise transformation per token, boosting expressiveness. A two-layer example with activation <span class="arithmatex">\(\phi\)</span> can be denoted as:</p>
<div class="arithmatex">\[
\mathrm{MLP}(U)=\phi(UW_1+b_1)\,W_2+b_2,\quad
W_1\in\mathbb{R}^{D\times D_{\mathrm{ff}}},\;\;W_2\in\mathbb{R}^{D_{\mathrm{ff}}\times D}.
\]</div>
<p><strong>Block output (two variants).</strong></p>
<p><em>Post-norm:</em></p>
<div class="arithmatex">\[
\tilde X=\mathrm{LayerNorm}\big(\mathrm{MLP}(Z)+Z\big).
\]</div>
<p><em>Pre-norm:</em></p>
<div class="arithmatex">\[
\tilde X=Z+\mathrm{MLP}(\mathrm{LayerNorm}(Z)).
\]</div>
<p>Layer normalization is generally used in both sublayers (MHA and MLP) to normalize per token to reduce covariate shift and to keep activations in a
well-scaled regime to keep the training process steady.</p>
<p><strong>Stacking.</strong> Repeat the block <span class="arithmatex">\(L\)</span> times to form a
deep transformer. Note that all mappings preserve shape <span class="arithmatex">\(N\times D\)</span>, enabling residuals.</p>
<h3 id="computational-complexity">Computational complexity</h3>
<p><strong>Setup.</strong> Input <span class="arithmatex">\(X\in\mathbb{R}^{N\times D}\)</span> (rows = tokens with <span class="arithmatex">\(D\)</span> features).
A transformer block outputs the same shape <span class="arithmatex">\(N\times D\)</span> as its input.
We use <span class="arithmatex">\(H\)</span> heads and for each head key/query/value widths (features) are <span class="arithmatex">\(d_k,d_k,d_v\)</span> (typically
<span class="arithmatex">\(d_k=d_v=D/H\)</span>).</p>
<p><strong>Baseline: fully connected on flattened sequence</strong><br />
Flatten <span class="arithmatex">\(X\)</span> to a vector in <span class="arithmatex">\(\mathbb{R}^{ND}\)</span> and map to <span class="arithmatex">\(\mathbb{R}^{ND}\)</span> with weight matrix
<span class="arithmatex">\(W\in\mathbb{R}^{ND\times ND}\)</span>.</p>
<div class="arithmatex">\[
\text{parameters}= \text{elements in the weight matrix = }N^2D^2,\qquad
\text{ FLOPs }\approx  2\,N^2D^2.
\]</div>
<p>Where, FLOPs mean Floating-point Operations.</p>
<p><strong>Multi-head self-attention (MHA)</strong></p>
<p><strong>1) Linear projections to <span class="arithmatex">\(Q,K,V\)</span>.</strong><br />
For each head <span class="arithmatex">\(h\)</span>:</p>
<div class="arithmatex">\[
Q_h=XW^{(q)}_h,\quad K_h=XW^{(k)}_h,\quad V_h=XW^{(v)}_h.
\]</div>
<p>with <span class="arithmatex">\(W^{(q)}_h,W^{(k)}_h\in\mathbb{R}^{D\times d_k}\)</span>, <span class="arithmatex">\(W^{(v)}_h\in\mathbb{R}^{D\times d_v}\)</span>.</p>
<div class="arithmatex">\[
\begin{aligned}
\text{Shapes: }&amp; Q_h,K_h\in\mathbb{R}^{N\times d_k},\; V_h\in\mathbb{R}^{N\times d_v}.
\end{aligned}
\]</div>
<div class="arithmatex">\[
\begin{aligned}
\text{FLOPs: }&amp;
\underbrace{N D d_k}_{XW^{(q)}_h}
+\underbrace{N D d_k}_{XW^{(k)}_h}
+\underbrace{N D d_v}_{XW^{(v)}_h}
= N D\,(2 d_k + d_v)\;\text{ per head}.
\end{aligned}
\]</div>
<div class="arithmatex">\[
\begin{aligned}
&amp;\Rightarrow\;\text{Total FLOPs (all \(H\) heads)} = N D H\,(2 d_k + d_v). \\
&amp;\text{If } d_k = d_v = D/H,\ \text{then this simplifies to } 3 N D^2.
\end{aligned}
\]</div>
<div class="arithmatex">\[
\text{parameters (projections)}=D(2H d_k + H d_v) = 3D^2.
\]</div>
<p><strong>2) Attention scores (scaled dot products).</strong><br />
For each head:</p>
<div class="arithmatex">\[
S_h=\frac{Q_h K_h^\top}{\sqrt{d_k}}\in\mathbb{R}^{N\times N}.
\]</div>
<div class="arithmatex">\[
\text{FLOPs: }N^2 d_k \text{ (matrix multiply)}\quad (\text{the divide by }\sqrt{d_k}\text{ is }O(N^2)).
\]</div>
<p>Softmax over rows:</p>
<div class="arithmatex">\[
A_h=\mathrm{Softmax}(S_h)\in\mathbb{R}^{N\times N},\qquad
\text{FLOPs: }O(N^2).
\]</div>
<p>Total FLOPs (all heads): <span class="arithmatex">\(H N^2 d_k\)</span> for <span class="arithmatex">\(QK^\top\)</span> and <span class="arithmatex">\(O(H N^2)\)</span> for softmax and parameters = 0.</p>
<p><strong>3) Value mixing.</strong><br />
For each head:</p>
<div class="arithmatex">\[
H_h=A_h V_h\in\mathbb{R}^{N\times d_v},\qquad
\text{FLOPs: }N^2 d_v = \frac{N^2D}{H}
 \text{ (matrix multiply)}, \qquad \text{parameters}=0
\]</div>
<p><strong>4) Concatenate and output projection.</strong><br />
Concatenate <span class="arithmatex">\(H_h\)</span> along features:</p>
<div class="arithmatex">\[
H=\mathrm{Concat}[H_1,\dots,H_H]\in\mathbb{R}^{N\times (H d_v)}.
\]</div>
<p>Project to width <span class="arithmatex">\(D\)</span>:</p>
<div class="arithmatex">\[
Y_{\text{attn}}=H W^{(o)},\quad W^{(o)}\in\mathbb{R}^{H d_v\times D}.
\]</div>
<div class="arithmatex">\[
\text{FLOPs: }N\,(H d_v)\,D,\qquad
\text{parameters (output proj)}=(H d_v)D.
\]</div>
<p><strong>Common setting <span class="arithmatex">\(d_k=d_v=D/H\)</span>.</strong><br />
Then</p>
<div class="arithmatex">\[
\begin{aligned}
\text{Parameters (MHA)} &amp;= D(2H\cdot \tfrac{D}{H} + H\cdot \tfrac{D}{H}) + 0 + 0 + (H\tfrac{D}{H})D
= 4D^2,\\
\text{FLOPs (MHA)} &amp;=
\underbrace{N D (2H d_k + H d_v)}_{=\,3ND^2}
+\underbrace{H N^2 d_k}_{=\,N^2 D}
+\underbrace{H N^2 d_v}_{=\,N^2 D}
+\underbrace{N(H d_v)D}_{=\,N D^2}\\
&amp;= \boxed{2N^2 D + 4 N D^2}\quad(\text{softmax adds }O(N^2)).
\end{aligned}
\]</div>
<p><strong>Residual adds</strong><br />
<span class="arithmatex">\(X+Y_{\text{attn}}\)</span>: elementwise add, <span class="arithmatex">\(\;\text{FLOPs}=N D, \qquad \#\text{parmas} = 0 \)</span></p>
<p><strong>Layer normalization</strong><br />
Given an input token (row) <span class="arithmatex">\(x \in \mathbb{R}^D\)</span>, LayerNorm computes</p>
<div class="arithmatex">\[
\mu = \frac{1}{D} \sum_{i=1}^D x_i, 
\qquad
\sigma^2 = \frac{1}{D} \sum_{i=1}^D (x_i - \mu)^2,
\]</div>
<div class="arithmatex">\[
\hat{x}_i = \frac{x_i - \mu}{\sqrt{\sigma^2 + \varepsilon}},
\qquad \text{final output is given by: }
y_i = \gamma_i \hat{x}_i + \beta_i,
\quad i = 1,\dots,D,
\]</div>
<p>where <span class="arithmatex">\(\gamma, \beta \in \mathbb{R}^D\)</span> are learnable scale and bias.</p>
<p>For a batch of <span class="arithmatex">\(N\)</span> tokens (matrix <span class="arithmatex">\(X \in \mathbb{R}^{N \times D}\)</span>), FLOPs (in forward pass):</p>
<div class="arithmatex">\[
\text{FLOPs} \approx c \, N D
\]</div>
<p>for a small constant <span class="arithmatex">\(c\)</span> (mean/var + normalize + scale/shift).</p>
<p>Parameters per LayerNorm:</p>
<div class="arithmatex">\[
\text{parameters} = 2D \quad (\gamma,\beta).
\]</div>
<p><strong>Position-wise MLP (shared across tokens)</strong><br />
Two-layer MLP with hidden width <span class="arithmatex">\(D_{\text{ff}}\)</span>:</p>
<div class="arithmatex">\[
\mathrm{MLP}(U)=\phi(UW_1+b_1)\,W_2+b_2,\quad
W_1\in\mathbb{R}^{D\times D_{\text{ff}}},\;W_2\in\mathbb{R}^{D_{\text{ff}}\times D}.
\]</div>
<div class="arithmatex">\[
\text{parameters for }W_1, W_2, b_1, b_2 = D D_{\text{ff}} + D_{\text{ff}} D + D_{\text{ff}} + D
\approx 2 D D_{\text{ff}}.
\]</div>
<p><strong>FLOPs.</strong> The two matrix multiplications dominate:
<span class="arithmatex">\(\;UW_1: (N\times D)(D\times D_{\text{ff}})\)</span> and <span class="arithmatex">\((\cdot)W_2: (N\times D_{\text{ff}})(D_{\text{ff}}\times D)\)</span>,
each costing <span class="arithmatex">\(\approx  N D D_{\text{ff}}\)</span> FLOPs, so in total
<span class="arithmatex">\(\text{FLOPs} \approx 2 N D D_{\text{ff}}\)</span>.</p>
<p>Common choice <span class="arithmatex">\(D_{\text{ff}}=cD\)</span> (e.g.<span class="arithmatex">\(c{=}4\)</span>) gives <span class="arithmatex">\(\text{parameters}\approx 2c D^2\)</span>
and <span class="arithmatex">\(\text{FLOPs}=2c N D^2\)</span>.</p>
<p><strong>Block totals (one transformer block, pre-/post-norm similar)</strong><br />
Ignoring small <span class="arithmatex">\(ND\)</span> terms from residuals/LayerNorm:</p>
<div class="arithmatex">\[
\boxed{
\text{FLOPs} \;\approx\; \underbrace{2 N^2 D}_{\text{attention mixes}}
\;+\; \underbrace{4 N D^2}_{\text{QKV+out proj}}
\;+\; \underbrace{2c N D^2}_{\text{MLP}}
}
\]</div>
<div class="arithmatex">\[
\text{FLOPs} \;\approx\; 2 N^2 D \;+\; (4+2c)\,N D^2.
\]</div>
<div class="arithmatex">\[
\boxed{
\text{parameters} \;\approx\; \underbrace{4 D^2}_{\text{MHA}}
\;+\; \underbrace{2c D^2}_{\text{MLP}}
\;+\; \underbrace{4D}_{\text{two LayerNorms}}
}
\]</div>
<p><strong>When does which term dominate?</strong><br />
Attention dominates for long sequences (<span class="arithmatex">\(N\gg D\)</span>) since it scales as <span class="arithmatex">\(N^2 D\)</span>.
The MLP dominates for wide models (<span class="arithmatex">\(D\gg N\)</span>) since it scales as <span class="arithmatex">\(N D^2\)</span>.
Compared to a dense <span class="arithmatex">\(\mathbb{R}^{ND}\!\to\!\mathbb{R}^{ND}\)</span> layer
(<span class="arithmatex">\(\Theta(N^2D^2)\)</span> params/FLOPs), a transformer block is vastly more efficient.</p>
<h3 id="positional-encoding">Positional encoding</h3>
<p><strong>Why we need it.</strong><br />
Transformers have a really cool property, since it shares <span class="arithmatex">\((W_h^{(q)},W_h^{(k)},W_h^{(v)})\)</span> across input tokens and applies the
same computations to every row of <span class="arithmatex">\(X\in\mathbb{R}^{N\times D}\)</span>.<br />
This makes permuting the input rows results in the same permutation of the rows of the output matrix (permutation equivariance). Since parameters are shared across inputs, it gives two major benefits to the network, firstly it makes the computation parallel, and secondly it makes long range dependencies just as effective as the short range ones. This is because the same weight matrices (for attention, feed-forward layers, etc.) are shared across all tokens in the sequence, the model doesn’t need different parameters for each position or word. This means every token can be run through the same computations at the same time, letting GPUs/TPUs process all tokens in parallel instead of one after another. But for sequences (language, etc.), order matters, so we need to inject
positional information into the data. Since we want to keep the two nice properties of our attention layers discussed above, we’d rather encode token order directly in the input representations, instead of baking it into the network architecture itself.</p>
<p><strong>Additive positional encoding.</strong><br />
Associate each position <span class="arithmatex">\(n\)</span> with a vector <span class="arithmatex">\(r_n\in\mathbb{R}^{D}\)</span> as each token has <span class="arithmatex">\(D\)</span> dimensional features and add it to the
token embedding <span class="arithmatex">\(x_n\)</span>:</p>
<div class="arithmatex">\[
\tilde x_n = x_n + r_n \quad(=\text{row } n \text{ of } \tilde X).
\]</div>
<p>This might seem counter-intuitive, as this might mess up the input vector, but in reality this works really well. As in high-dimensional spaces, two unrelated vectors are almost orthogonal, so the model can keep token identity and position information relatively separate even when they’re added. Residual connections across layers help preserve this position information as it flows through the network. And because the layers are mostly linear, using addition of token and position embeddings behaves a lot like concatenating them and then applying a linear layer (add then linear is a special case of concatenation then linear where the bigger weight matrix is a concatenation of two same smaller weight matrices). This also preserves the model’s width <span class="arithmatex">\(D\)</span> (concatenation would increase cost).</p>
<p><strong>Example</strong><br />
The simplest example for this is <span class="arithmatex">\(r_n=\{1,2,3,\cdots\}\)</span>. But in this case the magnitude can get very high and start corrupting the input. In addition, it may not generalize well to new input sequences that are longer than samples in training dataset.</p>
<p><strong>Design goals.</strong><br />
A good positional code should be: (i) unique per position, (ii) bounded,
(iii) generalize to longer sequences, and (iv) support relative offsets.
Therefore, a good example of it can be positional encoding between (0,1).</p>
<p><strong>Learned Encoding</strong><br />
Another common way to add position information is with learned positional encodings. Here, each position in the sequence gets its own trainable vector, learned together with the rest of the model instead of being hand-designed. Because these position vectors are not shared across positions, permuting the tokens changes their positions and thus breaks permutation invariance, which is exactly what we want from a positional encoding. But the downside is that this scheme doesn’t naturally generalize to positions beyond those seen in training, meaning newer positions just have untrained embeddings. So, learned positional encoding are mainly a good fit when sequence lengths stay roughly the same during training and inference.</p>
<h2 id="references">References</h2>
<ul>
<li>Bishop, C. M., &amp; Bishop, H. (2023). Transformers. In Deep Learning: Foundations and Concepts (pp. 357-406). Cham: Springer International Publishing.</li>
</ul>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "..", "features": [], "search": "../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.79ae519e.min.js"></script>
      
        <script src="../js/mathjax-config.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>