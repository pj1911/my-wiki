{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"About Me","text":"<p>Hey all, I am Prajwal Chauhan a PhD student at NYU Tandon school of Engineering. Welcome to My Personal wiki page. If you are interested in understanding the math behind Machine Learning and Artificial intelligence or if you are just looking for a quick refresher on certain topics in them, stay tuned. I am uploading very simple explanations of these topics along with some new models that I come across. This page will be updated with content according to my reading habits so sometimes it'll be something that I recently read or came across other times it'll be something basic that I just did a refresher on (some of the things might not be the State-of-the-art but still new to me, so I will still try to explain them in my own simple terms). I hope you enjoy reading, cause it's going to be goooood. Here is a little bit more about myself:</p> <p>What I\u2019m into right now: Neural operators, physics-informed deep learning, and spatiotemporal ML. I like taking mathy ideas and making them useful for real mobility problems like traffic + pedestrian flow, trajectory prediction, and sim + RL for autonomy.</p> <p>Where I\u2019ve been: I\u2019m doing my ph.d. in transportation systems at nyu tandon (\u201923\u2013\u201927-ish). Before that I did my masters at Indian Institute of technology Kanpur and Bachelors at Gbpiet pauri. Lots of civil/transport roots, now very ML-heavy.</p> <p>Recent work: Solving PDEs with Neural Operators, showing when they work and when they don't and proposing our own Neural Operator models that outperforms SOTA models. Traffic-state estimation using physics-informed deep learning on traffic datasets. Fun result: matched adaptive smoothing method level accuracy with way less compute using PIDL.</p>"},{"location":"#publications","title":"Publications","text":"<ul> <li> <p>Understanding the mechanism of lane changing process and dynamics using microscopic traffic data \u2014 we built/cleaned NGSIM data for lane changes, engineered features, and validated the patterns. (Physica A, 2022) paper \u00b7 doi:10.1016/j.physa.2022.126981</p> </li> <li> <p>Neural operators struggle to learn complex PDEs in pedestrian mobility: Hughes model case study \u2014 we built solid benchmarks (Godunov + Wavefront Tracking), ran a bunch of operator baselines, and shared what actually works and what doesn't. (Artificial Intelligence for Transportation, 2025) paper \u00b7 doi:10.1016/j.ait.2025.100005</p> </li> <li> <p>Monte Carlo-Type Neural Operator for Differential Equations \u2014 Built a lightweight, surprisingly capable way to solve the Kernel integral operator in the neural operator framework using MC. Workshop/Conference versions are out at Neurips 2025 (ML4PS) and ICMLA 2025 respectively.  arXiv:2510.05620</p> </li> <li> <p>Backpressure-based Mean-field Type Game for Scheduling in Multi-Hop Wireless Sensor Networks \u2014 accepted to EUSIPCO 2025. arXiv:2506.03059</p> </li> </ul> <p>Tools I reach for: python (pytorch, jupyter, matplotlib), docker, git, and SUMO for sims. I\u2019m a \u201ckeep it simple, measure it, ship it\u201d person.</p> <p>Say Hi / Follow along: github: pj1911 \u00b7 linkedin: prajwalchauhan \u00b7 email: pc3377@nyu.edu</p>"},{"location":"Fuzzy%20Inference%20Systems/","title":"Fuzzy Inference Systems","text":""},{"location":"Fuzzy%20Inference%20Systems/#introduction","title":"Introduction","text":"<p>Fuzzy inference systems sound mysterious if we are hearing about 'fuzzy' things for the first time. In reality, they are just a way to let computers handle vague, human-style reasoning like 'temperature is warm' or 'speed is a bit high'. This chapter builds up to fuzzy inference systems, starting with classical sets and logic, then introducing fuzzy sets, and finally showing how fuzzy rules come together into a full fuzzy inference system.</p>"},{"location":"Fuzzy%20Inference%20Systems/#why-care-about-fuzziness","title":"Why care about fuzziness?","text":"<p>Let us start with an everyday question: Is this coffee hot? A typical answer is not just ''yes'' or ''no''. we might say:</p> <ul> <li>''It's lukewarm'' or ''It's kind of hot, but we can drink it.''</li> </ul> <p>Human language is full of words like tall, young, expensive, close, fast, etc. These are not crisp categories with sharp boundaries. Classical logic and classical sets, however, are based on sharp yes/no decisions: either \\(x\\) is in the set \\(A\\), or it is not. Fuzzy logic and fuzzy inference systems were developed to bridge this gap: they allow us to mathematically model concepts like 'kind of hot' or 'very tall' and use them in reasoning and control. Before we get to fuzzy stuff, let us quickly recall how classical sets and logic work, and why they struggle with vagueness.</p>"},{"location":"Fuzzy%20Inference%20Systems/#classical-sets-and-logic-crisp-world","title":"Classical sets and logic (crisp world)","text":""},{"location":"Fuzzy%20Inference%20Systems/#crisp-sets","title":"Crisp sets","text":"<p>Consider some collection of objects we care about, called the universe of discourse and usually denoted by \\(X\\). For example:</p> \\[ X = \\{\\text{all real temperatures between } 0^\\circ\\mathrm{C} \\text{ and } 100^\\circ\\mathrm{C}\\}. \\] <p>A crisp set \\(A\\) is a subset of \\(X\\). Membership is all-or-nothing: an element \\(x \\in X\\) either belongs to \\(A\\) or it does not. We can express this with an indicator function (also called a characteristic function):</p> \\[ \\chi_A(x) = \\begin{cases} 1 &amp; \\text{if } x \\in A,\\\\ 0 &amp; \\text{if } x \\notin A. \\end{cases} \\] <p>For example, let \\(X\\) be temperatures and define the set</p> \\[ A = \\{\\text{temperatures considered 'hot'}\\}. \\] <p>In classical set theory, we must draw a sharp line: maybe \\(A = \\{ x \\in X : x \\ge 60^\\circ\\mathrm{C} \\}\\). Then:</p> \\[ \\chi_A(59.9^\\circ\\mathrm{C}) = 0, \\quad \\chi_A(60^\\circ\\mathrm{C}) = 1. \\]"},{"location":"Fuzzy%20Inference%20Systems/#classical-logic","title":"Classical logic","text":"<p>Classical logic deals with propositions that are either true or false. We use operations like: NOT (\\(\\neg\\)), AND (\\(\\land\\)), OR (\\(\\lor\\)). If \\(P\\) and \\(Q\\) are true/false statements, then:</p> <ul> <li>\\(\\neg P\\) is true when \\(P\\) is false.</li> <li>\\(P \\land Q\\) is true when both \\(P\\) and \\(Q\\) are true.</li> <li>\\(P \\lor Q\\) is true when at least one of them is true.</li> </ul> <p>This matches the crisp set view nicely: being in a set is a boolean property, and combining sets (intersection, union, complement) mirrors these logical operations.</p>"},{"location":"Fuzzy%20Inference%20Systems/#where-crisp-logic-struggles","title":"Where crisp logic struggles","text":"<p>The problem shows up with vague concepts: 'Is Alice tall?'. If we are forced to answer strictly yes/no, we:</p> <ul> <li>either draw an arbitrary boundary (e.g., 'tall if height \\(\\ge 170\\)cm'), or</li> <li>end up disagreeing, because different people choose different boundaries.</li> </ul> <p>There is no natural, universally accepted sharp cut-off where 'not tall' suddenly becomes 'tall'. The concept itself is inherently gradual. This is where fuzzy sets and fuzzy logic come in.</p>"},{"location":"Fuzzy%20Inference%20Systems/#vagueness-vs-randomness","title":"Vagueness vs randomness","text":"<p>Before introducing fuzziness, it is useful to distinguish it from randomness. These are different kinds of uncertainty. Probability theory deals with randomness or lack of knowledge about which outcome will occur. For example, we toss a fair coin. We do not know if it will land heads or tails, but after the toss it is definitely either heads or tails. A probability \\(P(\\text{heads}) = 0.5\\) does not mean the coin is 'half-heads and half-tails'. It means, roughly, that in the long run we will see heads about half the time.</p>"},{"location":"Fuzzy%20Inference%20Systems/#vagueness-fuzziness","title":"Vagueness (fuzziness)","text":"<p>Consider the question: \"Is a \\(168\\) cm person tall?\". There is no randomness here. We are not uncertain about the height, we already know it exactly. The uncertainty is about how well this precise height fits a vague category like 'tall'. Instead of forcing a yes/no answer, fuzzy logic lets us assign a number that says how well something fits a vague description.  For instance, we might say:</p> \\[ \\text{\"Tall\"}(168\\text{ cm}) = 0.6, \\] <p>meaning '168 cm is tall to degree \\(0.6\\)'. This is a different numerical quantity than a probability. With that distinction in mind, we can now define fuzzy sets.</p>"},{"location":"Fuzzy%20Inference%20Systems/#fuzzy-sets-gradual-membership","title":"Fuzzy sets: gradual membership","text":""},{"location":"Fuzzy%20Inference%20Systems/#from-indicator-functions-to-membership-functions","title":"From indicator functions to membership functions","text":"<p>Recall, if \\(X\\) is a set defined on temperatures and A is a subset of it, defined as</p> \\[ A = \\{\\text{temperatures considered \"hot\"}\\}. \\] <p>In classical set theory, we must draw a sharp line: maybe \\(A = \\{ x \\in X : x \\ge 60^\\circ\\mathrm{C} \\}\\). We can express this with an indicator or membership function:</p> \\[ \\chi_A(x) = \\begin{cases} 1 &amp; \\text{if } x \\in A,\\\\ 0 &amp; \\text{if } x \\notin A. \\end{cases} \\] <p>But, a fuzzy set \\(A\\) on \\(X\\) is defined by a membership function:</p> \\[ \\mu_A : X \\to [0,1]. \\] <p>For each element \\(x \\in X\\), the value \\(\\mu_A(x)\\) tells us the degree to which \\(x\\) belongs to the fuzzy set \\(A\\):</p> <ul> <li>\\(\\mu_A(x) = 0\\) means 'definitely not in \\(A\\)',</li> <li>\\(\\mu_A(x) = 1\\) means 'fully in \\(A\\)',</li> <li>values between \\(0\\) and \\(1\\) represent partial membership.</li> </ul>"},{"location":"Fuzzy%20Inference%20Systems/#basic-fuzzy-set-operations","title":"Basic fuzzy set operations","text":"<p>Given fuzzy sets \\(A\\) and \\(B\\) with membership functions \\(\\mu_A(x)\\) and \\(\\mu_B(x)\\), we can define operations analogous to union, intersection, and complement. One common choice is:</p> <ul> <li>Complement:</li> </ul> \\[ \\mu_{\\neg A}(x) = 1 - \\mu_A(x). \\] <ul> <li>Intersection (AND):</li> </ul> \\[ \\mu_{A \\cap B}(x) = \\min\\big(\\mu_A(x), \\mu_B(x)\\big). \\] <ul> <li>Union (OR):</li> </ul> \\[ \\mu_{A \\cup B}(x) = \\max\\big(\\mu_A(x), \\mu_B(x)\\big). \\] <p>These are not the only possible definitions, but they are intuitive and widely used:</p> <ul> <li>For '\\(x\\) is in both \\(A\\) and \\(B\\)', the degree is limited by the smaller membership.</li> <li>For '\\(x\\) is in \\(A\\) or \\(B\\)', the degree is given by the larger membership.</li> </ul> <p>So far, we have only talked about static fuzzy sets. To build fuzzy inference systems, we need to connect fuzzy sets with variables and rules.</p>"},{"location":"Fuzzy%20Inference%20Systems/#linguistic-variables-and-fuzzy-rules","title":"Linguistic variables and fuzzy rules","text":""},{"location":"Fuzzy%20Inference%20Systems/#linguistic-variables","title":"Linguistic variables","text":"<p>A linguistic variable is a variable whose values are words (or short phrases) rather than numbers. For example, instead of writing Temperature \\(= 25^\\circ\\mathrm{C}\\), we might describe it as</p> \\[ \\texttt{Temperature} \\in \\{\\text{'cold'}, \\text{'cool'}, \\text{'warm'}, \\text{'hot'}\\}. \\] <p>Each word here is not just a label, it corresponds to a fuzzy set on the underlying numeric domain. Concretely:</p> <ul> <li>the universe of discourse is the numeric range of possible temperatures (e.g. \\(0\\)--\\(100^\\circ\\mathrm{C}\\)),</li> <li>each linguistic term ('cold', 'cool', 'warm', 'hot', etc.) is represented by its own fuzzy set over this range.</li> </ul> <p>For example, 'cold' might be modeled so that</p> \\[ \\mu_{\\text{cold}}(10^\\circ\\mathrm{C}) = 1,\\quad \\mu_{\\text{cold}}(15^\\circ\\mathrm{C}) = 0.5,\\quad \\mu_{\\text{cold}}(25^\\circ\\mathrm{C}) = 0, \\] <p>which means \\(10^\\circ\\)C is fully cold, \\(15^\\circ\\)C is 'half cold', and \\(25^\\circ\\)C is not cold at all.</p> <p>Thus a single numeric value, such as \\(25^\\circ\\mathrm{C}\\), can belong to several of these fuzzy sets at once, but with different degrees of membership.</p>"},{"location":"Fuzzy%20Inference%20Systems/#fuzzy-if-then-rules","title":"Fuzzy IF-THEN rules","text":"<p>Now we can express knowledge in a very natural form using fuzzy sets. For instance, in a simple temperature control scenario, we might want to decide how fast a fan should run based on how warm the room feels. A human would say something like: IF temperature is warm THEN fan speed is medium. More formally, this is a fuzzy IF-THEN rule of the form:</p> \\[ \\text{IF } x \\text{ is } A \\text{ THEN } y \\text{ is } B. \\] <p>Here:</p> <ul> <li>\\(x\\) is an input variable (e.g., temperature),</li> <li>\\(A\\) is a fuzzy set on the \\(x\\)-domain (e.g., 'warm'),</li> <li>\\(y\\) is an output variable (e.g., fan speed),</li> <li>\\(B\\) is a fuzzy set on the \\(y\\)-domain (e.g., 'medium').</li> </ul> <p>A fuzzy inference system will take a collection of such rules, combine them, and produce an output given specific inputs. To understand how, we need to look at the structure of a fuzzy inference system.</p>"},{"location":"Fuzzy%20Inference%20Systems/#what-is-a-fuzzy-inference-system","title":"What is a fuzzy inference system?","text":"<p>A fuzzy inference system (FIS) is a mapping from inputs to outputs using fuzzy logic. It usually has the following components:</p> <ol> <li>Fuzzification: convert numerical inputs into degrees of membership in fuzzy sets.</li> <li>Rule evaluation (inference): compute how strongly each fuzzy rule is activated by the current inputs.</li> <li>Aggregation: combine the effects of all rules to obtain fuzzy output sets.</li> <li>Defuzzification: convert the fuzzy output sets back into crisp numerical outputs.</li> </ol> <p>We now walk through one concrete fuzzy inference system from start to finish.</p> <p>Setup. We use two inputs and one output:</p> <ul> <li>Temperature \\(T\\) in \\([0,40]\\) (\\(^\\circ\\mathrm{C}\\)).</li> <li>Humidity \\(H\\) in \\([0,100]\\) (percent).</li> <li>Fan speed \\(S\\) in \\([0,10]\\) (arbitrary units).</li> </ul> <p>For \\(T\\) we use two fuzzy sets:</p> \\[ \\text{\"cool\"}, \\quad \\text{\"warm\"}, \\] <p>for \\(H\\) we use two:</p> \\[ \\text{\"dry\"}, \\quad \\text{\"humid\"}, \\] <p>and for \\(S\\) we use three:</p> \\[ \\text{\"low\"}, \\quad \\text{\"medium\"}, \\quad \\text{\"high\"}. \\] <p>We will use the fixed input</p> \\[ T = 24^\\circ\\mathrm{C}, \\qquad H = 55\\%. \\] <p>Membership functions.</p> <p>Temperature.</p> \\[ \\mu_{\\text{cool}}(T) = \\begin{cases} 1, &amp; T \\le 18,\\\\ \\dfrac{28 - T}{10}, &amp; 18 &lt; T &lt; 28,\\\\ 0, &amp; T \\ge 28, \\end{cases} \\qquad \\mu_{\\text{warm}}(T) = \\begin{cases} 0, &amp; T \\le 20,\\\\ \\dfrac{T - 20}{10}, &amp; 20 &lt; T &lt; 30,\\\\ 1, &amp; T \\ge 30. \\end{cases} \\] <p>Humidity.</p> \\[ \\mu_{\\text{dry}}(H) = \\begin{cases} 1, &amp; H \\le 40,\\\\ \\dfrac{60 - H}{20}, &amp; 40 &lt; H &lt; 60,\\\\ 0, &amp; H \\ge 60, \\end{cases} \\qquad \\mu_{\\text{humid}}(H) = \\begin{cases} 0, &amp; H \\le 50,\\\\ \\dfrac{H - 50}{30}, &amp; 50 &lt; H &lt; 80,\\\\ 1, &amp; H \\ge 80. \\end{cases} \\] <p>Fan speed. (triangular shapes)</p> \\[ \\begin{aligned} \\mu_{\\text{low}}(S) &amp;= \\begin{cases} \\dfrac{S}{2}, &amp; 0 &lt; S \\le 2,\\\\ \\dfrac{4 - S}{2}, &amp; 2 &lt; S &lt; 4,\\\\ 0, &amp; \\text{otherwise}, \\end{cases} \\\\ \\mu_{\\text{medium}}(S) &amp;= \\begin{cases} \\dfrac{S - 3}{2}, &amp; 3 &lt; S \\le 5,\\\\ \\dfrac{7 - S}{2}, &amp; 5 &lt; S &lt; 7,\\\\ 0, &amp; \\text{otherwise}, \\end{cases} \\\\ \\mu_{\\text{high}}(S) &amp;= \\begin{cases} \\dfrac{S - 6}{2}, &amp; 6 &lt; S \\le 8,\\\\ \\dfrac{10 - S}{2}, &amp; 8 &lt; S &lt; 10,\\\\ 0, &amp; \\text{otherwise}. \\end{cases} \\end{aligned} \\] <p>Step 1: Fuzzification.</p> <p>For \\(T = 24\\):</p> \\[ \\mu_{\\text{cool}}(24) = \\frac{28 - 24}{10} = 0.4, \\qquad \\mu_{\\text{warm}}(24) = \\frac{24 - 20}{10} = 0.4. \\] <p>For \\(H = 55\\):</p> \\[ \\mu_{\\text{dry}}(55) = \\frac{60 - 55}{20} = 0.25, \\qquad \\mu_{\\text{humid}}(55) = \\frac{55 - 50}{30} \\approx 0.17. \\] <p>So the crisp input \\((T,H) = (24,55)\\) becomes the fuzzy description</p> \\[ \\mu_{\\text{cool}}(24) = 0.4, \\quad \\mu_{\\text{warm}}(24) = 0.4, \\quad \\mu_{\\text{dry}}(55) = 0.25, \\quad \\mu_{\\text{humid}}(55) \\approx 0.17. \\] <p>Step 2: Rule evaluation.</p> <p>We use three rules:</p> <ul> <li>R1: IF \\(T\\) is cool AND \\(H\\) is dry THEN \\(S\\) is low.</li> <li>R2: IF \\(T\\) is warm AND \\(H\\) is dry THEN \\(S\\) is medium.</li> <li>R3: IF \\(T\\) is warm AND \\(H\\) is humid THEN \\(S\\) is high.</li> </ul> <p>We take AND as minimum (discussed above). The firing strength \\(\\alpha_k\\) of rule \\(k\\) is then given by:</p> \\[ \\begin{aligned} \\alpha_1 &amp;= \\min\\big(\\mu_{\\text{cool}}(24), \\mu_{\\text{dry}}(55)\\big)         = \\min(0.4, 0.25) = 0.25,\\\\ \\alpha_2 &amp;= \\min\\big(\\mu_{\\text{warm}}(24), \\mu_{\\text{dry}}(55)\\big)         = \\min(0.4, 0.25) = 0.25,\\\\ \\alpha_3 &amp;= \\min\\big(\\mu_{\\text{warm}}(24), \\mu_{\\text{humid}}(55)\\big)         = \\min(0.4, 0.17) \\approx 0.17. \\end{aligned} \\] <p>So all three rules fire, with different strengths.</p> <p>Step 3: Aggregation of rule outputs.</p> <p>By this point, each rule \\(R_k\\) has:</p> <ul> <li>a firing strength \\(\\alpha_k \\in [0,1]\\) that says how true its IF-part is for the current input, and</li> <li>an output fuzzy set (low, medium and high) that tells, for each possible fan speed \\(S \\in [0,10]\\), how well that fan speed fits the label in the THEN-part if the rule were fully true (\\(\\mu_{\\text{low}}\\), \\(\\mu_{\\text{medium}}\\) and \\(\\mu_{\\text{high}}\\)).</li> </ul> <p>Step 3 answers the question: Given the firing strengths \\(\\alpha_k\\) of all rules and their corresponding output fuzzy sets (such as \\(\\mu_{\\text{low}}\\), \\(\\mu_{\\text{medium}}\\), and \\(\\mu_{\\text{high}}\\) over fan speeds \\(S\\)), how strongly does the rule base as a whole support each possible fan speed \\(S\\)? We do this in two stages:</p> <p>To keep things concrete, suppose from Step 2 we already have</p> \\[ \\alpha_1 = 0.25, \\qquad \\alpha_2 = 0.25, \\qquad \\alpha_3 = 0.17. \\] <p>Inside a single rule:</p> <p>Take one rule in isolation, for example</p> \\[ \\text{R2: IF } T \\text{ is warm AND } H \\text{ is dry THEN } S \\text{ is medium.} \\] <p>For the current input \\((T,H)\\), the IF-part (the antecedent) is not fully true, its truth is \\(\\alpha_2 = 0.25\\). The fuzzy set \\(\\mu_{\\text{medium}}(S)\\) tells us, for each \\(S\\), how 'medium' that speed would be if the rule were completely true. But in reality the rule is only true to degree \\(0.25\\), so its recommendation should be weakened accordingly. For each \\(S\\), we therefore combine:</p> <ul> <li>the degree to which \\(S\\) is 'medium': \\(\\mu_{\\text{medium}}(S)\\),</li> <li>the degree to which the rule is true: \\(\\alpha_2 = 0.25\\).</li> </ul> <p>The standard Mamdani choice (a classic and widely used fuzzy inference scheme, chosen here because it is simple, interpretable, and closely follows human IF-THEN rules) is</p> \\[ \\mu_{S|\\mathrm{R2}}(S) = \\min\\big(\\alpha_2, \\mu_{\\text{medium}}(S)\\big). \\] <p>For example, imagine that from the output membership function we know:</p> \\[ \\mu_{\\text{medium}}(4.5) = 0.8, \\qquad \\mu_{\\text{medium}}(3.0) = 0.2. \\] <p>Then R2's support for these two speeds becomes</p> \\[ \\mu_{S|\\mathrm{R2}}(4.5) = \\min(0.25, 0.8) = 0.25,\\qquad \\mu_{S|\\mathrm{R2}}(3.0) = \\min(0.25, 0.2) = 0.2. \\] <p>So:</p> <ul> <li>At \\(S=4.5\\), the rule would like this speed quite a lot ('medium' to degree \\(0.8\\)), but the rule itself is weak (\\(0.25\\)), so we cap the support at \\(0.25\\).</li> <li>At \\(S=3.0\\), the rule is equally weak (\\(0.25\\)) but \\(S=3.0\\) is barely 'medium' (\\(0.2\\)), so the support is only \\(0.2\\).</li> </ul> <p>In other words, the rule's support for a particular \\(S\\) is always the weaker of:</p> \\[     \"\\text{how true is the rule?}\" \\quad \\text{vs.} \\quad \"\\text{how well does } S \\text{ fit the label in the THEN-part?}\" \\] <p>This minimum operator also has two nice properties:</p> <ul> <li>If everything is crisp (\\(\\alpha_2 \\in \\{0,1\\}\\) and \\(\\mu_{\\text{medium}}(S) \\in \\{0,1\\}\\)), this reduces to ordinary logic: the consequent is true only when both the rule fires and \\(S\\) satisfies the label.</li> <li>Geometrically, it corresponds to 'cutting' the top of the fuzzy set \\(\\mu_{\\text{medium}}(S)\\) at height \\(\\alpha_2\\). When plotted, each rule produces a 'truncated hill' over the output axis.</li> </ul> <p>We do the same thing for every rule, so that each fuzzy IF-THEN rule contributes its own truncated output membership function, i.e. its own partial, weighted opinion about \\(S\\). Formally, in our example:</p> \\[ \\begin{aligned} \\mu_{S|\\mathrm{R1}}(S) &amp;= \\min\\big(\\alpha_1, \\mu_{\\text{low}}(S)\\big),\\\\ \\mu_{S|\\mathrm{R2}}(S) &amp;= \\min\\big(\\alpha_2, \\mu_{\\text{medium}}(S)\\big),\\\\ \\mu_{S|\\mathrm{R3}}(S) &amp;= \\min\\big(\\alpha_3, \\mu_{\\text{high}}(S)\\big). \\end{aligned} \\] <p>Across rules:</p> <p>Now we fix a particular output value \\(S^\\star\\) (say \\(S^\\star=4.5\\)). Suppose the truncated fuzzy sets from the three rules give:</p> \\[ \\mu_{S|\\mathrm{R1}}(4.5) = 0.10, \\qquad \\mu_{S|\\mathrm{R2}}(4.5) = 0.25, \\qquad \\mu_{S|\\mathrm{R3}}(4.5) = 0.05. \\] <p>This means:</p> <ul> <li>Rule 1 says: '\\(S^\\star\\) is low' to degree \\(0.10\\).</li> <li>Rule 2 says: '\\(S^\\star\\) is medium' to degree \\(0.25\\).</li> <li>Rule 3 says: '\\(S^\\star\\) is high' to degree \\(0.05\\).</li> </ul> <p>The question is: overall, how acceptable is \\(S^\\star\\) as an output according to the whole rule base? The intuitive answer is: \\(S^\\star\\) is acceptable if at least one rule supports it, and the more strongly any rule supports it, the more acceptable it is. This is a fuzzy version of logical OR over rules. In standard fuzzy logic, OR is modeled by the maximum of the truth degrees, so we take:</p> \\[ \\mu_{\\text{out}}(S^\\star) = \\max\\big(\\mu_{S|\\mathrm{R1}}(S^\\star), \\mu_{S|\\mathrm{R2}}(S^\\star), \\mu_{S|\\mathrm{R3}}(S^\\star)\\big) = \\max(0.10, 0.25, 0.05) = 0.25. \\] <p>Doing this for every \\(S\\) gives the final aggregated output fuzzy set:</p> \\[ \\mu_{\\text{out}}(S) = \\max\\big(\\mu_{S|\\mathrm{R1}}(S), \\mu_{S|\\mathrm{R2}}(S), \\mu_{S|\\mathrm{R3}}(S)\\big). \\] <p>Geometrically, this means we place all the truncated hills from the individual rules on the same axis and, at each \\(S\\), take the highest one. The resulting outline is the combined fuzzy belief about \\(S\\) after considering all rules.</p> <p>Why this min-max scheme, and what else is possible?</p> <p>The min-inside / max-across pattern is the classic Mamdani choice because:</p> <ul> <li>it is easy to interpret: each rule's influence is limited by its truth, and the system accepts an output if any rule supports it.</li> <li>it matches crisp logic when all memberships are \\(0\\) or \\(1\\).</li> <li>it is very easy to visualize and implement.</li> </ul> <p>However, it is not the only design:</p> <ul> <li>Instead of \\(\\min(\\alpha_k, \\mu(S))\\) inside a rule, one can use product \\(\\alpha_k \\cdot \\mu(S)\\). For example, if \\(\\alpha_2 = 0.25\\) and \\(\\mu_{\\text{medium}}(4.5) = 0.8\\), product inference would give \\(0.25 \\cdot 0.8 = 0.20\\) instead of \\(\\min(0.25,0.8)=0.25\\), smoothly scaling the fuzzy set rather than flat-cutting it.</li> <li>Instead of combining rules with \\(\\max\\), one can use other aggregation operators that behave like a soft OR and give slightly different shapes to \\(\\mu_{\\text{out}}(S)\\).</li> <li>We also have another type of system known as Sugeno-type systems, the consequents are not fuzzy sets at all but crisp functions of the inputs. For example, each rule might have</li> </ul> \\[ \\text{R}_k: \\ \\text{IF (conditions)} \\ \\text{THEN } S = f_k(T,H) = a_k T + b_k H + c_k. \\] <p>After computing the firing strengths \\(\\alpha_k\\) of all rules, the final output is a weighted average of these rule outputs:</p> \\[ S^\\ast = \\frac{\\sum_k \\alpha_k \\, f_k(T,H)}{\\sum_k \\alpha_k}. \\] <p>So if two rules fire with \\(\\alpha_1 = 0.6\\), \\(\\alpha_2 = 0.4\\) and give \\(f_1(T,H)=5\\), \\(f_2(T,H)=8\\), then</p> \\[ S^\\ast = \\frac{0.6 \\cdot 5 + 0.4 \\cdot 8}{0.6 + 0.4} = 6.2. \\] <p>We stick to the Mamdani min-max style here because it gives a clean first mental model: each rule draws a fuzzy bump over the output axis, cut at the level of its truth, and the system as a whole takes the upper envelope of all these bumps as its final fuzzy recommendation.</p> <p>Step 4: Defuzzification.</p> <p>To obtain a single crisp fan speed, we use the centroid (center of gravity) of \\(\\mu_{\\text{out}}\\):</p> \\[ S^\\ast = \\frac{\\displaystyle \\int_0^{10} S \\, \\mu_{\\text{out}}(S)\\, dS}      {\\displaystyle \\int_0^{10}     \\mu_{\\text{out}}(S)\\, dS}. \\] <p>For the specific shapes and firing strengths above, this centroid is numerically around</p> \\[ S^\\ast \\approx 4.7. \\] <p>So, given \\(T = 24^\\circ\\mathrm{C}\\) and \\(H = 55\\%\\), the fuzzy inference system suggests a fan speed a bit below the center of 'medium'. Each step (fuzzification, rule evaluation, aggregation, defuzzification) is just an explicit, numerical way of turning vague rules into a concrete control action.</p>"},{"location":"Fuzzy%20Inference%20Systems/#when-are-fuzzy-inference-systems-useful","title":"When are fuzzy inference systems useful?","text":"<p>Fuzzy inference systems shine in situations where:</p> <ul> <li>Human expertise is available in the form of fuzzy rules like 'IF \\(A\\) is high AND \\(B\\) is low THEN ...'.</li> <li>The system is too complex or poorly understood to model precisely with differential equations or detailed physics.</li> <li>Inputs and outputs can be reasonably described using linguistic terms (e.g., control systems, decision support, heuristic policies).</li> </ul> <p>They are common in:</p> <ul> <li>Consumer products (e.g., washing machines, cameras, air conditioners).</li> <li>Control and automation.</li> <li>Decision-making and ranking systems.</li> </ul>"},{"location":"Fuzzy%20Inference%20Systems/#limitations","title":"Limitations","text":"<p>Fuzzy inference systems also have limitations:</p> <ul> <li>Designing good membership functions and rules can be somewhat artful and domain-specific.</li> <li>They do not magically 'learn' from data unless combined with learning methods (e.g., neuro-fuzzy systems).</li> <li>For very high-dimensional problems with many inputs, the rule base can explode combinatorially.</li> </ul>"},{"location":"Fuzzy%20Inference%20Systems/#wrapping-up","title":"Wrapping up","text":"<p>We started from classical sets and logic, where membership and truth are crisp, and saw that they struggle with inherently vague concepts like 'tall' or 'warm'. Fuzzy sets extend the idea of set membership to degrees in \\([0,1]\\), which can capture how well a specific value fits a vague category. Using fuzzy sets, we defined linguistic variables and fuzzy IF-THEN rules. A fuzzy inference system then:</p> <ol> <li>fuzzifies numerical inputs,</li> <li>evaluates fuzzy rules,</li> <li>aggregates their fuzzy outputs,</li> <li>and defuzzifies the aggregated result to get a crisp output.</li> </ol> <p>The result is a system that can reason in a way that resembles human use of language.</p>"},{"location":"Generative%20Adversarial%20Networks/","title":"Generative Adversarial Networks","text":""},{"location":"Generative%20Adversarial%20Networks/#generative-modeling-problem","title":"Generative modeling problem","text":"<p>The generative modeling problem (GMP) is the task of learning a probability distribution from data so that we can generate new, realistic samples that look like they came from the real dataset. GANs are a kind of AI algorithm that are designed to solve GMPs. They are especially famous for how good they are at creating realistic, high-resolution images.</p>"},{"location":"Generative%20Adversarial%20Networks/#generative-modeling","title":"Generative modeling","text":"<p>Generative modeling is an unsupervised approach where we observe samples \\(\\mathbf{x}\\) from unknown distribution (\\(\\mathbf{x} \\sim p_{\\text{data}}(\\mathbf{x})\\)) and learn a model \\(p_{\\text{model}}(\\mathbf{x})\\) that matches this distribution. We choose a parametric form \\(p_{\\text{model}}(\\mathbf{x}; \\theta)\\) and fit \\(\\theta\\) so \\(p_{\\text{model}}\\) resembles \\(p_{\\text{data}}\\), typically by maximum likelihood, i.e., minimizing \\(\\mathrm{KL}\\big(p_{\\text{data}} \\,\\|\\, p_{\\text{model}}\\big)\\). </p> <p>In many applications, we are interested in conditional generative models of the form \\(p(\\mathbf{x} \\mid \\mathbf{c}, \\mathbf{w})\\), where \\(\\mathbf{c}\\) is a vector of conditioning variables. For instance, in a generative model for animal images, \\(\\mathbf{c}\\) can indicate which animal we want, such as a cat or a dog, so the model generates images that matches the chosen class.</p>"},{"location":"Generative%20Adversarial%20Networks/#common-issues","title":"Common issues","text":"<p>Explicit density models work nicely in classic statistics, where we use simple distributions over a few variables. In modern deep learning, however, we often use complex neural networks, and their exact density can be intractable. People have mostly tried two fixes: (1) design models with tractable densities, or (2) use approximate methods to learn intractable ones. Both are hard and still struggle on tasks like generating realistic high-resolution images.</p> <p>To be concrete, sometimes we do not specify \\(p_{\\text{model}}(\\mathbf{x}; \\theta)\\) directly. Instead, we introduce a latent variable \\(\\mathbf{z}\\): a hidden variable that we never observe in the data but that describes how \\(\\mathbf{x}\\) is generated. We choose a simple prior over \\(\\mathbf{z}\\), for example \\(p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{z} \\mid 0, I)\\), and a nonlinear generator function \\(\\mathbf{x} = g(\\mathbf{z}, \\theta)\\) given by a neural network (introduced in the next section). We can then generate samples by first drawing \\(\\mathbf{z} \\sim p(\\mathbf{z})\\) and then setting \\(\\mathbf{x} = g(\\mathbf{z}, \\theta)\\). In this way, the distribution over \\(\\mathbf{x}\\) is defined implicitly, meaning it is specified by the sampling procedure rather than by an explicit closed-form formula.</p> <p>The model defines a joint distribution</p> \\[ p(\\mathbf{x}, \\mathbf{z}; \\theta) = p(\\mathbf{z})\\, p(\\mathbf{x} \\mid \\mathbf{z}; \\theta). \\] <p>The marginal distribution over \\(\\mathbf{x}\\) is obtained by marginalizing out the latent variable \\(\\mathbf{z}\\) and using the continuous version of the law of total probability:</p> \\[ p_{\\text{model}}(\\mathbf{x}; \\theta) = \\int p(\\mathbf{x}, \\mathbf{z}; \\theta)\\, d\\mathbf{z} = \\int p(\\mathbf{z})\\, p(\\mathbf{x} \\mid \\mathbf{z}; \\theta)\\, d\\mathbf{z}. \\] <p>For a deterministic generator, once \\(\\mathbf{z}\\) and \\(\\theta\\) are fixed, \\(\\mathbf{x}\\) is completely determined by \\(\\mathbf{x} = g(\\mathbf{z}, \\theta)\\). This means that, conditioned on \\(\\mathbf{z}\\), all probability mass is concentrated at the point \\(g(\\mathbf{z}, \\theta)\\). In continuous space, this is written using a Dirac delta:</p> \\[ p(\\mathbf{x} \\mid \\mathbf{z}; \\theta) = \\delta\\big(\\mathbf{x} - g(\\mathbf{z}, \\theta)\\big). \\] <p>Plugging this into the marginalization formula gives</p> \\[ p_{\\text{model}}(\\mathbf{x}; \\theta) = \\int p(\\mathbf{z})\\, \\delta\\big(\\mathbf{x} - g(\\mathbf{z}, \\theta)\\big)\\, d\\mathbf{z}. \\] <p>For a general deep nonlinear \\(g\\), this integral has no closed-form solution, so \\(p_{\\text{model}}(\\mathbf{x}; \\theta)\\) is intractable, and we cannot directly optimize \\(\\theta\\) using maximum likelihood.</p>"},{"location":"Generative%20Adversarial%20Networks/#fix-learn-sampling-procedure-directly","title":"Fix: Learn sampling procedure directly","text":"<p>An alternative to these explicit density models is to skip the tractable density entirely and learn only a tractable sampling procedure. These are called implicit generative models and GANs belong to this family. Before GANs, the leading deep implicit model was the generative stochastic network, which produced approximate samples via a Markov chain. GANs were proposed to instead generate high-quality samples in a single step, avoiding the incremental and approximate nature of Markov-chain sampling.</p>"},{"location":"Generative%20Adversarial%20Networks/#generative-adversarial-networks_1","title":"Generative adversarial networks","text":"<p>GANs are built in the game-theory sense, a game between two models, usually neural networks, which are trained jointly and where the second network (discriminator) provides a training signal to update the weights of the first network (generator). The generator, implicitly defines \\(p_{\\text{model}}(\\mathbf{x})\\). In general, it cannot compute this density as we saw in the previous section, but it can draw samples from it. The generator starts from a simple prior \\(p(\\mathbf{z})\\) over a latent vector \\(\\mathbf{z}\\) (for example, a multivariate Gaussian or a uniform distribution over a hypercube). A sample \\(\\mathbf{z} \\sim p(\\mathbf{z})\\) is just noise. The generator is a function \\(G(\\mathbf{z}; \\theta_G)\\) that learns to transform this noise into realistic samples, with \\(\\theta_G\\) representing its learnable parameters or \u201cstrategy\u2019\u2019 in the game.</p> <p>The other player, the discriminator, looks at a sample \\(\\mathbf{x}\\) and outputs a score \\(D(\\mathbf{x}; \\theta_D)\\) estimating whether \\(\\mathbf{x}\\) is real (from the training data) or fake (from \\(p_{\\text{model}}\\), via the generator). In the original GAN, this score is the probability that \\(\\mathbf{x}\\) is real, assuming real and fake examples are shown equally often.</p> <p>Each player has its own cost: \\(J_G(\\theta_G, \\theta_D)\\) for the generator and \\(J_D(\\theta_G, \\theta_D)\\) for the discriminator, and each tries to minimize its own cost. The discriminator\u2019s cost pushes it to classify correctly and the generator\u2019s cost pushes it to make fake samples that the discriminator classifies as real working against each other, hence the name 'Adversarial'.</p>"},{"location":"Generative%20Adversarial%20Networks/#loss-for-generator-and-discriminator","title":"Loss for generator and discriminator","text":"<p>In the original GAN, the discriminator sees two kinds of examples: real data \\(\\mathbf{x} \\sim p_{\\text{data}}\\) with label 1, and generated data \\(G(\\mathbf{z})\\) with label 0.</p> <p>We interpret the discriminator output as the probability that a data point \\(\\mathbf{x}\\) is real:</p> \\[ P(target = 1 \\mid \\mathbf{x}) = D(\\mathbf{x}; \\theta_D). \\] <p>Its loss is just the usual binary cross-entropy:</p> \\[ J_D(\\theta_D, \\theta_G) = - \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}} \\big[\\log D(\\mathbf{x})\\big]   - \\mathbb{E}_{\\mathbf{z} \\sim p(\\mathbf{z})} \\big[\\log \\big(1 - D(G(\\mathbf{z}))\\big)\\big]. \\] <p>The first term says \u201creal samples should have \\(D(\\mathbf{x})\\) close to 1,\u201d and the second says \u201cfake samples should have \\(D(G(\\mathbf{z}))\\) close to 0.\u201d So \\(D\\) is trained exactly like a standard real-vs-fake classifier. For the generator, two options are usually proposed:</p> <ul> <li> <p>Minimax GAN (M-GAN): \\(J_G = -J_D\\), giving a clean minimax game.     This makes GAN training a standard zero-sum game: whenever the     discriminator gets better, the generator \u201closes,\u201d and vice versa., and at equilibrium this     setup corresponds to minimizing a well-defined divergence between     \\(p_{\\text{data}}\\) and \\(p_{\\text{model}}\\).</p> </li> <li> <p>Non-saturating GAN (NS-GAN): In the original minimax version, the generator minimizes</p> </li> </ul> \\[ J_G^{\\text{minimax}}(\\theta_G, \\theta_D) = \\mathbb{E}_{\\mathbf{z} \\sim p(\\mathbf{z})}   \\big[\\log\\big(1 - D(G(\\mathbf{z}))\\big)\\big], \\] <p>By minimizing \\(\\log(1 - D(G(\\mathbf{z})))\\), the generator effectively tries to make \\(D(G(\\mathbf{z}))\\) large (so its fakes look real). However, early in training we typically have \\(D(G(\\mathbf{z})) \\approx 0\\). In this regime the sigmoid in \\(D\\) is saturated, so \\(\\partial D(G(\\mathbf{z}))/\\partial \\theta_G \\approx 0\\), and the gradient of \\(\\log(1 - D(G(\\mathbf{z})))\\) with respect to \\(\\theta_G\\) is very small. As a result, the generator receives almost no learning signal.</p> <p>In NS-GAN, we \u201cflip the labels\u2019\u2019 for the generator: it acts as if its fake samples were real and tries to push \\(D(G(\\mathbf{z}))\\) toward 1. Its loss is</p> \\[ J_G^{\\text{NS}}(\\theta_G, \\theta_D) = - \\mathbb{E}_{\\mathbf{z} \\sim p(\\mathbf{z})}   \\big[\\log D(G(\\mathbf{z}))\\big], \\] <p>the negative log-likelihood of the \u201creal\u2019\u2019 label for fake samples. This gives stronger gradients when the discriminator is confident (i.e., \\(D(G(\\mathbf{z}))\\) is small), so the generator keeps learning instead of getting stuck.</p> <p>NS-GAN is often preferred in practice because it helps avoid gradient saturation during training.</p> <p>We can think of GANs like counterfeiters and police. The generator is the counterfeiter, making fake money and the discriminator is the police, trying to catch fakes while letting real money through. As they compete, the fakes get better and better until, in the ideal case, the police can no longer tell real from fake. The twist is that, in GANs, the generator learns from the discriminator\u2019s gradient, as if the counterfeiters had a mole inside the police force explaining exactly how they spot fakes.</p>"},{"location":"Generative%20Adversarial%20Networks/#how-to-train-gans","title":"How to train GANs","text":"<p>GANs use game-theoretic ideas in a challenging setting: the losses are non-convex, and both actions and policies live in continuous, high-dimensional spaces (whether we view an action as choosing parameters \\(\\theta_G\\) or as producing a sample \\(\\mathbf{x}\\)). The learning goal is to reach a local Nash equilibrium: a point where each player\u2019s loss is at a local minimum with respect to its own parameters. In such a state, with only small (local) changes and holding the other player fixed, no player can further reduce its loss.</p> <p>The most common way to train a GAN is simple: use a gradient-based optimizer to update both players\u2019 parameters in turn, each trying to reduce its own loss. When this works well, the trained generator can produce very realistic samples, even for complex datasets with high-resolution images. A high-level reason GANs can be so effective is that they avoid many of the approximations used in other generative models. We never have to approximate an intractable density, instead, we directly train the generator to fool the discriminator. The main sources of error are then just statistical (finite data) and optimization-related (not reaching the ideal equilibrium), rather than additional approximation errors from Markov chains, variational bounds, and so on.</p>"},{"location":"Generative%20Adversarial%20Networks/#convergence-of-gans","title":"Convergence of GANs","text":"<p>Convergence for a GAN means that training settles into a stable state: the generator\u2019s samples and the discriminator\u2019s predictions stop changing (up to small noise), and neither player can improve its loss by making a small change in its parameters. The original GAN paper gave two key (idealized) results regarding this:</p> <ol> <li>In the space of all possible density functions \\(p_{\\text{model}}\\) and         discriminators \\(D\\), there is only one local Nash equilibrium:         the point where the model matches the data perfectly,         \\(p_{\\text{model}} = p_{\\text{data}}\\).</li> <li>If we had an ideal optimizer that, for any fixed \\(p_{\\text{model}}\\),         could find the best possible discriminator \\(D^\\ast\\), then the         following loop would converge to that equilibrium:<ol> <li>fix \\(p_{\\text{model}}\\) and optimize \\(D\\) all the way to             \\(D^\\ast\\);</li> <li> <p>then take a small gradient step on \\(p_{\\text{model}}\\) to             reduce its loss, keeping \\(D^\\ast\\) fixed.</p> <p>Repeating these steps would eventually make \\(p_{\\text{model}}\\) equal to \\(p_{\\text{data}}\\).</p> </li> </ol> </li> </ol> <p>In practice, things are messier. We do not move directly in the space of all distributions. Instead, both \\(p_{\\text{model}}\\) and \\(D\\) are neural networks with finitely many parameters, trained with noisy, alternating gradient steps. The losses are highly non-convex, and each update of one player changes the landscape seen by the other. Because of this, training can oscillate, diverge, or collapse to poor solutions rather than neatly settling to the nice equilibrium guaranteed in the idealized theory.</p>"},{"location":"Generative%20Adversarial%20Networks/#references","title":"References","text":"<ul> <li>Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., ... &amp; Bengio, Y. (2020). Generative adversarial networks. Communications of the ACM, 63(11), 139-144.</li> </ul>"},{"location":"Hamilton-Jacobi%20equations/","title":"1 - Hamilton Jacobi Equation","text":""},{"location":"Hamilton-Jacobi%20equations/#introduction","title":"Introduction","text":"<p>A first-order PDE is an equation for an unknown function \\(u\\) of several variables where the highest derivatives that appear are only first derivatives. For \\(u=u(x,t)\\), the most general way to write such an equation is</p> \\[ F\\!\\big(x,t,\\,u(x,t),\\,u_x(x,t),\\,u_t(x,t)\\big)=0, \\] <p>meaning: at each point \\((x,t)\\), if we substitute the numbers \\(x,t,u,u_x,u_t\\) into the function \\(F\\), the result must be zero. This is called writing the PDE implicitly, as in this case the equation is not arranged to isolate any derivatives, it is just a constraint relating the variables and derivatives. Sometimes, we can rearrange the same PDE into an explicit form where one derivative is isolated, for example</p> \\[ u_t = G(x,t,u,u_x) \\qquad\\text{or equivalently}\\qquad u_t + a(x,t,u)\\,u_x = b(x,t,u), \\] <p>but this rearrangement is not always possible globally (or it may become messy or ambiguous), so the implicit form is the most general starting point. </p>"},{"location":"Hamilton-Jacobi%20equations/#implicit-function-theorem","title":"Implicit function theorem","text":"<p>But an implicit first-order PDE cannot always be rewritten in the explicit form. For example, say we have the implicit form</p> \\[ F(x,t,u,u_x,u_t)=0. \\] <p>For many initial-value problems we would like to isolate the time derivative and write an ``evolution law''</p> \\[ u_t = G(x,t,u,u_x), \\] <p>meaning: once \\(x,t,u,u_x\\) are given, the equation selects one value of \\(u_t\\) (at least locally). The key tool for deciding when an implicit PDE can be solved locally for a chosen derivative (or can be written explicitly) is the Implicit Function Theorem (IFT) (we do not prove it here). But, for a general understanding, let \\(F\\) be smooth and suppose at \\((x_0,t_0)\\) we have</p> \\[ F(x_0,t_0,u_0,p_0,q_0)=0, \\qquad u_0=u(x_0,t_0),\\; p_0=u_x(x_0,t_0),\\; q_0=u_t(x_0,t_0). \\] <p>If, in addition,</p> \\[ \\frac{\\partial F}{\\partial u_t}(x_0,t_0,u_0,p_0,q_0)\\neq 0, \\] <p>then the IFT guarantees that near this point one can solve uniquely for \\(u_t\\) as a smooth function of the other variables. This is plausible because if we freeze \\((x,t,u,p)\\) and view</p> \\[ \\phi(q)=F(x,t,u,p,q) \\] <p>as a one-variable function. At the base point, \\(\\phi(q_0)=0\\) (because \\(F(...)=0\\)) and \\(\\phi'(q_0)\\neq 0\\), so \\(\\phi\\) crosses \\(0\\) transversely and is locally monotone, hence the zero is isolated and unique. Small perturbations of \\((x,t,u,p)\\) change \\(\\phi\\) only slightly, so a nearby zero persist, the nonvanishing of \\(\\partial F/\\partial u_t\\) keeps the crossing monotone, preserving uniqueness. Consequently, by IFT there is a smooth function \\(G\\) such that, on some neighborhood of the base point \\((x_0,t_0,u_0,p_0,q_0)\\), the equation \\(F(x,t,u,p,q)=0\\) can be solved uniquely for \\(q=u_t\\) in terms of the other variables, i.e. \\(q=G(x,t,u,p)\\). In other words, for \\((x,t,u,u_x)\\) sufficiently close to \\((x_0,t_0,u_0,p_0)\\), there exists exactly one \\(u_t\\) near \\(q_0\\) satisfying \\(F=0\\), and this \\(u_t\\) depends smoothly on \\((x,t,u,u_x)\\), so locally</p> \\[ F(x,t,u,u_x,u_t)=0 \\quad\\Longleftrightarrow\\quad u_t=G(x,t,u,u_x). \\] <p>If the condition fails, uniqueness (or even existence) can break. For example,</p> \\[ u_t^2-u_x=0 \\quad\\Rightarrow\\quad u_t=\\pm\\sqrt{u_x}, \\] <p>so there are two branches for \\(u_x&gt;0\\), no real solutions for \\(u_x&lt;0\\), and branching occurs precisely where \\(\\frac{\\partial F}{\\partial u_t}=2u_t=0\\).</p>"},{"location":"Hamilton-Jacobi%20equations/#initial-value-problem-characteristics-curves","title":"Initial value problem: Characteristics curves","text":"<p>Once a first-order PDE is in an explicit (or at least locally explicit) form, the most common problem type is an initial value problem (Cauchy problem): we provide the value of the unknown along an initial curve and ask the PDE to extend it away from that curve. For \\(u=u(x,t)\\) a standard explicit model is the quasi-linear equation</p> \\[ u_t + a(x,t,u)\\,u_x = b(x,t,u), \\] <p>together with initial data at (say) \\(t=0\\),</p> \\[ u(x,0)=\\phi(x). \\] <p>Now a simple question arises why is this not just a \u201cplug and solve\u201d type of equation? Because the PDE does not directly tell us \\(u\\). It tells us how \\(u\\) must change when we move in \\((x,t)\\) while keeping the relation between the partial slopes \\(u_t\\) and \\(u_x\\).</p> <p>The key observation here is that the particular combination \\(u_t+a\\,u_x\\) has a very concrete meaning: it is the directional derivative of \\(u\\) in the \\((x,t)\\)-plane along a direction that tilts with slope \\(a\\). To see this, pick any differentiable curve in the \\((x,t)\\)-plane, written as \\(t\\mapsto (x(t),t)\\). Along this curve we can track how the value of the field changes by defining a single-variable function</p> \\[ U(t):=u(x(t),t). \\] <p>By the ordinary chain rule,</p> \\[ \\frac{dU}{dt}=\\frac{d}{dt}u(x(t),t)=u_x(x(t),t)\\,x'(t)+u_t(x(t),t). \\] <p>So the rate of change of \\(u\\) that an observer would measure while moving along the curve is</p> \\[ \\frac{d}{dt}u(x(t),t)=u_t+ x'(t)\\,u_x. \\] <p>Now compare this with the PDE term \\(u_t+a\\,u_x\\): if we choose the curve so that its slope satisfies</p> \\[ x'(t)=a(x(t),t,u(x(t),t)), \\] <p>then the observed change along that curve becomes exactly</p> \\[ \\frac{d}{dt}u(x(t),t)=u_t+a\\,u_x. \\] <p>This is why, along those special curves, a PDE like \\(u_t+a\\,u_x=b\\) turns into the single ODE \\(\\frac{d}{dt}u=b\\).  Indeed, along any differentiable curve \\(t\\mapsto (x(t),t)\\), if we choose \\(x'(t)=a(x,t,u)\\), the PDE becomes the ODE</p> \\[ \\frac{d}{dt}u(x(t),t)=b(x,t,u). \\] <p>Thus the PDE can be attacked by finding special curves in the \\((x,t)\\)-plane (later called characteristics) that start on the initial line \\(t=0\\) and carry the initial values \\(\\phi(x)\\) forward by an ODE.</p>"},{"location":"Hamilton-Jacobi%20equations/#from-a-pde-to-odes-along-characteristic-curves","title":"From a PDE to ODEs along characteristic curves","text":"<p>The previous identity suggests a strategy: instead of trying to solve for \\(u(x,t)\\) everywhere at once, we try to trace curves in the \\((x,t)\\)-plane along which the PDE becomes an ODE. Let's take an example of the quasi-linear PDE</p> \\[ u_t + a(x,t,u)\\,u_x = b(x,t,u), \\qquad u(x,0)=\\phi(x). \\] <p>Pick a starting point on the initial line, say \\(x(t=0)=\\xi\\) then \\(u(\\xi,0)=\\phi(\\xi)\\). Now, look for a curve \\(t\\mapsto (x(t),t)\\) and the value of the solution along it, \\(t\\mapsto u(t):=u(x(t),t)\\), such that the PDE holds along that curve. Using the chain rule,</p> \\[ \\frac{d}{dt}u(x(t),t)=u_t(x(t),t)+x'(t)\\,u_x(x(t),t). \\] <p>If we choose the curve so that</p> \\[ x'(t)=a\\big(x(t),t,u(t)\\big), \\] <p>then substituting into the chain rule and using the PDE gives</p> \\[ \\frac{d}{dt}u(t)=b\\big(x(t),t,u(t)\\big), \\qquad u(0)=\\phi(\\xi). \\] <p>So each initial point \\(\\xi\\) generates a pair of coupled ODEs,</p> \\[ \\begin{cases} x'(t)=a(x,t,u), &amp; x(0)=\\xi,\\\\ u'(t)=b(x,t,u), &amp; u(0)=\\phi(\\xi). \\end{cases} \\] <p>Solving the characteristic ODEs does not immediately give \\(u\\) as an explicit function of \\((x,t)\\). Instead it gives a family of curves and values, indexed by the starting point \\((\\xi)\\) on the initial line \\((t=0)\\). For each fixed \\(\\xi\\) we solve the ODEs and obtain</p> \\[ t\\mapsto x(t;\\xi), \\qquad t\\mapsto u(t;\\xi), \\] <p>with initial values</p> \\[ x(0;\\xi)=\\xi, \\qquad u(0;\\xi)=\\phi(\\xi). \\] <p>Thus, for a given \\(\\xi\\), the curve \\(t \\longmapsto (x(t;\\xi),\\,t)\\)  is a characteristic in the \\((x,t)\\)-plane and \\(u(t;\\xi)\\) is the value of the solution along that characteristic. To convert this Lagrangian/parametric description into the usual Eulerian form \\(u=u(x,t)\\), we must be able to solve for the label \\(\\xi\\) from the relation</p> \\[ x = x(t;\\xi). \\] <p>Equivalently, we need the map</p> \\[ (\\xi,t)\\ \\longmapsto\\ (x(t;\\xi),\\,t) \\] <p>to be locally invertible, so that \\(\\xi=\\xi(x,t)\\) and then</p> \\[ u(x,t)=u\\bigl(t;\\xi(x,t)\\bigr). \\] <p>In practice this means: for a given point \\((x,t)\\), there should be exactly one label \\(\\xi\\) such that \\(x=x(t;\\xi)\\). If such a unique \\(\\xi=\\xi(x,t)\\) exists, we define the solution by</p> \\[ u(x,t)=u\\big(t;\\xi(x,t)\\big). \\] <p>If instead two different labels \\(\\xi_1\\neq \\xi_2\\) produce the same point \\((x,t)\\) (i.e. \\(x(t;\\xi_1)=x(t;\\xi_2)\\)), then two characteristic curves ``arrive'' at the same location carrying possibly different values of \\(u\\), so the PDE cannot have a single-valued classical solution there, this is exactly where crossing characteristics signal the breakdown of smooth solutions (and motivates weak solutions/shocks in conservation laws).</p>"},{"location":"Hamilton-Jacobi%20equations/#example-the-transport-equation-why-characteristics-feel-natural","title":"Example: the transport equation (why characteristics feel natural)","text":"<p>To see the method working in the simplest possible setting, consider the constant-coefficient first-order PDE</p> \\[ u_t + c\\,u_x = 0, \\qquad u(x,0)=\\phi(x), \\] <p>where \\(c\\) is a constant (a fixed ``speed''). Here \\(a(x,t,u)=c\\) and \\(b(x,t,u)=0\\), so the characteristic ODEs become</p> \\[ x'(t)=c,\\quad x(0)=\\xi \\qquad\\Rightarrow\\qquad x(t;\\xi)=\\xi+ct, \\] <p>and</p> \\[ u'(t)=0,\\quad u(0)=\\phi(\\xi) \\qquad\\Rightarrow\\qquad u(t;\\xi)=\\phi(\\xi)\\ \\ \\text{(constant along the curve).} \\] <p>So each initial point \\(\\xi\\) launches a straight line in the \\((x,t)\\)-plane, \\(x=\\xi+ct\\), and a constant solution value is simply carried along that line. To write the answer as \\(u(x,t)\\), solve \\(x=\\xi+ct\\) for the label: \\(\\xi=x-ct\\). Substituting gives the explicit solution</p> \\[ u(x,t)=\\phi(x-ct). \\] <p>This formula is worth pausing on: the entire initial profile \\(\\phi\\) is translated to the right if \\(c&gt;0\\) (left if \\(c&lt;0\\)) at speed \\(|c|\\), without changing shape. In other words, the PDE does not \u201ccreate\u201d new values, it moves the data along characteristic lines from the initial points, which is exactly the geometric content hidden inside the derivatives \\(u_t\\) and \\(u_x\\).</p>"},{"location":"Hamilton-Jacobi%20equations/#nonlinearity-when-characteristics-can-collide","title":"Nonlinearity: when characteristics can collide","text":"<p>The transport example worked smoothly because the characteristic speed \\(c\\) was constant, so straight characteristic lines never intersect. Things become more interesting (and more realistic) when the speed depends on the solution itself. Consider</p> \\[ u_t+u\\,u_x=0,\\qquad u(x,0)=\\phi(x), \\] <p>the inviscid Burgers equation. Along a characteristic curve \\(t\\mapsto (x(t),t),\\) we track the solution value on that curve by</p> \\[ U(t):=u(x(t),t). \\] <p>Which is the same as the case in the previous section, hence comparing gives: \\(a(x,t,u)=u\\) and \\(b(x,t,u)=0\\), so the pair of characteristic ODEs become</p> \\[ \\frac{dx}{dt}=u(t),\\qquad \\frac{du}{dt}=0. \\] <p>with initial conditions \\(x(0)=\\xi\\) and \\(u(\\xi,0)=\\phi(\\xi)\\). Since \\(u'(t)=0\\), the carried value along each characteristic is constant (vary across different initial points \\(\\xi\\)):</p> \\[ u(t;\\xi)=\\phi(\\xi). \\] <p>Then the characteristic curve itself has slope determined by that constant value \\(\\xi\\):</p> \\[ x'(t)=\\phi(\\xi)\\quad\\Rightarrow\\quad x(t;\\xi)=x(0;\\xi)+\\phi(\\xi)\\,t=\\xi+\\phi(\\xi)\\,t. \\] <p>So different parts of the initial profile move at different speeds: the point starting at \\(\\xi\\) moves with speed \\(\\phi(\\xi)\\). If \\(\\phi\\) is larger somewhere, that region travels faster and can catch up to slower regions ahead; mathematically this means that the map \\(\\xi\\mapsto x(t;\\xi)\\) can stop being one-to-one. Differentiating,</p> \\[ \\frac{\\partial x}{\\partial \\xi}(t;\\xi)=1+\\phi'(\\xi)\\,t. \\] <p>When this derivative becomes zero for some \\((t,\\xi)\\), two nearby labels \\(\\xi\\) get mapped to the same \\(x\\), i.e. characteristics collide. After the earliest collision the PDE cannot have a single-valued smooth solution without changing the notion of solution (this is where shocks/weak solutions enter).</p>"},{"location":"Hamilton-Jacobi%20equations/#general-first-order-pdes-and-the-full-characteristic-system","title":"General first-order PDEs and the full characteristic system","text":"<p>The equations we solved so far had the special quasi-linear form \\(u_t+a(x,t,u)u_x=b(x,t,u)\\). A fully general first-order PDE may depend on \\(u_x\\) and \\(u_t\\) in a nonlinear way,</p> \\[ F(x,t,u,u_x,u_t)=0. \\] <p>In this setting, choosing \\(x'(t)\\) cleverly is not enough, because the PDE does not give \\(u_t\\) as a single linear combination of \\(u_x\\), instead, the geometry of the constraint \\(F=0\\) tells us how \\((x,t,u)\\) and the derivatives \\((u_x,u_t)\\) must move together. The method of characteristics introduces additional unknowns to track the derivatives along a curve:</p> \\[ p(t):=u_x(x(t),t),\\qquad q(t):=u_t(x(t),t), \\] <p>so that along the curve we have the constraint</p> \\[ F\\big(x(t),t,u(t),p(t),q(t)\\big)=0, \\] <p>in addition to the chain rule identities (showing how the solution varies along the characteristic curve and the speed of that curve)</p> \\[ \\frac{d}{dt}u(t)=u_t(x(t),t)+x'(t)\\,u_x(x(t),t)=p(t)\\,x'(t)+q(t),\\qquad \\frac{d}{dt}x(t)=x'(t). \\] <p>The key idea is to choose \\(x'(t)\\) (hence the curve) so that the constraint \\(F=0\\) remains true as we move. This yields a closed system of ODEs for</p> \\[ (x(t),\\,t,\\,u(t),\\,p(t),\\,q(t)), \\qquad p(t)=u_x(x(t),t),\\ \\ q(t)=u_t(x(t),t). \\] <p>This is the ``full'' characteristic method: we evolve the point \\((x,t,u)\\) together with its local slopes \\((p,q)\\) in a way that keeps us on the solution surface \\(F=0\\). Unlike the initial-value construction, we are not parameterizing characteristics by a starting label \\(\\xi\\) here, we simply follow a single characteristic trajectory determined by an initial state for \\((x,t,u,p,q)\\).</p>"},{"location":"Hamilton-Jacobi%20equations/#moving-to-several-space-variables-the-step-needed-for-hamilton-jacobi","title":"Moving to several space variables (the step needed for Hamilton-Jacobi)","text":"<p>So far we used one space variable \\(x\\) and one time variable \\(t\\). For Hamilton-Jacobi we almost always need many space variables, so let \\(x\\in\\mathbb{R}^n\\) and write the unknown as \\(u(x,t)\\). Then the first derivatives are</p> \\[ u_t=\\frac{\\partial u}{\\partial t},\\qquad \\nabla u=\\left(\\frac{\\partial u}{\\partial x_1},\\dots,\\frac{\\partial u}{\\partial x_n}\\right), \\] <p>and a general first-order PDE becomes</p> \\[ F\\big(x,t,u,\\nabla u,u_t\\big)=0. \\] <p>Let \\(s\\in\\mathbb{R}\\) be a parameter (think of it as ``time along the characteristic'') and consider a characteristic curve \\(s\\longmapsto (x(s),t(s)).\\) Along a characteristic, \\(t(s)\\) need not be strictly increasing (it could even stay constant), so we introduce this independent parameter \\(s\\) that always parametrizes progress along the curve and lets us write the characteristic ODEs uniformly.</p> <p>The method of characteristics still says: solve the PDE by following the curves \\((x(s),t(s))\\)  carrying the solution value \\(u(s)=u(x(s),t(s))\\) along them. But now \\(x(s)\\) is a vector curve and the \u201cslope\u201d variable becomes a vector</p> \\[ p(s):=\\nabla_x u(x(s),t(s))\\in\\mathbb{R}^n,\\qquad q(s):=u_t(x(s),t(s))\\in\\mathbb{R}. \\] <p>The same chain rule idea becomes</p> \\[ \\frac{du}{ds}=p\\cdot \\frac{dx}{ds}+q\\,\\frac{dt}{ds}, \\] <p>where \\(\\cdot\\) is the dot product. Along the curve the PDE becomes the constraint</p> \\[ F\\big(x(s),t(s),u(s),p(s),q(s)\\big)=0. \\] <p>Differentiating this constraint with respect to \\(s\\) gives:</p> \\[ 0=\\frac{d}{ds}F =F_x\\cdot x' + F_t\\,t' + F_u\\,u' + F_p\\cdot p' + F_q\\,q'. \\] <p>Then, a standard choice for the curve direction is</p> \\[ \\frac{dx}{ds}= x'=F_p\\in\\mathbb{R}^n,\\qquad \\frac{dt}{ds}=t'=F_q\\in\\mathbb{R}, \\] <p>and the remaining ODEs for \\((u,p,q)\\) given by</p> \\[ u'=p\\cdot x' + q\\,t' = p\\cdot F_p + q\\,F_q. \\] \\[ p'=-F_x - F_u\\,p,\\qquad q'=-F_t - F_u\\,q. \\] <p>evolve  so that the constraint \\(F=0\\) stays true along the curve. The key point is: even when \\(x\\in\\mathbb{R}^n\\), a characteristic is still a single parametrized path \\(s\\longmapsto (x(s),t(s))\\), so it's still \\(1\\)-dimensional. To actually solve the PDE, we lift this path to the variables</p> \\[ s\\longmapsto \\big(x(s),t(s),u(s),p(s),q(s)\\big), \\] <p>and evolve them by their respective characteristic ODEs.  The vector \\(p(s)=\\nabla_x u(x(s),t(s))\\) records the spatial slopes of \\(u\\) along the path, allowing the PDE condition \\(F=0\\) to determine how \\((x,t,u,p,q)\\) move together.</p>"},{"location":"Hamilton-Jacobi%20equations/#the-hamilton-jacobi-equation-as-a-special-first-order-pde","title":"The Hamilton-Jacobi equation as a special first-order PDE","text":"<p>With the multi-variable setup in place, we can now introduce the main object. The Hamilton-Jacobi (HJ) equation is a first-order PDE for a scalar function \\(S(x,t)\\) (often called the action or phase) of the form</p> \\[ S_t + H\\!\\big(x,\\nabla S,t\\big)=0, \\qquad x\\in\\mathbb{R}^n, \\] <p>together with an initial condition such as</p> \\[ S(x,0)=S_0(x). \\] <p>Here \\(H\\) is a given function (the Hamiltonian), and the PDE is typically nonlinear because \\(\\nabla S\\) appears inside \\(H\\).</p>"},{"location":"Hamilton-Jacobi%20equations/#characteristics-of-hamilton-jacobi","title":"Characteristics of Hamilton-Jacobi","text":"<p>Take the Hamilton-Jacobi PDE</p> \\[ S_t+H(x,\\nabla S,t)=0, \\qquad p:=\\nabla S,\\ q:=S_t, \\] <p>and rewrite it as the constraint</p> \\[ F(x,t,S,p,q):=q+H(x,p,t)=0. \\] <p>Now plug this specific \\(F\\) into the symmetric characteristic system</p> \\[ \\frac{dx}{ds}=F_p,\\quad \\frac{dt}{ds}=F_q,\\quad \\frac{dS}{ds}=p\\cdot F_p+qF_q,\\quad \\frac{dp}{ds}=-(F_x+pF_S),\\quad \\frac{dq}{ds}=-(F_t+qF_S). \\] <p>Because \\(F=q+H(x,p,t)\\) we have</p> \\[ F_p=H_p,\\qquad F_q=1,\\qquad F_x=H_x,\\qquad F_t=H_t,\\qquad F_S=0. \\] <p>So \\(dt/ds=1\\), meaning we can take \\(s=t\\). The characteristic ODEs become the clean system</p> \\[ \\frac{dx}{dt}=H_p(x,p,t),\\qquad \\frac{dp}{dt}=-\\,H_x(x,p,t), \\] <p>which are exactly Hamilton's equations (with \\(x\\) and \\(p\\) evolving along characteristics). The action value \\(S\\) is then obtained by integrating along the same curve:</p> \\[ \\frac{dS}{dt}=p\\cdot \\frac{dx}{dt}+q = p\\cdot H_p(x,p,t)+q. \\] <p>Finally, the constraint \\(F=0\\) says \\(q=-H(x,p,t)\\), so</p> \\[ \\frac{dS}{dt}=p\\cdot H_p(x,p,t)-H(x,p,t). \\] <p>In other words: to solve the HJ PDE, we (i) solve the ODEs for \\((x(t),p(t))\\), and then (ii) recover \\(S\\) by a single line integral along those characteristic curves.  To solve the Hamilton-Jacobi initial value problem by characteristics, we launch one characteristic from each initial point \\(\\xi\\in\\mathbb{R}^n\\) on the surface \\(t=0\\), so \\(x(0)=\\xi\\). The initial spatial slope of the solution is fixed by the initial data, giving an initial momentum \\(p(0)=\\nabla S_0(\\xi)\\), and then \\((x(t),p(t))\\) evolve according to the characteristic ODEs determined by the Hamiltonian \\(H\\). This evolution defines a flow map \\(\\Phi_t:\\xi\\mapsto x(t;\\xi)\\). As long as \\(\\Phi_t\\) can be inverted (so each point \\(x\\) comes from a unique label \\(\\xi\\)), we can reconstruct the PDE solution \\(S(x,t)\\) by taking the value carried along the corresponding characteristic. The classical construction fails when the flow map folds i.e. when two different labels reach the same point, so the solution becomes multi-valued and one needs a generalized notion of solution beyond that time.</p>"},{"location":"Hyperbolic%20PDEs-1-weak%20solutions/","title":"Introduction - weak solution to hyperbolic PDEs","text":"<p>This chapter is part of a broader series on neural-network methods for approximating solutions of PDEs. Here we narrow the focus to hyperbolic PDEs and the core analytical issue they raise: solutions can develop discontinuities, so one must work with weak solutions (rather than classical smooth solutions).</p>"},{"location":"Hyperbolic%20PDEs-1-weak%20solutions/#partial-differential-equations","title":"Partial Differential Equations","text":"<p>Partial differential equations (PDEs) describe many physical and engineering systems, from traffic flow to electromagnetism, so solving them efficiently and accurately matters, in practice, many applications need results that are trustworthy, and the computation fitting within limited time and memory.</p> <p>A general time-dependent PDE can be written as</p> \\[ \\begin{equation} \\left\\lbrace \\begin{aligned} \\partial_t u + Lu &amp;= \\xi, &amp;&amp; (t,x)\\in [0,T]\\times \\mathcal U,\\\\ u(0,x) &amp;= u_0(x), &amp;&amp; x\\in \\mathcal U,\\\\ u(t,x) &amp;= g(t), &amp;&amp; (t,x)\\in [0,T]\\times \\partial\\mathcal U. \\end{aligned} \\right. \\end{equation} \\] <p>Here, \\(t\\in[0,T]\\) is the time variable, and \\(T&gt;0\\) is the final time up to which the evolution is studied. The spatial variable is \\(x\\in\\mathcal U\\subset\\mathbb R^d\\), where \\(\\mathcal U\\) is the region of interest (an interval in 1D, an area in 2D, or a volume in 3D), and \\(d\\) is the number of spatial dimensions. The set \\(\\partial\\mathcal U\\) denotes the boundary of \\(\\mathcal U\\) (endpoints in 1D, a curve in 2D, a surface in 3D). The unknown \\(u(t,x)\\) is the quantity being solved for, more generally,</p> \\[ u:[0,T]\\times\\mathcal U \\to \\mathbb R^n. \\] <p>The set \\([0,T]\\times\\mathcal U\\) is the space--time domain: the \\(\\times\\) means a Cartesian product, i.e., all pairs \\((t,x)\\) with \\(t\\in[0,T]\\) and \\(x\\in\\mathcal U\\). Here, \\(u\\) can be a scalar (\\(n=1\\), one value at each \\((t,x)\\)) or a vector-valued field (\\(n&gt;1\\), several coupled values at each \\((t,x)\\)). Furthermore, \\(\\partial_t u\\) denotes the partial derivative of \\(u\\) with respect to time: it measures how \\(u(t,x)\\) changes as \\(t\\) varies while keeping \\(x\\) fixed. The operator \\(L\\) is a spatial differential operator: for each fixed time \\(t\\), it takes the spatial function \\(u(t,\\cdot):\\mathcal U\\to\\mathbb R^n\\) as input and returns another function on \\(\\mathcal U\\), built from derivatives with respect to \\(x\\) (and possibly \\(x\\)-dependent coefficients), i.e. \\(L[u(t,\\cdot)]:\\mathcal U\\to\\mathbb R^n\\).  For example, \\(L\\) may contain</p> <ul> <li>Transport/advection terms (first derivatives), such as \\(a(x)\\cdot\\nabla u\\), which move the profile of \\(u\\)   through space at a velocity field \\(a(x)\\).</li> <li>Diffusion terms (second derivatives), such as \\(\\nabla\\!\\cdot\\!\\big(\\kappa(x)\\nabla u\\big)\\) (often written like \\(\\kappa(x)\\Delta u\\) in simple cases),   which spread and smooth \\(u\\) in space where the coefficient \\(\\kappa(x)\\) controls how strong this smoothing is.</li> <li>Reaction/zeroth-order terms (no derivatives), such as \\(c(x)u\\), which locally scale, damp, or amplify \\(u\\)   without moving it in space.</li> </ul> <p>The term \\(\\xi(t,x)\\) is an external input (a source/forcing): it represents effects not captured by transport, diffusion, or other spatial interactions. At each point \\((t,x)\\) it directly changes the value of \\(u(t,x)\\) by adding (a source) or removing (a sink) quantity. For example, if \\(u(t,x)\\) is temperature, then \\(\\xi(t,x)\\) can represent a heater embedded in the material: \\(\\xi(t,x)&gt;0\\) where the heater is active (heat is added there) and \\(\\xi(t,x)&lt;0\\) where a cooling device removes heat. In other words, \\(\\xi\\) is a modeled exchange with the environment (energy supplied or extracted). The initial condition \\(u(0,x)=u_0(x)\\) fixes the state at time \\(t=0\\) for every \\(x\\in\\mathcal U\\). The boundary condition \\(u(t,x)=g(t)\\) prescribes values of \\(u\\) on the boundary \\(x\\in\\partial\\mathcal U\\) for all \\(t\\in[0,T]\\).</p>"},{"location":"Hyperbolic%20PDEs-1-weak%20solutions/#solution-of-pdes","title":"Solution of PDEs","text":"<p>A function \\(u\\) is called a solution of the PDE if it satisfies the PDE together with the initial and boundary conditions in an appropriate mathematical sense. For smooth solutions, \\(u\\), the needed derivatives (like \\(\\partial_t u\\) and the spatial derivatives inside \\(Lu\\)) exist in the usual calculus sense, so the PDE holds at each point \\((t,x)\\), this is a classical solution.</p>"},{"location":"Hyperbolic%20PDEs-1-weak%20solutions/#weak-solutions","title":"Weak solutions","text":"<p>In hyperbolic PDEs where \\(u\\) is not smooth (for example, it has jump discontinuities), pointwise derivatives such as \\(\\partial_t u\\) or \\(\\nabla u\\) may not exist. In that case, the PDE is interpreted in a weak (integral) sense: Instead of requiring the PDE to hold at every single point, it is required to hold after being integrated against some smooth test functions. This is justifiable because integration only requires \\(u\\) to be integrable (not differentiable). Test functions can be chosen to be nonzero only inside a very small space--time region and zero everywhere else, so the integral checks the PDE balance inside that local neighborhood. Requiring the identity to hold for all such local tests forces the PDE to hold throughout the whole domain, while avoiding pointwise derivatives at jumps. More concretely, the weak form does not check \\(\\partial_t u + Lu = \\xi\\) pointwise, it checks that</p> \\[ \\int_{0}^{T}\\!\\!\\int_{\\mathcal U} \\big(\\partial_t u + Lu - \\xi\\big)\\,\\varphi \\,dx\\,dt = 0 \\quad \\text{for all smooth } \\varphi. \\] <p>Equivalently, we can define the PDE residual \\(r:=\\partial_t u+Lu-\\xi\\). The condition</p> \\[ \\int_{0}^{T}\\!\\!\\int_{\\mathcal U} r\\,\\varphi \\,dx\\,dt = 0 \\quad \\text{for all smooth } \\varphi \\] <p>means: no matter how a smooth weight \\(\\varphi\\) is chosen, the weighted integral of \\(r\\) is always zero. Intuitively, this means \\(r\\) must be zero when viewed through all test-function integrals (so it cannot be nonzero on any region without being detected by some choice of \\(\\varphi\\)). Why this is essentially the same as solving the PDE:</p> <ul> <li>If \\(u\\) is smooth, integration by parts turns the identity into the pointwise PDE, so nothing   is lost.</li> <li>If \\(u\\) is not smooth, \\(r\\) may not make sense pointwise, but the weighted average   \\(\\int r\\,\\varphi\\) still makes sense. Requiring the integral to vanish for every test function is a strong and useful condition:   if the residual were nonzero on any region of positive size, a test function supported there would   detect it, so the residual cannot ``hide'' anywhere in the domain.</li> </ul> <p>For example: take \\(\\varphi\\) to be a smooth bump that is \\(1\\) in a small space--time neighborhood and \\(0\\) outside a slightly larger neighborhood. Then the integral above mainly tests whether the PDE balances inside that neighborhood. Varying the bump over all locations and sizes enforces the balance everywhere, while avoiding undefined pointwise derivatives at jumps. For hyperbolic PDEs, there is an additional issue: even the definition of a solution must be handled carefully, since shocks can form and classical derivatives can fail. This motivates weak formulations and, to select the physically relevant weak solution, entropy conditions.</p>"},{"location":"Hyperbolic%20PDEs-1-weak%20solutions/#entropy-solutions","title":"Entropy solutions","text":"<p>Weak solutions are often not unique for hyperbolic conservation laws. A standard example is</p> \\[ \\partial_t u + \\nabla\\!\\cdot f(u)=0, \\] <p>where shocks (jumps) can form even from smooth initial data. The weak form alone may admit multiple candidates \\(u\\) that all satisfy the integral identity, so an extra rule is needed to pick the physically relevant one. For this an entropy solution is introduced, it is a weak solution that, in addition to weak solution property, satisfies a family of integral inequalities of the form \\(\\partial_t \\eta(u)+\\nabla\\!\\cdot q(u)\\le 0\\) (in the weak sense) for every convex entropy \\(\\eta\\) and its associated entropy flux \\(q\\).</p> <p>Here, an entropy is a convex scalar function \\(\\eta:\\mathbb R\\to\\mathbb R\\) applied to the state \\(u\\), where \\(u(t,x)\\) is a scalar value. An associated entropy flux is a function \\(q:\\mathbb R\\to\\mathbb R^d\\) (in \\(d\\) space dimensions) chosen to be compatible with the physical flux \\(f\\). Compatibility means that, for smooth solutions of the conservation law \\(\\partial_t u+\\nabla\\!\\cdot f(u)=0\\), the quantity \\(\\eta(u)\\) also satisfies a conservation law \\(\\partial_t \\eta(u)+\\nabla\\!\\cdot q(u)=0\\). To relate \\(\\eta(u)\\) and \\(q(u)\\), assume the 1D conservation law has a smooth solution \\(u(t,x)\\):</p> \\[ \\partial_t u + \\partial_x f(u)=0. \\] <p>Since \\(u\\) is smooth, the chain rule gives</p> \\[ \\partial_x f\\big(u(t,x)\\big)=f'\\big(u(t,x)\\big)\\,\\partial_x u(t,x). \\] <p>Equivalently,</p> \\[ \\frac{\\partial}{\\partial x} f\\big(u(t,x)\\big) =\\frac{df}{du}\\Big|_{u=u(t,x)}\\,\\frac{\\partial u}{\\partial x}(t,x). \\] <p>We can write it as, \\(\\partial_x f(u)=f'(u)\\,\\partial_x u\\), so</p> \\[ \\partial_t u + f'(u)\\,\\partial_x u = 0 \\quad\\Longrightarrow\\quad \\partial_t u = -f'(u)\\,\\partial_x u. \\] <p>Let \\(\\eta\\) be a smooth entropy and let \\(q\\) be an (unknown) flux depending only on \\(u\\). By the chain rule,</p> \\[ \\partial_t \\eta(u)=\\eta'(u)\\,\\partial_t u, \\qquad \\partial_x q(u)=q'(u)\\,\\partial_x u. \\] <p>Therefore,</p> \\[ \\partial_t \\eta(u)+\\partial_x q(u) = \\eta'(u)\\,\\partial_t u + q'(u)\\,\\partial_x u. \\] <p>Substitute \\(\\partial_t u = -f'(u)\\,\\partial_x u\\):</p> \\[ \\partial_t \\eta(u)+\\partial_x q(u) = \\eta'(u)\\big(-f'(u)\\,\\partial_x u\\big) + q'(u)\\,\\partial_x u = \\big(q'(u)-\\eta'(u)f'(u)\\big)\\,\\partial_x u. \\] <p>So, if \\(q\\) is chosen to satisfy</p> \\[ q'(u)=\\eta'(u)\\,f'(u), \\] <p>then \\(\\partial_t \\eta(u)+\\partial_x q(u)=0\\) for smooth solutions. Note, here \\(f'(u)\\) and \\(q'(u)\\) denote derivatives with respect to \\(u\\). In the scalar case, these are ordinary derivatives but in multiple spatial dimensions, \\(f'(u)\\) and \\(q'(u)\\) are \\(d\\)-dimensional vectors. Equivalently, \\(q\\) can be recovered (up to an additive constant) by</p> \\[ q(u)=\\int^{u}\\eta'(s)\\,f'(s)\\,ds. \\] <p>This derivation shows how to pair an entropy \\(\\eta\\) with the correct entropy flux \\(q\\): for smooth solutions, the original conservation law automatically implies a conservation law for \\(\\eta(u)\\)</p> \\[ \\partial_t \\eta(u) + \\nabla\\!\\cdot q(u)=0. \\] <p>This is the starting point for the entropy inequality used when shocks appear, where the same pair \\((\\eta,q)\\) is kept but the equality is relaxed to select the physically relevant weak solution. Therefore, across shocks, the weak form alone can accept multiple solutions. The entropy condition adds an extra rule: pick the solution where entropy does not increase across the shock.</p>"},{"location":"Hyperbolic%20PDEs-1-weak%20solutions/#traffic-intuition-lwr-model","title":"Traffic intuition (LWR model)","text":"<p>In \\(\\partial_t\\rho+\\partial_x f(\\rho)=0\\), shocks represent compression (cars brake and bunch up). Compression can happen abruptly, so a low\\(\\to\\)high density jump can be an admissible shock. The opposite transition, high\\(\\to\\)low density, is decompression: gaps open up gradually as information travels through the flow. A sudden high\\(\\to\\)low jump (a rarefaction shock) would require an instantaneous response, so it is ruled out by the entropy condition. Instead, the model produces a rarefaction fan, a smooth spreading wave.</p> <p>What the entropy inequality adds (and how it matches the intuition).</p> <p>The intuition above says: a compressive jump is allowed, but a decompressive jump is not. The entropy inequality is the mathematical way to enforce that idea. It introduces a scalar ``disorder'' measure \\(\\eta(\\rho)\\) (any convex function) and requires that this quantity cannot be created inside the road, except for what enters through the boundaries. Pick any convex entropy function \\(\\eta(\\rho)\\) and its associated entropy flux \\(q(\\rho)\\). An entropy solution satisfies</p> \\[ \\partial_t \\eta(\\rho) + \\partial_x q(\\rho)\\le 0 \\quad\\text{(in the weak sense).} \\] <p>This inequality rules out rarefaction shocks because those jumps would act like an internal source of entropy; compressive shocks are allowed because they act like dissipation. The inequality is easiest to read on a road segment \\([a,b]\\). Define the entropy content on \\([a,b]\\) by</p> \\[ E(t):=\\int_a^b \\eta(\\rho(t,x))\\,dx. \\] <p>Differentiate under the integral sign:</p> \\[ \\frac{d}{dt}E(t)=\\int_a^b \\partial_t \\eta(\\rho(t,x))\\,dx. \\] <p>Now using the entropy inequality</p> \\[ \\partial_t \\eta(\\rho) + \\partial_x q(\\rho)\\le 0 \\quad\\Longrightarrow\\quad \\partial_t \\eta(\\rho)\\le -\\partial_x q(\\rho), \\] <p>we get</p> \\[ \\frac{d}{dt}E(t)\\le \\int_a^b \\big(-\\partial_x q(\\rho(t,x))\\big)\\,dx. \\] <p>Finally, apply the fundamental theorem of calculus in \\(x\\):</p> \\[ \\int_a^b -\\partial_x q(\\rho(t,x))\\,dx = -\\Big[q(\\rho(t,x))\\Big]_{x=a}^{x=b} = q(\\rho(t,a)) - q(\\rho(t,b)). \\] <p>So,</p> \\[ \\frac{d}{dt}E(t)\\;\\le\\; q(\\rho(t,a)) - q(\\rho(t,b)). \\] <p>This makes the link to the intuition concrete: the only way entropy inside \\([a,b]\\) can increase is if more entropy enters at \\(x=a\\) than leaves at \\(x=b\\). If the boundaries are not injecting entropy, then \\(E(t)\\) cannot increase, meaning any shock inside the segment must be entropy-dissipating (compressive) rather than entropy-creating (a rarefaction shock).</p>"},{"location":"Hyperbolic%20PDEs-1-weak%20solutions/#understanding-the-inequality","title":"Understanding the inequality","text":"<p>The inequality \\(\\partial_t\\eta(u)+\\nabla\\!\\cdot q(u)\\le 0\\) is understood in the same weak (integral) way as before: derivatives are moved onto a smooth test function. Concretely, for every smooth nonnegative test function \\(\\varphi(t,x)\\ge 0\\) (often taken to vanish on the boundary and at \\(t=T\\)), start from the formal step (valid for smooth \\(u\\))</p> \\[ \\int_{0}^{T}\\!\\!\\int_{\\mathcal U}\\big(\\partial_t\\eta(u)+\\nabla\\!\\cdot q(u)\\big)\\,\\varphi \\,dx\\,dt \\le 0. \\] <p>A practical way to derive the weak (integral) identities is to start from a pointwise product rule, integrate it, and then rearrange terms.</p>"},{"location":"Hyperbolic%20PDEs-1-weak%20solutions/#integration-by-parts-in-time","title":"Integration by parts in time","text":"<p>Step 1 (pointwise identity). For smooth \\(a(t,x)\\) and \\(\\varphi(t,x)\\),</p> \\[ \\partial_t(a\\varphi)= (\\partial_t a)\\,\\varphi + a\\,\\partial_t\\varphi. \\] <p>Step 2 (integrate the identity). Since the equality holds at every point \\((t,x)\\), integrate both sides over the full space--time domain \\([0,T]\\times\\mathcal U\\):</p> \\[ \\int_{0}^{T}\\int_{\\mathcal U}\\partial_t\\big(a\\varphi\\big)\\,dx\\,dt = \\int_{0}^{T}\\int_{\\mathcal U}(\\partial_t a)\\,\\varphi\\,dx\\,dt + \\int_{0}^{T}\\int_{\\mathcal U}a\\,\\partial_t\\varphi\\,dx\\,dt. \\] <p>This turns a pointwise statement into a statement about averaged (integrated) quantities, which is the form needed later when derivatives of \\(u\\) may not exist pointwise.</p> <p>Step 3 (evaluate the left-hand side). Focus on</p> \\[ \\int_{0}^{T}\\!\\!\\int_{\\mathcal U}\\partial_t(a\\varphi)\\,dx\\,dt. \\] <p>First, fix \\(x\\) and integrate in time:</p> \\[ \\int_{0}^{T}\\partial_t(a\\varphi)(t,x)\\,dt = (a\\varphi)(T,x)-(a\\varphi)(0,x), \\] <p>by the fundamental theorem of calculus. Now integrate this result over \\(x\\in\\mathcal U\\):</p> \\[ \\int_{0}^{T}\\!\\!\\int_{\\mathcal U}\\partial_t(a\\varphi)\\,dx\\,dt = \\int_{\\mathcal U}(a\\varphi)(T,x)\\,dx-\\int_{\\mathcal U}(a\\varphi)(0,x)\\,dx = \\Big[\\int_{\\mathcal U}a\\,\\varphi\\,dx\\Big]_{t=0}^{t=T}. \\] <p>Step 4 (rearrange to isolate the term with \\(\\partial_t a\\)). Combine Step 2 and Step 3:</p> \\[ \\Big[\\int_{\\mathcal U}a\\,\\varphi\\,dx\\Big]_{t=0}^{t=T} =\\int_{0}^{T}\\!\\!\\int_{\\mathcal U}(\\partial_t a)\\,\\varphi\\,dx\\,dt +\\int_{0}^{T}\\!\\!\\int_{\\mathcal U}a\\,\\partial_t\\varphi\\,dx\\,dt. \\] <p>Now subtract the last integral from both sides:</p> \\[ \\int_{0}^{T}\\!\\!\\int_{\\mathcal U}(\\partial_t a)\\,\\varphi\\,dx\\,dt =\\Big[\\int_{\\mathcal U}a\\,\\varphi\\,dx\\Big]_{t=0}^{t=T} -\\int_{0}^{T}\\!\\!\\int_{\\mathcal U}a\\,\\partial_t\\varphi\\,dx\\,dt. \\] <p>This is the time integration-by-parts formula used in the weak form.</p>"},{"location":"Hyperbolic%20PDEs-1-weak%20solutions/#integration-by-parts-in-space","title":"Integration by parts in space","text":"<p>Step 1 (pointwise identity). For a smooth vector field \\(F(x)\\) and a smooth scalar test function \\(\\varphi(x)\\),</p> \\[ \\nabla\\!\\cdot(\\varphi F)= (\\nabla\\varphi)\\cdot F + \\varphi\\,(\\nabla\\!\\cdot F). \\] <p>Step 2 (integrate the identity over space). Integrate both sides over \\(x\\in\\mathcal U\\):</p> \\[ \\int_{\\mathcal U}\\nabla\\!\\cdot(\\varphi F)\\,dx = \\int_{\\mathcal U}(\\nabla\\varphi)\\cdot F\\,dx + \\int_{\\mathcal U}\\varphi\\,(\\nabla\\!\\cdot F)\\,dx. \\] <p>Step 3 (apply the divergence theorem to the left-hand side). The divergence theorem says</p> \\[ \\int_{\\mathcal U}\\nabla\\!\\cdot(\\varphi F)\\,dx = \\int_{\\partial\\mathcal U}\\varphi\\,F\\cdot n\\,dS, \\] <p>where \\(n(x)\\) is the outward unit normal vector on the boundary \\(\\partial\\mathcal U\\), and \\(dS\\) is the surface measure on \\(\\partial\\mathcal U\\) (a length element in 2D, a surface-area element in 3D).</p> <p>Step 4 (rearrange to isolate the term with \\(\\nabla\\!\\cdot F\\)). Combine Step 2 and Step 3:</p> \\[ \\int_{\\partial\\mathcal U}\\varphi\\,F\\cdot n\\,dS = \\int_{\\mathcal U}(\\nabla\\varphi)\\cdot F\\,dx + \\int_{\\mathcal U}\\varphi\\,(\\nabla\\!\\cdot F)\\,dx. \\] <p>Now move the \\((\\nabla\\varphi)\\cdot F\\) term to the other side:</p> \\[ \\int_{\\mathcal U}\\varphi\\,(\\nabla\\!\\cdot F)\\,dx = \\int_{\\partial\\mathcal U}\\varphi\\,F\\cdot n\\,dS - \\int_{\\mathcal U}F\\cdot\\nabla\\varphi\\,dx. \\] <p>Step 5 (extend to space-time). Apply the spatial identity at each fixed time \\(t\\), then integrate the whole equation over \\(t\\in[0,T]\\):</p> \\[ \\int_{0}^{T}\\int_{\\mathcal U}\\varphi\\,(\\nabla\\!\\cdot F)\\,dx\\,dt = \\int_{0}^{T}\\int_{\\partial\\mathcal U}\\varphi\\,F\\cdot n\\,dS\\,dt - \\int_{0}^{T}\\int_{\\mathcal U}F\\cdot\\nabla\\varphi\\,dx\\,dt. \\] <p>In the entropy setting, the choice is \\(F=q(u)\\), so</p> \\[ \\int_{0}^{T}\\int_{\\mathcal U}(\\nabla\\!\\cdot q(u))\\,\\varphi\\,dx\\,dt = \\int_{0}^{T}\\int_{\\partial\\mathcal U}\\varphi\\,q(u)\\cdot n\\,dS\\,dt - \\int_{0}^{T}\\int_{\\mathcal U}q(u)\\cdot\\nabla\\varphi\\,dx\\,dt. \\] <p>Step 6 (derive the weak entropy inequality).</p> <p>Start from the pointwise entropy inequality (formal, for smooth \\(u\\)):</p> \\[ \\partial_t\\eta(u)+\\nabla\\!\\cdot q(u)\\le 0. \\] <p>Multiply by a smooth nonnegative test function \\(\\varphi\\ge 0\\) and integrate over \\([0,T]\\times\\mathcal U\\):</p> \\[ \\int_{0}^{T}\\int_{\\mathcal U}\\big(\\partial_t\\eta(u)+\\nabla\\!\\cdot q(u)\\big)\\,\\varphi\\,dx\\,dt \\le 0. \\] <p>Now apply the two integration-by-parts results:</p> <ul> <li>For the time term \\(\\int\\!\\!\\int \\partial_t\\eta(u)\\,\\varphi\\), use the time formula with   \\(a=\\eta(u)\\).</li> <li>For the space term \\(\\int\\!\\!\\int (\\nabla\\!\\cdot q(u))\\,\\varphi\\), use the space formula with   \\(F=q(u)\\).</li> </ul> <p>Step 7 (Substituting the values)</p> <p>Start from</p> \\[ \\int_{0}^{T}\\int_{\\mathcal U}\\big(\\partial_t\\eta(u)+\\nabla\\!\\cdot q(u)\\big)\\,\\varphi\\,dx\\,dt \\le 0, \\qquad \\varphi\\ge 0. \\] <p>Split the two terms:</p> \\[ \\int_{0}^{T}\\int_{\\mathcal U}\\partial_t\\eta(u)\\,\\varphi\\,dx\\,dt \\;+\\; \\int_{0}^{T}\\int_{\\mathcal U}(\\nabla\\!\\cdot q(u))\\,\\varphi\\,dx\\,dt \\le 0. \\] <p>Time term (use time integration by parts with \\(a=\\eta(u)\\)).</p> \\[ \\int_{0}^{T}\\int_{\\mathcal U}\\partial_t\\eta(u)\\,\\varphi\\,dx\\,dt = \\Big[\\int_{\\mathcal U}\\eta(u)\\,\\varphi\\,dx\\Big]_{t=0}^{t=T} -\\int_{0}^{T}\\int_{\\mathcal U}\\eta(u)\\,\\partial_t\\varphi\\,dx\\,dt. \\] <p>If \\(\\varphi(T,\\cdot)=0\\), then \\(\\int_{\\mathcal U}\\eta(u(T,x))\\,\\varphi(T,x)\\,dx=0\\), hence</p> \\[ \\Big[\\int_{\\mathcal U}\\eta(u)\\,\\varphi\\,dx\\Big]_{t=0}^{t=T} = 0-\\int_{\\mathcal U}\\eta(u(0,x))\\,\\varphi(0,x)\\,dx = -\\int_{\\mathcal U}\\eta(u(0,x))\\,\\varphi(0,x)\\,dx. \\] <p>Now use the initial condition \\(u(0,x)=u_0(x)\\), which implies \\(\\eta(u(0,x))=\\eta(u_0(x))\\), giving</p> \\[ \\Big[\\int_{\\mathcal U}\\eta(u)\\,\\varphi\\,dx\\Big]_{t=0}^{t=T} = -\\int_{\\mathcal U}\\eta(u_0(x))\\,\\varphi(0,x)\\,dx. \\] <p>Therefore,</p> \\[ \\int_{0}^{T}\\int_{\\mathcal U}\\partial_t\\eta(u)\\,\\varphi\\,dx\\,dt = -\\int_{\\mathcal U}\\eta(u_0(x))\\,\\varphi(0,x)\\,dx -\\int_{0}^{T}\\int_{\\mathcal U}\\eta(u)\\,\\partial_t\\varphi\\,dx\\,dt. \\] <p>Space term (use spatial integration by parts with \\(F=q(u)\\)).</p> \\[ \\int_{0}^{T}\\int_{\\mathcal U}(\\nabla\\!\\cdot q(u))\\,\\varphi\\,dx\\,dt = \\int_{0}^{T}\\int_{\\partial\\mathcal U}\\varphi\\,q(u)\\cdot n\\,dS\\,dt -\\int_{0}^{T}\\int_{\\mathcal U}q(u)\\cdot\\nabla\\varphi\\,dx\\,dt. \\] <p>If \\(\\varphi=0\\) on \\(\\partial\\mathcal U\\), then the boundary integral is \\(0\\), so</p> \\[ \\int_{0}^{T}\\int_{\\mathcal U}(\\nabla\\!\\cdot q(u))\\,\\varphi\\,dx\\,dt = -\\int_{0}^{T}\\int_{\\mathcal U}q(u)\\cdot\\nabla\\varphi\\,dx\\,dt. \\] <p>Put both pieces back into the inequality.</p> \\[ -\\int_{\\mathcal U}\\eta(u_0(x))\\,\\varphi(0,x)\\,dx -\\int_{0}^{T}\\int_{\\mathcal U}\\eta(u)\\,\\partial_t\\varphi\\,dx\\,dt -\\int_{0}^{T}\\int_{\\mathcal U}q(u)\\cdot\\nabla\\varphi\\,dx\\,dt \\le 0. \\] <p>Rearranging,</p> \\[ \\int_{\\mathcal U}\\eta(u_0(x))\\,\\varphi(0,x)\\,dx +\\int_{0}^{T}\\int_{\\mathcal U}\\eta(u)\\,\\partial_t\\varphi\\,dx\\,dt +\\int_{0}^{T}\\int_{\\mathcal U}q(u)\\cdot\\nabla\\varphi\\,dx\\,dt \\ge 0, \\] <p>i.e.</p> \\[ \\int_{0}^{T}\\int_{\\mathcal U} \\Big(\\eta(u)\\,\\partial_t \\varphi + q(u)\\cdot \\nabla \\varphi \\Big)\\,dx\\,dt + \\int_{\\mathcal U}\\eta\\big(u_0(x)\\big)\\,\\varphi(0,x)\\,dx \\ge 0. \\] <p>The goal of these steps is to rewrite the entropy inequality in a form that avoids pointwise derivatives of \\(u\\). The final statement is an integral condition involving only \\(\\eta(u)\\) and \\(q(u)\\) tested against smooth \\(\\varphi\\), so it still makes sense when \\(u\\) has jumps. This is the standard definition of an entropy solution: a weak solution that also satisfies this inequality for all convex entropies (and all nonnegative tests). It is also the form used in analysis (for uniqueness/stability) and in practice (to design entropy-stable numerical schemes and learning losses that enforce the correct solution selection).</p> <p>At this point, two natural technical questions come up about the choice of test function \\(\\varphi\\): </p> <p>1) Why require \\(\\varphi\\ge 0\\)? Because the starting statement is an inequality (not an equality). Testing it against all nonnegative \\(\\varphi\\) is the standard weak way to encode '\\(\\le 0\\)' without needing pointwise derivatives.</p> <p>2) What if \\(\\varphi\\) does not vanish on \\(\\partial\\mathcal U\\) or at \\(t=T\\)? Then the boundary terms kept earlier stay in the formula:</p> \\[ \\Big[\\int_{\\mathcal U}\\eta(u)\\,\\varphi\\,dx\\Big]_{t=0}^{t=T} + \\int_{0}^{T}\\int_{\\partial\\mathcal U}\\varphi\\,q(u)\\cdot n\\,dS\\,dt \\] <p>and the precise entropy condition depends on how boundary data are imposed. Setting \\(\\varphi(T,\\cdot)=0\\) and \\(\\varphi|_{\\partial\\mathcal U}=0\\) is mainly a clean choice to present the core idea without extra terms.</p>"},{"location":"Hyperbolic%20PDEs-1-weak%20solutions/#wrapping-up","title":"Wrapping up","text":"<p>Connection to the weak formulation:</p> <ul> <li>The weak form encodes the PDE by integrated balances, so it remains meaningful even   when \\(u\\) has jumps.</li> <li>For hyperbolic problems, the weak form can admit multiple solutions. The   entropy condition adds an extra inequality that selects the physically relevant one   (and typically gives uniqueness within the right class).</li> <li>Many numerical methods are built to be entropy stable, so their discrete solutions   respect the same selection principle and avoid converging to a wrong weak solution.</li> </ul> <p>Intuition: a shock can satisfy the conservation law in an averaged sense, yet still be assembled in more than one mathematically valid way. Entropy rules out the nonphysical shock patterns by enforcing the correct direction of information flow and the expected dissipation across the jump.</p>"},{"location":"Hyperbolic%20PDEs-2-Conservation%20laws%20and%20entropy%20form/","title":"3 - Entropy Stable and Hyperbolic Solutions","text":""},{"location":"Hyperbolic%20PDEs-2-Conservation%20laws%20and%20entropy%20form/#introduction-to-conservation-law","title":"Introduction to conservation law","text":"<p>A conservation law is a mathematical statement that something is neither created nor destroyed inside a region, except by flowing across the region's boundary or by external sources. The key idea is change = stuff in - stuff out + sources. Suppose, \\(u(x,t)\\) represent the density of that conserved quantity at position \\(x\\) and time \\(t\\). Then the conservation principle in 1D says:</p> \\[ \\frac{d}{dt} \\Big(\\text{amount of $u$ in an interval}\\Big) = \\text{(flux entering)} - \\text{(flux leaving)} + \\text{(sources inside)}. \\] <p>This flowing in and flowing out of any quantity is defined by the term flux. Flux is the rate at which the conserved quantity crosses a boundary. In 1D, the boundary of an interval \\([a,b]\\) is just the two endpoints \\(a\\) and \\(b\\). If \\(f(u)\\) is the flux function, then:</p> <ul> <li>\\(f(u(a,t))\\) is the rate at which \\(u\\) flows from left to right across \\(x=a\\),</li> <li>\\(f(u(b,t))\\) is the rate at which \\(u\\) flows from left to right across \\(x=b\\).</li> </ul>"},{"location":"Hyperbolic%20PDEs-2-Conservation%20laws%20and%20entropy%20form/#conservation-law-in-integral-and-differential-form","title":"Conservation law in integral and differential form","text":"<p>The amount of density \\((u)\\) in the interval \\([a,b]\\) is given by:</p> \\[ \\int_a^b u(x,t)dx. \\] <p>If the flux in the positive \\(x-\\)direction is \\(f(u)\\), assuming no sources for now, gives the conservation law as:</p> \\[ \\frac{d}{dt}\\int_a^b u(x,t)dx = f(u(a,t)) - f(u(b,t)) + 0. \\] <p>This is the conservation law in integral form. It is extremely important because it remains meaningful even when \\(u\\) is not smooth (for example, if there is a shock).</p>"},{"location":"Hyperbolic%20PDEs-2-Conservation%20laws%20and%20entropy%20form/#from-integral-form-to-differential-form","title":"From integral form to differential form","text":"<p>Assume \\(u\\) is smooth enough that we can move derivatives inside integrals and use the fundamental theorem of calculus. We can rewrite the above as:</p> \\[ \\frac{d}{dt}\\int_a^b udx + \\big(f(u(b,t)) - f(u(a,t))\\big) = 0. \\] <p>Notice that:</p> \\[ \\frac{d}{dt}\\int_a^b u(x,t)dx = \\int_a^b \\frac{\\partial u}{\\partial t}(x,t)dx. \\] <p>and</p> \\[ f(u(b,t)) - f(u(a,t)) = \\int_a^b \\frac{\\partial}{\\partial x}f(u(x,t))dx. \\] <p>So:</p> \\[ \\int_a^b \\left( \\frac{\\partial u}{\\partial t}(x,t) + \\frac{\\partial}{\\partial x}f(u(x,t))\\right)dx = 0. \\] <p>If this holds for every interval \\([a,b]\\), then in the global sense, the integrand must be zero:</p> \\[ \\frac{\\partial u}{\\partial t}(x,t) + \\frac{\\partial}{\\partial x}f(u(x,t)) = u_t + \\big(f(u)\\big)_x = 0. \\] <p>This is the differential form of a 1D conservation law.</p>"},{"location":"Hyperbolic%20PDEs-2-Conservation%20laws%20and%20entropy%20form/#adding-sources","title":"Adding sources","text":"<p>If there is creation or removal inside the region, represented by a source term \\(s(x,t)\\), then:</p> \\[ \\frac{d}{dt}\\int_a^b udx = f(u(a,t)) - f(u(b,t)) + \\int_a^b s(x,t)dx, \\] <p>and the differential form becomes:</p> \\[ u_t + (f(u))_x = s(x,t). \\]"},{"location":"Hyperbolic%20PDEs-2-Conservation%20laws%20and%20entropy%20form/#finite-volume-method-fvm","title":"Finite Volume Method (FVM)","text":"<p>The finite volume method is built directly from the integral conservation law, so it automatically respects conservation. It works by integrating the PDE over small control volumes (cells) and approximating the fluxes crossing the cell boundaries. Information is exchanged between cells only through these fluxes, so a conservative flux approximation immediately yields a globally conservative scheme.</p> <p>Step 1: break space into cells:</p> <p>For any given cell \\(i\\), with center \\(x_i\\). Divide the 1D space into cells:</p> \\[ [x_{i-\\tfrac12}, x_{i+\\tfrac12}], \\quad \\Delta x = x_{i+\\tfrac12}-x_{i-\\tfrac12}. \\] <p>Then we can define the cell average (per unit length for 1D case):</p> \\[ \\bar{u}_i(t) = \\frac{1}{\\Delta x}\\int_{x_{i-\\tfrac12}}^{x_{i+\\tfrac12}} u(x,t)dx. \\] <p>Step 2: integrate the PDE over a cell:</p> <p>Start from the differential form:</p> \\[ u_t + (f(u))_x = 0. \\] <p>Integrate over cell \\(i\\):</p> \\[ \\int_{x_{i-\\tfrac12}}^{x_{i+\\tfrac12}} u_tdx +  \\int_{x_{i-\\tfrac12}}^{x_{i+\\tfrac12}} (f(u))_xdx = 0. \\] <p>From the first term, we can say for a given cell:</p> \\[ \\int u_tdx = \\frac{d}{dt}\\int udx = \\frac{d}{dt}\\big(\\Delta x\\bar{u}_i(t)\\big). \\] <p>The second term uses the fundamental theorem of calculus:</p> \\[ \\int (f(u))_xdx = f(u(x_{i+\\tfrac12},t)) - f(u(x_{i-\\tfrac12},t)). \\] <p>So:</p> \\[ \\frac{d}{dt}\\big(\\Delta x\\bar{u}_i\\big) +  \\big(f_{i+\\tfrac12} - f_{i-\\tfrac12}\\big)=0, \\] <p>where \\(f_{i\\pm \\tfrac12}\\) denotes the flux through the cell face. Dividing by \\(\\Delta x\\) gives:</p> \\[ \\frac{d\\bar{u}_i}{dt} = -\\frac{1}{\\Delta x}\\left(f_{i+\\tfrac12}-f_{i-\\tfrac12}\\right). \\]"},{"location":"Hyperbolic%20PDEs-2-Conservation%20laws%20and%20entropy%20form/#key-numerical-problem","title":"Key numerical problem","text":"<p>The above equation shows that the evolution of \\(\\bar{u}_i\\) is determined by the boundary or face fluxes \\(f_{i\\pm \\tfrac12}\\), but these are not directly known from the stored data. We do not know the true face fluxes \\(f_{i\\pm \\tfrac12}\\) because we only store cell averages. Instead, we introduce a numerical flux at each interface:</p> \\[ \\hat{f}_{i+\\tfrac12} = \\hat{f}\\big(u_{i+\\tfrac12}^L,u_{i+\\tfrac12}^R\\big),  \\qquad \\hat{f}_{i-\\tfrac12} = \\hat{f}\\big(u_{i-\\tfrac12}^L,u_{i-\\tfrac12}^R\\big), \\] <p>where \\(u_{i\\pm\\tfrac12}^L\\) and \\(u_{i\\pm\\tfrac12}^R\\) are the reconstructed states immediately to the left and right of the interface \\(x_{i\\pm\\tfrac12}\\), obtained from the neighboring cells. For example:</p> <ul> <li>at \\(x_{i+\\tfrac12}\\), \\(u_{i+\\tfrac12}^L\\) comes from cell \\(i\\) and \\(u_{i+\\tfrac12}^R\\) from cell \\(i+1\\),</li> <li>at \\(x_{i-\\tfrac12}\\), \\(u_{i-\\tfrac12}^L\\) comes from cell \\(i-1\\) and \\(u_{i-\\tfrac12}^R\\) from cell \\(i\\).</li> </ul> <p>The semi-discrete finite volume update is then</p> \\[ \\frac{d\\bar{u}_i}{dt}   = -\\frac{1}{\\Delta x}\\left(\\hat{f}_{i+\\tfrac12}-\\hat{f}_{i-\\tfrac12}\\right). \\]"},{"location":"Hyperbolic%20PDEs-2-Conservation%20laws%20and%20entropy%20form/#time-stepping-turning-dbarudt-into-updates","title":"Time stepping (turning \\(d\\bar{u}/dt\\) into updates)","text":"<p>The semi-discrete scheme given above gives, for each cell, an ODE in time for the averages \\(\\bar{u}_i(t)\\), where the right-hand side depends on the cell averages at the current time but not explicitly on \\(x\\). Thus, the spatial dependence has been replaced by cell indices, leaving a system of ODEs in the single variable \\(t\\). To obtain an implementable method, we now discretize time by choosing a time-stepping scheme for this system. A common simplest method is Forward Euler in time:</p> \\[ \\bar{u}_i^{n+1} = \\bar{u}_i^n - \\frac{\\Delta t}{\\Delta x}\\left(\\hat{f}_{i+\\tfrac12}^n-\\hat{f}_{i-\\tfrac12}^n\\right), \\] <p>where \\(n\\) represents the \\(n^{th}\\) time step for any cell \\(i\\). Higher accuracy in time uses methods like Runge--Kutta (RK2, RK3), but the flux idea stays the same.</p>"},{"location":"Hyperbolic%20PDEs-2-Conservation%20laws%20and%20entropy%20form/#cfl-condition","title":"CFL condition","text":"<p>Information in hyperbolic conservation laws travels at finite speeds (wave speeds). Numerically, we do not want a wave to travel farther than one cell in a single time step: the fastest signal should not skip over cells, so that each update depends only on information from neighboring cells. If \\(|\\lambda|\\) is a characteristic wave speed, then in one time step the distance traveled is approximately \\(|\\lambda|\\Delta t\\). To keep this below one cell width \\(\\Delta x\\), we require</p> \\[ |\\lambda|\\Delta t \\;\\lesssim\\; \\Delta x \\quad\\Longrightarrow\\quad \\Delta t \\;\\le\\; \\text{CFL}\\cdot \\frac{\\Delta x}{\\max |\\lambda|}, \\] <p>where \\(\\text{CFL}\\in(0,1]\\) is a chosen safety factor. For our scalar conservation law \\(u_t + (f(u))_x = 0\\), the characteristic speed is</p> \\[ \\lambda = f'(u), \\] <p>so we typically use \\(\\max |\\lambda| = \\max |f'(u)|\\) over the grid when computing the time step.</p>"},{"location":"Hyperbolic%20PDEs-2-Conservation%20laws%20and%20entropy%20form/#from-interface-states-to-numerical-fluxes","title":"From interface states to numerical fluxes","text":"<p>At each cell interface the solution is, in general, discontinuous, so the left and right traces differ:</p> \\[ u(x_{i+\\tfrac12}^-,t) \\approx u_L, \\qquad u(x_{i+\\tfrac12}^+,t) \\approx u_R. \\] <p>This setup defines a local Riemann problem: the PDE with piecewise constant initial data having a single jump at \\(x_{i+\\tfrac12}\\). A good numerical flux \\(\\hat{f}(u_L,u_R)\\) aims to approximate the physical flux at the interface generated by the solution of this local Riemann problem. As an example, Godunov's method can be used to approximate this flux.</p>"},{"location":"Hyperbolic%20PDEs-2-Conservation%20laws%20and%20entropy%20form/#godunovs-idea","title":"Godunov's idea","text":"<ol> <li>Approximate the solution in each cell by a constant (piecewise constant reconstruction).</li> <li>At every interface, pose and solve the corresponding Riemann problem exactly.</li> <li>Take the interfacial flux from this exact Riemann solution as the numerical flux.</li> </ol> <p>This procedure is very faithful to the underlying physics, but exact Riemann solvers are often costly or complicated. In practice, we therefore use approximate Riemann solvers, i.e. closed-form numerical flux formulas that mimic the behavior of the exact solution. For Godunov's original scheme we use a first-order (piecewise constant) reconstruction:</p> \\[ u_L = \\bar{u}_i, \\qquad u_R = \\bar{u}_{i+1}, \\] <p>so the numerical flux at \\(x_{i+\\tfrac12}\\) reduces to a function \\(\\hat{f}(\\bar{u}_i,\\bar{u}_{i+1})\\) of neighboring cell averages only. This is very robust, but the low-order reconstruction makes the method diffusive, smearing sharp discontinuities and steep gradients.</p>"},{"location":"Hyperbolic%20PDEs-2-Conservation%20laws%20and%20entropy%20form/#higher-order-reconstruction","title":"Higher order reconstruction","text":"<p>To reduce this numerical diffusion, we can move to a higher-order (piecewise linear) reconstruction. In each cell we reconstruct a slope \\(\\sigma_i\\) and define</p> \\[ u_{i+\\tfrac12}^L = \\bar{u}_i + \\frac{1}{2}\\sigma_i, \\qquad u_{i+\\tfrac12}^R = \\bar{u}_{i+1} - \\frac{1}{2}\\sigma_{i+1}, \\] <p>so that the left and right states at the interface \\(x_{i+\\tfrac12}\\) come from linear profiles inside cells \\(i\\) and \\(i+1\\). The slopes \\(\\sigma_i\\) are typically chosen with a limiter (e.g. minmod, van Leer, MC) to obtain second-order accuracy in smooth regions. A limiter is a nonlinear function that reduces the slope when neighboring cell averages are inconsistent (e.g. near a discontinuity), preventing the creation of new spurious extrema and suppressing Gibbs-type oscillations.</p> <p>Near true discontinuities the limiter forces the reconstruction to drop back to (first-order) more diffusive behavior, which is unavoidable if we want to avoid spurious oscillations. The gain is that away from shocks the scheme remains second-order accurate, so smooth regions are much less smeared than with a purely first-order method. In summary, a high-resolution finite volume scheme consists of a reconstruction procedure plus a suitable numerical flux \\(\\hat{f}(u_L,u_R)\\). There are several popular numerical fluxes for scalar conservation laws. Each offers a different trade-off between accuracy, robustness, and computational cost. A few example of those are upwind flux, Lax-Friedrichs (Rusanov) flux, Godunov flux, HLL-type flux and many more.</p>"},{"location":"Hyperbolic%20PDEs-2-Conservation%20laws%20and%20entropy%20form/#entropy-conservative-and-entropy-stable-fluxes","title":"Entropy-conservative and entropy-stable fluxes","text":"<p>For many conservation laws it is useful to control, in addition to the conserved variables \\(u\\), a convex entropy \\(\\eta(u)\\). For the scalar conservation law</p> \\[ u_t + f(u)_x = 0, \\] <p>an entropy/entropy-flux pair \\((\\eta,q)\\) is defined by the relation</p> \\[ q'(u) = \\eta'(u)f'(u). \\] <p>Assuming \\(u\\) is smooth, the chain rule gives</p> \\[ \\eta(u)_t = \\eta'(u)u_t, \\qquad q(u)_x = q'(u)u_x. \\] <p>From our conservation law we have \\(u_t = -f'(u)u_x\\), hence</p> \\[ \\eta(u)_t   = \\eta'(u)u_t   = -\\eta'(u)f'(u)u_x   = -q'(u)u_x   = -q(u)_x. \\] <p>Therefore we get a conservation law in entropy terms as</p> \\[ \\eta(u)_t + q(u)_x = 0 \\] <p>for smooth solutions, and the inequality \\(\\eta(u)_t + q(u)_x \\le 0\\) for weak (entropy) solutions. This is discussed in great detail in chapter 2 (weak solutions) of this series.</p> <p>In d-dimensional system</p> <p>For a standard \\(d\\)-dimensional system of conservation laws, we get</p> \\[ \\frac{\\partial u}{\\partial t} + \\sum_{i=1}^d \\frac{\\partial}{\\partial x_i} f_i(u) = 0, \\] <p>where</p> <ul> <li>\\(u(t,x) \\in \\mathbb{R}^p\\) is the vector of conserved variables,</li> <li>for each spatial direction \\(i = 1,\\dots,d\\),   \\(f_i : D \\subset \\mathbb{R}^p \\to \\mathbb{R}^p\\) is the flux in the   \\(x_i\\)-direction.</li> </ul> <p>In entropy terms the original system can be written as a scalar conservation law for \\(\\eta\\), i.e.</p> \\[ \\frac{\\partial}{\\partial t}\\eta(u) + \\sum_{i=1}^d \\frac{\\partial}{\\partial x_i} g_i(u) = 0, \\] <p>The equation above expresses exactly the condition that \\(\\eta,\\{g_i\\}\\) form an entropy / entropy\u2013flux pair compatible with the fluxes \\(\\{f_i\\}\\).</p>"},{"location":"Hyperbolic%20PDEs-2-Conservation%20laws%20and%20entropy%20form/#entropy-variables-and-flux-potential","title":"Entropy variables and flux potential","text":"<p>We have our conservation law for smooth solutions in entropy terms as</p> \\[ \\eta(u)_t + q(u)_x = 0 \\] <p>Then, we can define the entropy variables \\(v := \\eta'(u)\\). Using the chain rule, the above equation can be written as</p> \\[ v^\\top u_t + q'(u) u_x = 0. \\] <p>From our conservation law \\(u_t = -f(u)_x = -f'(u)u_x\\) we obtain</p> \\[ -v^\\top f'(u)u_x + q'(u)u_x = 0 \\quad\\Rightarrow\\quad \\bigl(q'(u) - v^\\top f'(u)\\bigr)u_x = 0. \\] <p>Since this must hold for arbitrary smooth \\(u(x,t)\\), we require the compatibility condition</p> \\[ q'(u) = v^\\top f'(u) \\quad\\text{or}\\quad \\nabla_u q(u) = \\eta'(u)^\\top f'(u). \\] <p>Using the chain rule and the compatibility relation from above, we have</p> \\[ q(u)_x = q'(u)u_x = v^\\top f'(u)u_x = v^\\top f(u)_x. \\] <p>Thus the entropy law can already be written as</p> \\[ \\eta(u)_t + v^\\top f(u)_x = 0. \\] <p>In d-dimensional system</p> <p>Since, the entropy function \\(\\eta\\) is strictly convex, the map</p> \\[ u \\;\\longmapsto\\; v(u) := \\bigl(\\nabla_{u}\\eta(u)\\bigr)^\\top \\] <p>is one\u2013to\u2013one on the domain of interest.  Therefore there exists an inverse map</p> \\[ \\Psi : \\mathbb{R}^p \\to \\mathbb{R}^p, \\qquad \\Psi(v) = u \\quad\\text{such that}\\quad v = \\bigl(\\nabla_{u}\\eta(u)\\bigr)^\\top. \\] <p>We can now re-express each physical flux \\(f_i\\) as a function of the entropy variables \\(v\\), by composition with the above function:</p> \\[ g_i(v) := f_i\\!\\bigl(\\Psi(v)\\bigr), \\qquad i = 1,\\dots,d. \\] <p>We can now rewrite the conservation law in terms of entropy variables using the chain rule. As \\(u\\) depends on \\(v\\), we have</p> \\[ \\frac{\\partial u}{\\partial t} = \\frac{\\partial u}{\\partial v} \\frac{\\partial v}{\\partial t} = \\nabla_{v}u(v)  \\frac{\\partial v}{\\partial t}. \\] <p>Similarly, for each spatial coordinate \\(x_i\\),</p> \\[ \\frac{\\partial}{\\partial x_i}f_i(u) = \\frac{\\partial}{\\partial x_i}g_i(v) = \\nabla_{v}g_i(v)  \\frac{\\partial v}{\\partial x_i}. \\] <p>Substitute these expressions into the conservation law:</p> \\[ \\begin{aligned} 0 &amp;= \\frac{\\partial u}{\\partial t} + \\sum_{i=1}^d \\frac{\\partial}{\\partial x_i}f_i(u) \\\\ 0 &amp;= \\nabla_{v}u(v)  \\frac{\\partial v}{\\partial t} + \\sum_{i=1}^d \\nabla_{v}g_i(v)  \\frac{\\partial v}{\\partial x_i}. \\end{aligned} \\] <p>Rearranging, we obtain the symmetrized form</p> \\[ \\nabla_{v}u(v)\\frac{\\partial v}{\\partial t} + \\sum_{i=1}^d \\nabla_{v}g_i(v)\\frac{\\partial v}{\\partial x_i} = 0. \\] <p>Using the inverse map \\(\\Psi\\) introduced above, we can write</p> \\[ u = \\Psi(v), \\] \\[ \\nabla_{v}\\Psi(v)\\frac{\\partial v}{\\partial t} + \\sum_{i=1}^d \\nabla_{v}g_i(v)\\frac{\\partial v}{\\partial x_i} = 0, \\] <p>where</p> \\[ \\nabla_{v}g_i(v) = \\nabla_{v}\\bigl(f_i(\\Psi(v))\\bigr) = \\nabla_{u}f_i\\bigl(\\Psi(v)\\bigr)\\nabla_{v}\\Psi(v). \\] <p>The above equation is the symmetric form of the original conservation law, written entirely in terms of the entropy variables \\(v\\) and the inverse map \\(\\Psi\\).</p> <p>Interface notation.</p> <p>At each cell interface \\(x_{i+\\frac12}\\) we have a left state and a right state.  For example, \\(u_{i+\\frac12}^-\\) and \\(u_{i+\\frac12}^+\\) are the left and right limits of \\(u(x)\\) at \\(x_{i+\\frac12}\\), and the corresponding entropy variables are \\(v_{i+\\frac12}^\\pm := \\eta'(u_{i+\\frac12}^\\pm)\\).</p>"},{"location":"Hyperbolic%20PDEs-2-Conservation%20laws%20and%20entropy%20form/#entropy-conservative-numerical-flux","title":"Entropy-conservative numerical flux","text":"<p>In a finite-volume method, the only information available at the interface \\(x_{i+\\frac12}\\) is the left and right cell states \\(u_{i+\\frac12}^-\\) and \\(u_{i+\\frac12}^+\\). Therefore, we use a two-point numerical flux, which is simply a function that uses only these two states to approximate the physical flux at the interface:</p> \\[ f^{\\mathrm{ec}}_{i+\\frac12}   = f^{\\mathrm{ec}}(u_{i+\\frac12}^-,u_{i+\\frac12}^+), \\] <p>with the consistency requirement \\(f^{\\mathrm{ec}}(u,u) = f(u)\\) where \\(ec\\) denotes  entropy-conservative.</p> <p>We now ask for this flux to be compatible with the entropy structure. Consider the semi-discrete finite-volume scheme built from \\(f^{\\mathrm{ec}}_{i+\\frac12}\\),</p> \\[ \\frac{d}{dt}\\bar{u}_i(t)   = -\\frac{1}{\\Delta x}     \\bigl(f^{\\mathrm{ec}}_{i+\\frac12} - f^{\\mathrm{ec}}_{i-\\frac12}\\bigr). \\] <p>Using the chain rule cell by cell, we first differentiate the entropy in each cell:</p> \\[ \\frac{d}{dt}\\eta(\\bar u_i(t))   = \\eta'(\\bar u_i(t))^\\top \\frac{d}{dt}\\bar u_i(t)   = v_i^\\top \\frac{d}{dt}\\bar u_i(t), \\] <p>since by definition \\(v_i := \\eta'(\\bar u_i)\\). Now multiply by the cell width \\(\\Delta x\\) and sum over all cells (substituting value from the semi discrete finite volume scheme):</p> \\[ \\sum_i v_i^\\top \\frac{d}{dt}\\bar u_i(t)\\Delta x = -\\sum_i v_i^\\top        \\bigl(f^{\\mathrm{ec}}_{i+\\frac12}-f^{\\mathrm{ec}}_{i-\\frac12}\\bigr). \\] <p>A discrete integration-by-parts is just an index shift that makes the interface contributions telescope in the same way as in the continuous case. For this we start from</p> \\[ -\\sum_i v_i^\\top    \\bigl(f^{\\mathrm{ec}}_{i+\\frac12}-f^{\\mathrm{ec}}_{i-\\frac12}\\bigr)   = -\\sum_i v_i^\\top f^{\\mathrm{ec}}_{i+\\frac12}     + \\sum_i v_i^\\top f^{\\mathrm{ec}}_{i-\\frac12}. \\] <p>In the second sum, perform the index shift \\(j = i-1\\):</p> \\[ \\sum_i v_i^\\top f^{\\mathrm{ec}}_{i-\\frac12}   = \\sum_j v_{j+1}^\\top f^{\\mathrm{ec}}_{j+\\frac12}, \\] <p>so, renaming \\(j\\) back to \\(i\\),</p> \\[ -\\sum_i v_i^\\top    \\bigl(f^{\\mathrm{ec}}_{i+\\frac12}-f^{\\mathrm{ec}}_{i-\\frac12}\\bigr)   = -\\sum_i v_i^\\top f^{\\mathrm{ec}}_{i+\\frac12}     + \\sum_i v_{i+1}^\\top f^{\\mathrm{ec}}_{i+\\frac12}. \\] <p>Now combine the two sums:</p> \\[ -\\sum_i v_i^\\top    \\bigl(f^{\\mathrm{ec}}_{i+\\frac12}-f^{\\mathrm{ec}}_{i-\\frac12}\\bigr)   = \\sum_i (v_{i+1} - v_i)^\\top f^{\\mathrm{ec}}_{i+\\frac12}. \\] <p>If we interpret the left cell of interface \\(x_{i+\\frac12}\\) as \\(v_{i+\\frac12}^- := v_i\\) and the right cell as \\(v_{i+\\frac12}^+ := v_{i+1}\\), then</p> \\[ [v]_{i+\\frac12}   := v_{i+\\frac12}^+ - v_{i+\\frac12}^-    = v_{i+1} - v_i, \\] <p>and we can write</p> \\[ -\\sum_i v_i^\\top    \\bigl(f^{\\mathrm{ec}}_{i+\\frac12}-f^{\\mathrm{ec}}_{i-\\frac12}\\bigr) =    \\sum_i [v]_{i+\\frac12}^\\top f^{\\mathrm{ec}}_{i+\\frac12}. \\] <p>This is the discrete analogue of an integration-by-parts formula. To make this look like a discrete conservation law for the entropy case, we require that each interface contribution can be written as a jump of a scalar potential \\(\\psi\\), i.e.</p> \\[ [v]_{i+\\frac12}^\\top f^{\\mathrm{ec}}_{i+\\frac12} =    [\\psi]_{i+\\frac12} :=    \\psi\\bigl(v_{i+\\frac12}^+\\bigr) -       \\psi\\bigl(v_{i+\\frac12}^-\\bigr) \\] <p>for all left/right states.  This is the discrete entropy identity. With this condition, the total discrete entropy satisfies</p> \\[ \\frac{d}{dt}\\sum_i \\eta(\\bar u_i)\\Delta x = \\sum_i [\\psi]_{i+\\frac12}, \\] <p>which telescopes and vanishes under periodic (or suitable) boundary conditions. This is because each term \\([\\psi]_{i+\\frac12} = \\psi_{i+\\frac12}^+ - \\psi_{i+\\frac12}^-\\) is a difference of neighboring interface values, so</p> \\[ \\sum_i [\\psi]_{i+\\frac12}= \\sum_i \\bigl(\\psi_{i+\\frac12}^+ - \\psi_{i+\\frac12}^-\\bigr) \\] <p>forms a telescoping sum: all interior contributions cancel pairwise, leaving only boundary terms.  For periodic boundaries (or if the entropy flux is zero at the physical boundaries), these remaining boundary terms also cancel (or vanish), so the total sum is zero and the discrete entropy is conserved.</p> \\[ \\frac{d}{dt}\\sum_i \\eta(\\bar u_i)\\Delta x = 0. \\]"},{"location":"Hyperbolic%20PDEs-2-Conservation%20laws%20and%20entropy%20form/#adding-dissipation-entropy-stable-fluxes","title":"Adding dissipation: entropy-stable fluxes","text":"<p>Exact entropy conservation is too weak in the presence of shocks: the physically relevant (entropy) solution satisfies \\(\\eta(u)_t + q(u)_x \\le 0\\), i.e. entropy should decrease, not be preserved.  Our entropy-conservative flux \\(f^{\\mathrm{ec}}_{i+\\frac12}\\) gives</p> \\[ \\frac{d}{dt}\\sum_i \\eta(\\bar u_i)\\Delta x = 0, \\] <p>so we now modify the interface fluxes in such a way that the total entropy becomes nonincreasing.</p> <p>The idea is to add a dissipative term at each interface that is negative definite in terms of the entropy variables.  Since \\(v = \\eta'(u)\\) are the natural variables for the entropy, we take a correction that is linear in the jump \\([v]_{i+\\frac12}\\) and symmetric between the two neighboring cells.  This leads to the ansatz</p> \\[ \\hat f_{i+\\frac12}   := f^{\\mathrm{ec}}_{i+\\frac12}      - \\tfrac12 D_{i+\\frac12}[v]_{i+\\frac12}, \\] <p>where \\(D_{i+\\frac12}\\) is a symmetric positive semidefinite matrix (\\(D_{i+\\frac12} \\ge 0\\) in the scalar case).</p> <p>The form \\(f^{\\mathrm{ec}} - \\tfrac12 D[v]\\) is chosen for two reasons:</p> <ul> <li>The correction is centered at the interface and depends only on   the jump \\([v]_{i+\\frac12} = v_{i+\\frac12}^+ - v_{i+\\frac12}^-\\),   so it vanishes for smooth (locally constant) states and does not   destroy conservation of \\(u\\).</li> <li>When we repeat the entropy calculation with \\(\\hat f_{i+\\frac12}\\),   the extra term contributes</li> </ul> \\[ -\\frac12\\sum_i [v]_{i+\\frac12}^\\top                 D_{i+\\frac12}[v]_{i+\\frac12} \\le 0 \\] <p>to \\(\\dfrac{d}{dt}\\sum_i \\eta(\\bar u_i)\\Delta x\\), because   \\(D_{i+\\frac12}\\) is positive semidefinite.  Thus the total   discrete entropy becomes nonincreasing, which is exactly the   discrete analogue of the continuous entropy inequality.</p> <p>In this sense, an entropy-stable flux is obtained by starting from an entropy-conservative flux and adding a carefully designed, symmetric dissipation term written in entropy variables.</p> <p>Using \\(\\hat f_{i+\\frac12}\\) in our semi-discrete finite volume method, yields the semi-discrete scheme</p> \\[ \\frac{d}{dt}\\bar{u}_i(t) =    -\\frac{1}{\\Delta x}     \\bigl(\\hat f_{i+\\frac12} - \\hat f_{i-\\frac12}\\bigr). \\] <p>Repeating the above summation argument,</p> \\[ \\begin{aligned} \\frac{d}{dt}\\sum_i \\eta(\\bar u_i)\\Delta x   &amp;= \\sum_i [v]_{i+\\frac12}^\\top \\hat f_{i+\\frac12} \\\\   &amp;= \\sum_i [v]_{i+\\frac12}^\\top f^{\\mathrm{ec}}_{i+\\frac12}      - \\frac12\\sum_i [v]_{i+\\frac12}^\\top                       D_{i+\\frac12}[v]_{i+\\frac12}. \\end{aligned} \\] <p>The first term is the same as before and equals \\(\\sum_i [\\psi]_{i+\\frac12}\\), which vanishes under periodic (or suitable) boundary conditions.  Thus</p> \\[ \\frac{d}{dt}\\sum_i \\eta(\\bar u_i)\\Delta x = -    \\frac12\\sum_i [v]_{i+\\frac12}^\\top                     D_{i+\\frac12}[v]_{i+\\frac12}   \\;\\le\\; 0, \\] <p>because each quadratic form \\([v]_{i+\\frac12}^\\top D_{i+\\frac12}[v]_{i+\\frac12}\\) is nonnegative. Hence the total discrete entropy is nonincreasing in time, and the scheme is called entropy-stable.</p>"},{"location":"Hyperbolic%20PDEs-2-Conservation%20laws%20and%20entropy%20form/#entropy-stable-flux-formulation","title":"Entropy stable flux formulation","text":"<p>In the previous subsection, we obtained an entropy-stable numerical flux by starting from an entropy-conservative flux \\(f^{\\mathrm{ec}}_{i+\\frac12}\\) and adding a symmetric dissipation term \\(-\\tfrac12 D_{i+\\frac12}[v]_{i+\\frac12}\\), which guarantees a discrete entropy inequality.  It remains to specify how to construct \\(f^{\\mathrm{ec}}_{i+\\frac12}\\) itself. Now, we derive a practical entropy-conservative flux by enforcing the discrete entropy condition \\( [v]_{i+\\frac12}^\\top f^{\\mathrm{ec}}_{i+\\frac12} = [\\phi(v)]_{i+\\frac12} \\) at each interface with \\([v]_{i+\\frac12} := v_{i+\\frac12}^+ - v_{i+\\frac12}^-\\) and \\([\\phi(v)]_{i+\\frac12} := \\phi(v_{i+\\frac12}^+) - \\phi(v_{i+\\frac12}^-)\\).</p> <p>We start from a centered flux and add a correction by taking the arithmetic average of the physical fluxes</p> \\[ f^{\\mathrm{c}}_{i+\\frac12} :=  \\tfrac12\\Big(f(u_{i+\\frac12}^+)+f(u_{i+\\frac12}^-)\\Big), \\] <p>and look for \\(f^{\\mathrm{ec}}_{i+\\frac12}\\) of the form</p> \\[ f^{\\mathrm{ec}}_{i+\\frac12} =   f^{\\mathrm{c}}_{i+\\frac12} +     \\alpha_{i+\\frac12}[v]_{i+\\frac12}, \\] <p>where \\(\\alpha_{i+\\frac12}\\) is a scalar to be determined. Only the component of the correction parallel to \\([v]_{i+\\frac12}\\) can change the scalar product \\([v]_{i+\\frac12}^\\top f^{\\mathrm{ec}}_{i+\\frac12}\\), so this is the minimal modification needed to enforce the entropy condition.</p> <p>Now we impose the discrete entropy condition and plug the ansatz into \\([v]_{i+\\frac12}^\\top f^{\\mathrm{ec}}_{i+\\frac12} = [\\phi(v)]_{i+\\frac12}\\):</p> \\[ [v]_{i+\\frac12}^\\top f^{\\mathrm{c}}_{i+\\frac12} +  \\alpha_{i+\\frac12}\\|[v]_{i+\\frac12}\\|_2^2 =  [\\phi(v)]_{i+\\frac12}. \\] <p>Hence</p> \\[ \\alpha_{i+\\frac12}= \\frac{[\\phi(v)]_{i+\\frac12} -          [v]_{i+\\frac12}^\\top f^{\\mathrm{c}}_{i+\\frac12}}         {\\|[v]_{i+\\frac12}\\|_2^2}. \\] <p>Insert \\(\\alpha_{i+\\frac12}\\) back into the flux. Using the expression for \\(f^{\\mathrm{c}}_{i+\\frac12}\\), we obtain</p> \\[ f^{\\mathrm{ec}}_{i+\\frac12} =   \\tfrac12\\Big(f(u_{i+\\frac12}^+)                +f(u_{i+\\frac12}^-)\\Big) +   \\frac{     [\\phi(v)]_{i+\\frac12} -      \\tfrac12 [v]_{i+\\frac12}^\\top       \\Big(f(u_{i+\\frac12}^+)+f(u_{i+\\frac12}^-)\\Big)}     {\\|[v]_{i+\\frac12}\\|_2^2}    [v]_{i+\\frac12}, \\] <p>At this point we have an explicit interface flux \\(f^{\\mathrm{ec}}_{i+\\frac12}\\) that is entropy-conservative by construction, i.e. it satisfies \\([v]_{i+\\frac12}^\\top f^{\\mathrm{ec}}_{i+\\frac12}=[\\phi(v)]_{i+\\frac12}\\). This is precisely the baseline flux needed in the discrete entropy condition, adding the symmetric dissipation term then yields an entropy-stable flux.</p>"},{"location":"Hyperbolic%20PDEs-2-Conservation%20laws%20and%20entropy%20form/#rusanov-type-entropy-stable-flux","title":"Rusanov-type entropy-stable flux","text":"<p>Consider a hyperbolic system</p> \\[ u_t + f(u)_x = 0. \\] <p>The classical Rusanov (local Lax--Friedrichs) numerical flux can be written as a central flux plus a scalar dissipation proportional to the state jump,</p> \\[ f^{\\mathrm{Rus}}(u^-,u^+) = \\frac{f(u^-)+f(u^+)}{2} - \\frac{1}{2}\\,\\lambda^{\\max}\\,(u^+-u^-), \\] <p>where \\(\\lambda^{\\max}\\) is a local estimate of the largest characteristic speed (typically the spectral radius of the flux Jacobian).</p> <p>To obtain entropy stability, we mimic this structure but add dissipation in the direction of the entropy--variable jump. This yields a Rusanov--type (local Lax--Friedrichs) entropy--stable flux</p> \\[ \\tilde f_{i+1/2}^{\\theta,\\mu} = f_{i+1/2}^{\\theta,\\mu,ec} - \\frac{1}{2}\\,\\lambda^{\\max}_{i+1/2}   \\Big( H_u \\eta_\\theta(\\bar u_{i+1/2}) \\Big)^{-1}   \\Big(     \\nabla_u \\eta_\\theta(u_{i+1/2}^+) -      \\nabla_u \\eta_\\theta(u_{i+1/2}^-)   \\Big). \\] <p>Here \\(\\bar u_{i+1/2}\\) denotes a suitable average state (e.g., the arithmetic mean of \\(u_{i+1/2}^-\\) and \\(u_{i+1/2}^+\\)). Defining the diffusion matrix</p> \\[ D_{i+1/2} :=  \\lambda^{\\max}_{i+1/2}    \\Big( H_u \\eta_\\theta(\\bar u_{i+1/2}) \\Big)^{-1}, \\] <p>the added term is of local Lax-Friedrichs type: a scalar maximum wave speed \\(\\lambda^{\\max}_{i+1/2}\\) multiplied by a (symmetric) positive semidefinite diffusion operator acting on the entropy-variable jump. Indeed, \\(H_u\\eta_\\theta(\\bar u_{i+1/2})\\) is symmetric positive definite, and \\(\\lambda^{\\max}_{i+1/2}\\ge 0\\), hence \\(D_{i+1/2}\\) is symmetric positive semidefinite.</p> <p>The local maximum wave speed is chosen as the spectral radius</p> \\[ \\lambda^{\\max}_{i+1/2} = \\rho\\!\\big( B^{1/2} A B^{1/2} \\big), \\qquad A = H_v \\varphi_\\mu(\\bar v_{i+1/2}),\\quad B = H_u \\eta_\\theta(\\bar u_{i+1/2}), \\] <p>where \\(\\rho(M)\\) denotes the spectral radius of a square matrix \\(M\\),</p> \\[ \\rho(M) := \\max\\{\\,|\\lambda| : \\lambda \\in \\sigma(M)\\,\\}, \\] <p>with \\(\\sigma(M)\\) the set of eigenvalues of \\(M\\), \\(\\bar v_{i+1/2} = \\nabla_u\\eta_\\theta(\\bar u_{i+1/2})^\\top\\). Since \\(B^{1/2} A B^{1/2}\\) is symmetric, its eigenvalues are real, and \\(\\lambda^{\\max}_{i+1/2}\\) provides a local approximation of the largest characteristic speed of the learned hyperbolic system. Consequently, \\(\\tilde f_{i+1/2}^{\\theta,\\mu}\\) reduces to the entropy--conservative flux in smooth regions (when jumps are small) and adds the minimal upwind diffusion aligned with</p> \\[ [[\\nabla_u\\eta_\\theta(u)]]_{i+1/2} \\] <p>needed to enforce the discrete entropy inequality.</p>"},{"location":"Hyperbolic%20PDEs-2-Conservation%20laws%20and%20entropy%20form/#guarantee-entropy-stablility-and-hyperbolicity-in-solution-in-d-dimensional-case","title":"Guarantee entropy stablility and hyperbolicity in solution in d-dimensional case","text":"<p>To guarantee entropy stable solution and hyperbolicity, the requirement that the flux Jacobian has real eigenvalues and a complete set of eigenvectors, is a property essential for well-posedness and wavepropagation dynamics. We can restrict the entropy to be convex by certain parameterization (for example parameterizing it as a input convex neural network)</p> \\[ \\eta_\\theta : \\mathbb{R}^p \\to \\mathbb{R}, \\] <p>with parameters \\(\\theta\\) so by construction \\(\\eta_\\theta\\) is convex in \\(u\\). Then the corresponding entropy variables</p> \\[ v(u) := \\bigl(\\nabla_{u}\\eta_\\theta(u)\\bigr)^\\top. \\] <p>has its Hessian \\(H_{u}\\eta_\\theta(u)\\) as symmetric positive definite, so this map is again invertible (at least locally). Now we introduce a scalar potential function</p> \\[ \\phi_{\\mu,i} : \\mathbb{R}^p \\to \\mathbb{R}, \\] <p>with parameters \\(\\mu\\), and define the flux in entropy variables as the gradient of this potential:</p> \\[ g_i(v) := \\nabla_{v}\\phi_{\\mu,i}(v), \\qquad i=1,\\dots,d. \\] <p>Now we demand this to be symmetric in entropy variables \\(v\\), for this we would like the Jacobians</p> \\[ \\nabla_{v}g_i(v) \\] <p>to be symmetric matrices.  A very simple way to guarantee this is to represent each \\(g_i\\) as the gradient of the scalar potential \\(\\phi_{\\mu,i}\\), because then</p> \\[ \\nabla_{v}g_i(v) = \\nabla_{v}\\bigl(\\nabla_{v}\\phi_{\\mu,i}(v)\\bigr) = H_{v}\\phi_{\\mu,i}(v), \\] <p>which is automatically symmetric. So we are free to choose any parametric form for the flux \\(g_i\\); choosing it as a gradient \\(\\nabla_{v}\\phi_{\\mu,i}\\) is a deliberate design choice that builds the desired symmetry (and hence hyperbolic structure) into the model.</p> <p>We now want a flux written directly in terms of the conserved variables \\(u\\).  Conceptually, we demand that the flux in entropy variables \\(g_i\\) and the flux in conserved variables \\(f_i^{\\theta,\\mu}\\) are related by</p> \\[ g_i(v) = f_i^{\\theta,\\mu}\\bigl(\\Psi_\\theta(v_{\\theta}(u))\\bigr), \\qquad i = 1,\\dots,d. \\] <p>Equivalently, if we start from a given \\(u\\), we compute \\(v_\\theta(u) = (\\nabla_{u}\\eta_\\theta(u))^\\top\\) and plug this into \\(g_i\\).  This defines the parametric flux in conserved variables:</p> \\[ f_i^{\\theta,\\mu}(u) := g_i\\bigl(v_\\theta(u)\\bigr) = \\left.\\nabla_{v}\\phi_{\\mu,i}(v)\\right|_{v = (\\nabla_{u}\\eta_\\theta(u))^\\top}. \\] <p>Remember, that this is only a definition: we have chosen a particular parametric form for the flux and the flux-potential function. Finally, we return to the original conservative form and replace the unknown fluxes \\(f_i\\) by the parametric fluxes \\(f_i^{\\theta,\\mu}\\). This gives</p> \\[ \\begin{aligned} \\frac{\\partial u}{\\partial t} + \\sum_{i=1}^d \\frac{\\partial}{\\partial x_i} f_i^{\\theta,\\mu}(u) &amp;= 0, \\end{aligned} \\] \\[ \\boxed{ \\frac{\\partial u}{\\partial t} +  \\sum_{i=1}^d   \\frac{\\partial}{\\partial x_i}   \\underbrace{     \\nabla_{v}\\phi_{\\mu,i}\\!\\bigl(\\nabla_{u}\\eta_\\theta(u)\\bigr)   }_{\\displaystyle f_i^{\\theta,\\mu}(u)} = 0. } \\] <p>Hyperbolicity by construction.</p> <p>The flux Jacobian in direction \\(i\\) is</p> \\[ \\nabla_u f_i^{\\theta,\\mu}(u) = H_v \\phi_{\\mu,i}(v)\\, H_u \\eta_\\theta(u) =: A_i B, \\] <p>where \\(A_i\\) is symmetric and \\(B\\) is symmetric positive definite. The product \\(A_i B\\) is similar to the symmetric matrix \\(B^{1/2} A_i B^{1/2}\\), hence has real eigenvalues and a complete set of eigenvectors. Therefore the system is hyperbolic, with \\(\\eta_\\theta\\) serving as a strictly convex entropy.</p>"},{"location":"Hyperbolic%20PDEs-2-Conservation%20laws%20and%20entropy%20form/#references","title":"References","text":"<ul> <li>Liu, L., Zhang, L., &amp; Gelb, A. (2026). Parametric Hyperbolic Conservation Laws: A Unified Framework for Conservation, Entropy Stability, and Hyperbolicity. arXiv preprint arXiv:2601.21080.</li> </ul>"},{"location":"RL-1-Introduction/","title":"Introduction to Reinforcement Learning","text":"<p>Reinforcement Learning (RL) is the science of decision making: an agent learns what to do by interacting with an environment, trying to maximize it's reward over time. What makes RL different from many other machine learning methods is the kind of feedback it receives. Instead of a supervisor telling the correct answer, the agent only sees a reward signal, and that signal may be delayed, meaning the effect of an action might show up much later. Time is therefore central: the data comes as a sequence, not as independent samples (so it is not i.i.d.), and the agent's actions actively influence what it experiences next. Typical examples of RL includes learning to play games (like Atari or Chess), training robots to walk or manipulate objects, optimizing long-term choices in recommender systems, and controlling real-world systems such as traffic lights, resource allocation, or scheduling.</p>"},{"location":"RL-1-Introduction/#motivation-for-winning-rewards-and-reward-hypothesis","title":"Motivation for winning: Rewards and Reward Hypothesis","text":"<p>A natural next step is to talk about rewards. In RL, the reward is usually written as \\(R_t\\). It is a scalar value that acts as a feedback signal for the agent at time step \\(t\\). The agent's job is to maximize the cumulative (over time) reward.</p> <p>This naturally motivates the reward hypothesis: all goals can be described as maximizing the expected cumulative reward. This is a very strong assumption because it says that any notion of a goal can always be turned into an objective of maximizing the expected cumulative reward. But this assumption may not always hold: many goals are hard to compress into a single number. Many real objectives often include multiple preferences, safety constraints, fairness considerations, or instructions like do not do X,. These can be awkward to express with one reward signal. When the reward is an imperfect proxy, an agent may optimize the number while missing the real intent.</p> <p>For example, consider an autonomous car with the goal get to the destination quickly, but never endanger pedestrians, obey traffic rules, and keep passengers comfortable. Speed, safety, legality, and comfort can conflict, and turning all of that into one reward number is tricky. A poorly chosen reward might push the car to drive aggressively to gain time, or to exploit loopholes in the definition of safe, which shows why reducing a rich goal to a single reward can be fragile.</p> <p>In application, when the real objective is complex, RL still forces a simplification: the agent must ultimately make decisions by comparing actions using a single objective. Even if we care about many things at once (speed, safety, cost, comfort), the standard RL view is that these preferences must be converted into one scalar reward signal. In practice, the hard part is not the optimization itself, but choosing a reward function that correctly represents what we actually want, since any mismatch can push the agent to optimize the number rather than the intended goal.</p>"},{"location":"RL-1-Introduction/#sequential-decision-making-a-unifying-framework","title":"Sequential Decision Making (a Unifying Framework)","text":"<p>The key observation from the above examples is that RL problems are rarely one-shot decisions. They are sequential: an agent repeatedly chooses actions, those actions can have long-term consequences, and rewards can arrive later. Because of this, it can be optimal to give up an immediate reward if it leads to a larger total reward in the future.</p> <p>This is where the unifying framework comes in. RL models tasks as sequential decision-making problems where, at each time step \\(t\\), the agent observes the situation, takes an action, and receives a reward \\(R_t\\). The objective is not to maximize \\(R_t\\) at a single step, but to maximize the expected total future reward accumulated over time. This viewpoint lets very different problems share the same mathematical structure: investing money (pay a cost now, gain later), or blocking an opponent in a game (sacrifice a move now to improve winning chances later). In the next part, we will formalize this idea with the standard RL model used to describe the environment and the agent's interaction with it.</p>"},{"location":"RL-1-Introduction/#agent-environment-interaction","title":"Agent-Environment Interaction","text":"<p>RL starts with a simple setup: an agent interacts with an environment over time. The agent is the decision-maker we want to control (think of it as the brain), and the environment is everything outside the agent that reacts to its actions. Note, the agent does not directly control the environment.</p> <p>As a sequential decision process, let time is split into steps \\(t = 1,2,3,\\dots\\). At each step \\(t\\): the agent receives an observation \\(O_t\\) (what it can currently sense), uses it to choose an action \\(A_t\\), and then the environment responds by producing a reward and the next observation. A common way to write this is:</p> \\[ O_t \\xrightarrow{\\text{agent chooses}} A_t \\xrightarrow{\\text{environment responds}} (R_{t+1}, O_{t+1}). \\] <p>Then, \\(O_{t+1}\\) becomes the input for the next decision process. This loop repeats, so decisions can have long-term effects through how they change future observations and rewards. Over time, these interactions produce a history (full time-ordered record) of what the agent has observed, done, and received so far. One way to write the history at time \\(t\\) is</p> \\[ H_t = (O_1, R_1, A_1, \\dots, A_{t-1}, O_t, R_t), \\] <p>which we can think of as all observable variables up to time \\(t\\). This is the only information the agent can directly use. The environment may contain many hidden variables (for example, internal physics, other agents' intentions, or unobserved noise) that affect what happens next, but the agent does not get to see them. A completely general agent could choose actions as a function of the entire history, i.e., a decision rule of the form \\(A_t = \\pi(H_t)\\). The problem is that histories grow longer over time, which makes them inconvenient to store and reason about.</p> <p>This motivates the idea of a state: a compressed summary of the history that keeps the information needed to decide what to do next. Formally, we define the state at time \\(t\\) as a function of history,</p> \\[ S_t = f(H_t), \\] <p>so the agent can act using \\(S_t\\) instead of the full \\(H_t\\). The main goal is to choose \\(f\\) so that \\(S_t\\) captures what matters the most for predicting the future, while remaining much smaller and easier to work with than the entire history.</p>"},{"location":"RL-1-Introduction/#state","title":"State","text":"<p>The word state is overloaded in RL, and it helps to separate three related ideas.</p> <ul> <li>First, the environment state \\(S_t^{e}\\) is the environment's private description of the world: whatever internal variables it uses to generate the next observation and reward. The agent typically cannot see \\(S_t^{e}\\) directly. Even if it could, parts of it might be irrelevant for decision making, so having access to the full environment state is not always necessary.</li> <li>Second, the agent state \\(S_t^{a}\\) is the agent's internal representation of what is going on. This is what the agent actually stores and updates while interacting with the environment. In general, it can be any function of the observable history,</li> </ul> \\[ S_t^{a} = f(H_t), \\] <p>and actions are chosen based on this internal state. This is usually the most practical notion of state in application, because it is under the agent's control. - Third, an information state (or Markov state) is a special kind of state that contains all useful information from the history for predicting the future. One common way to express this is</p> \\[ \\mathbb{P}(S_{t+1}\\mid S_t) = \\mathbb{P}(S_{t+1}\\mid S_1,\\dots,S_t), \\] <p>which is the Markov property. When a state is Markov, we can treat it as a sufficient summary: in principle, we can throw away the full history and still act optimally using only \\(S_t\\).</p>"},{"location":"RL-1-Introduction/#fully-observable-environments-assumption","title":"Fully Observable Environments (Assumption)","text":"<p>For most of our discussion, we will focus on the simplest and most common setting: fully observable environments. Full observability means the agent directly observes the underlying state of the environment, so there is no hidden information from the agent's point of view. In this case, the observation is the state:</p> \\[ O_t = S_t. \\] <p>Because the agent can see the full state, we can treat the agent state, environment state, and information (Markov) state as the same object:</p> \\[ O_t = S_t^{a} = S_t^{e}. \\] <p>This is important because it makes the problem much easier to model and solve: the current state contains all the information needed to predict what happens next, so we do not need to carry the entire history. Formally, this setting is called a Markov decision process (MDP), and it will be the main framework used in the discussions that follow.</p>"},{"location":"RL-1-Introduction/#partially-observable-environments-reality","title":"Partially Observable Environments (Reality)","text":"<p>In many realistic settings, the agent cannot directly observe the true environment state. This is called partial observability: the agent only gets an observation signal that provides an indirect view of the environment. Typical examples are a robot with a camera that is not told its absolute location or a trading agent that only observes current prices not the trends.</p> <p>In this setting, the environment has an internal (often hidden) state \\(S_t^{e}\\), but the agent does not observe it directly. Instead, it receives observations \\(O_t\\). As a result, the agent's internal state is generally not equal to the environment state:</p> \\[ S_t^{a} \\neq S_t^{e}. \\] <p>Formally, this setup is modeled as a partially observable Markov decision process (POMDP). Even if the environment dynamics are Markov in \\(S_t^{e}\\), the agent only sees \\(O_t\\), so it must build its own state representation to decide well.</p> <p>A few common choices for the agent state \\(S_t^{a}\\) are:</p> <p>(1) Complete history:</p> \\[  S_t^{a} = H_t, \\] <p>where \\(H_t\\) is the full sequence of observations, actions, and rewards up to time \\(t\\).</p> <p>(2) Belief over environment states:</p> \\[ S_t^{a} = \\big(\\mathbb{P}[S_t^{e}=s^1], \\ldots, \\mathbb{P}[S_t^{e}=s^n]\\big), \\] <p>which is a probability distribution over possible environment states. This is often called the belief state, and it summarizes uncertainty about what the true hidden state might be.</p> <p>(3) Learned memory (recurrent neural network)</p> \\[ S_t^{a} = \\sigma\\!\\left(S_{t-1}^{a} W_s + O_t W_o\\right), \\] <p>where \\(\\sigma(\\cdot)\\) is a nonlinear function and \\(W_s, W_o\\) are parameters. Here the agent compresses the past into a compact internal representation that is updated every step.</p> <p>All three approaches aim for the same goal: construct an internal state \\(S_t^{a}\\) that contains enough information from the history to choose good actions, even when the true environment state is not directly observable.</p>"},{"location":"RL-1-Introduction/#major-components-of-an-rl-agent","title":"Major Components of an RL Agent","text":"<p>An RL agent is usually described in terms of a few core components that play different roles in decision making. Depending on the algorithm, an agent may use one or more of the following: a policy, a value function, and a model of the environment. Different RL methods emphasize different components, but these ideas form a common language across the field.</p>"},{"location":"RL-1-Introduction/#policy","title":"Policy","text":"<p>A policy describes the agent\u2019s behavior. It specifies how the agent chooses actions based on the current state. In other words, it is a mapping from states to actions. In the simplest case, the policy is deterministic, meaning it always picks the same action in a given state:</p> \\[ a = \\pi(s). \\] <p>More generally, policies are stochastic. A stochastic policy defines a probability distribution over actions given a state:</p> \\[ \\pi(a \\mid s) = \\mathbb{P}[A_t = a \\mid S_t = s]. \\] <p>Stochastic policies are useful when the environment is uncertain, when exploration is needed, or when randomization itself is beneficial.</p>"},{"location":"RL-1-Introduction/#value-functions","title":"Value Functions","text":"<p>Value functions predict future reward and are used to evaluate how good it is to be in a given situation. There are two closely related types.</p> <ul> <li>The state-value function evaluates a state \\(s\\) under policy \\(\\pi\\):</li> </ul> \\[ v_\\pi(s) = \\mathbb{E}_\\pi \\big[ R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\mid S_t = s \\big], \\] <p>where \\(\\gamma \\in [0,1]\\) is the discount factor.</p> <ul> <li>The action-value function (also called the \\(Q\\)-function) evaluates a state--action pair. It answers: if I am in state \\(s\\), take action \\(a\\) now, and then follow policy \\(\\pi\\), what long-term reward should I expect?</li> </ul> \\[ q_\\pi(s,a) = \\mathbb{E}_\\pi \\big[ R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\mid S_t = s,\\; A_t = a \\big]. \\] <p>Action-value functions are especially useful for action selection, since they directly compare which action looks better in the same state.</p>"},{"location":"RL-1-Introduction/#model","title":"Model","text":"<p>A model captures how the environment behaves. While a policy tells the agent what action to take and a value function predicts long-term reward, a model predicts what the environment will do next. Having a model allows the agent to reason about the consequences of actions without directly interacting with the real environment.</p> <p>In the standard RL setting, a model consists of two parts.</p> <ul> <li>The transition model predicts the next state given the current state and action:</li> </ul> \\[ \\mathcal{P}^{a}_{ss'} = \\mathbb{P}[S_{t+1} = s' \\mid S_t = s,\\; A_t = a]. \\] <ul> <li>The reward model predicts the expected immediate reward for taking an action in a state:</li> </ul> \\[ \\mathcal{R}^{a}_{s} = \\mathbb{E}[R_{t+1} \\mid S_t = s,\\; A_t = a]. \\] <p>Together, these components describe the dynamics of the environment as seen by the agent. Algorithms that explicitly learn or use \\(\\mathcal{P}\\) and \\(\\mathcal{R}\\) are called model-based methods. In contrast, model-free methods do not build an explicit model of the environment. They learn a policy and/or value functions directly from experience. Most of our discussion will focus on model-free RL, since it is often simpler to implement and is widely used in practice.</p>"},{"location":"RL-1-Introduction/#categorizing-rl-agents","title":"Categorizing RL Agents","text":""},{"location":"RL-1-Introduction/#classification-of-rl-agents-by-learned-components","title":"Classification of RL Agents by Learned Components","text":"<p>RL algorithms can be categorized by what they explicitly learn: a value function, a policy, or both. This classification helps clarify the differences between methods and when each is most appropriate.</p> <ul> <li>Value-based agents focus on learning a value function, usually an action-value function \\(q(s,a)\\). They do not explicitly learn a policy. Instead, the policy is implicit: at each time step, actions are chosen by comparing values and picking the best one (for example, choosing the action with the highest \\(q(s,a)\\)). Classic examples like Q-learning fall into this category.</li> <li>Policy-based agents directly learn a policy \\(\\pi(a \\mid s)\\) without maintaining a value function. The policy is optimized to maximize expected cumulative reward, often using gradient-based methods. These approaches are natural for continuous action spaces and stochastic policies, but they do not explicitly evaluate how good states or actions are.</li> <li>Actor-critic agents combine both ideas. The actor is the policy, which selects actions, while the critic is a value function that evaluates how good those actions are. The critic provides feedback to improve the actor, making learning more stable and efficient. Many modern RL algorithms use this structure because it balances the strengths of value-based and policy-based methods.</li> </ul>"},{"location":"RL-1-Introduction/#categorizing-rl-agents-model-free-vs-model-based","title":"Categorizing RL Agents: Model-Free vs. Model-Based","text":"<p>Another common way to categorize RL agents is based on whether they use an explicit model of the environment. As seen in the previous sectoin we can define these as:</p> <ul> <li>Model-free agents do not try to learn how the environment works internally. Instead, they learn a policy, a value function, or both, directly from experience. All learning happens through trial and error, using observed rewards and transitions, without explicitly predicting the next state or reward. Most standard RL algorithms fall into this category, and most of this discussion focuses on model-free methods because they are simple, flexible, and widely used in practice.</li> <li>Model-based agents, on the other hand, explicitly learn or are given a model of the environment. In addition to learning a policy and/or value function, they also learn how states transition and what rewards to expect. This allows the agent to plan ahead by simulating future outcomes before acting. Model-based methods can be more data-efficient, but they are often harder to design and computationally more expensive.</li> </ul> <p>This distinction also clarifies the difference between learning and planning in sequential decision making: both aim to improve the agent's policy, but they differ in what information is available. In model-free reinforcement learning, the environment is initially unknown, so the agent must learn through interaction and trial and error. In planning (model-based), a model of the environment is known or already learned, allowing the agent to simulate future trajectories and improve its policy internally through deliberation, reasoning, or search.</p>"},{"location":"RL-1-Introduction/#exploration-and-exploitation","title":"Exploration and Exploitation","text":"<p>Reinforcement learning is often described as trial-and-error learning. The agent must discover a good policy by interacting with the environment and learning from the outcomes of its actions. At the same time, it wants to collect as much reward as possible while learning, rather than performing poorly for a long time.</p> <p>This leads to a fundamental trade-off. Exploration refers to taking actions that may not seem optimal right now, but help the agent gather more information about the environment. Exploitation, on the other hand, means using the information the agent already has to choose actions that are expected to give high reward.</p> <p>Both are necessary. If the agent only exploits, it may get stuck with a suboptimal policy because it never tries alternatives. If it only explores, it may learn a lot but fail to accumulate reward. Effective RL algorithms balance exploration and exploitation so that the agent learns about the environment while still performing reasonably well along the way.</p>"},{"location":"RL-1-Introduction/#prediction-and-control","title":"Prediction and Control","text":"<p>Many problems in reinforcement learning can be viewed through the lens of prediction and control. These are two closely related, but distinct, objectives.</p> <ul> <li>Prediction asks: given a fixed policy, how good is it? The goal is to evaluate the future by estimating expected rewards when the agent follows a particular policy. This is typically done using value functions, which predict long-term reward without changing the policy itself.</li> <li>Control goes one step further. Instead of just evaluating a policy, the goal is to improve it. Control is about optimizing the future by finding the best possible policy, one that maximizes expected cumulative reward. Most RL algorithms alternate between prediction (evaluating how good things are) and control (using that information to choose better actions).</li> </ul>"},{"location":"RL-1-Introduction/#references","title":"References","text":"<ul> <li>https://github.com/zyxue/youtube_RL_course_by_David_Silver</li> </ul>"},{"location":"RL-2-Markov%20Decision%20Processes/","title":"Introduction to Markov Decision Processes","text":"<p>In reinforcement learning (RL), interaction with an environment is modeled as a random trajectory</p> \\[   S_0, A_0, R_1, S_1, A_1, R_2, S_2, \\dots   \\] <p>where \\(S_t \\in \\mathcal{S}\\) is the state at time \\(t\\), \\(A_t \\in \\mathcal{A}\\) is the action chosen at time \\(t\\), and \\(R_{t+1} \\in \\mathbb{R}\\) is the reward received after taking \\(A_t\\) in \\(S_t\\) and moving to \\(S_{t+1}\\).</p> <p>A Markov Decision Process (MDP) is a standard mathematical model for the environment in RL. An MDP assumes the environment is fully observable, meaning the agent can observe the true state. Equivalently, the state is intended to summarize all information needed to predict what can happen next, so the past matters only through the current state. Because of this, many RL problems can be formulated as MDPs.</p> <p>A key point to note here is that the trajectory variables \\(S_0, A_0, R_1, S_1, A_1, R_2, \\dots\\) are random variables, so there is a joint distribution over the entire sequence. Equivalently, for any particular full trajectory $ (S_0, A_0, R_1, S_1, A_1, R_2, \\dots),$ the model assigns a well-defined probability to observing exactly that sequence.</p> \\[   \\Pr(S_{t+1}=s' \\mid S_0,A_0,R_1,\\dots,S_t=s, A_t=a).   \\] <p>Rather than writing the full joint distribution over an entire trajectory explicitly as above, we usually describe the process through one-step conditional distribution.</p> \\[   \\Pr(S_{t+1}=s' \\mid S_0,A_0,R_1,\\dots,S_t=s, A_t=a) = \\Pr(S_{t+1}=s' \\mid S_t=s, A_t=a).   \\] <p>This is the Markov property. The key idea here is that the future depends only on the present, not on the full past, assuming a state captures all information from the history that is relevant for predicting the future. Often we also include reward in the Markov property:</p> \\[   \\Pr(S_{t+1}=s', R_{t+1}=r \\mid \\text{history up to } t,\\, S_t=s, A_t=a) = \\Pr(S_{t+1}=s', R_{t+1}=r \\mid S_t=s, A_t=a).   \\] <p>This means the distribution of the next state depends only on the current state. Equivalently, the state is a sufficient statistic of the future.</p> <p>Fully observable environment assumption. When the environment is fully observable and we have chosen a state variable \\(S_t\\) that captures everything relevant, the Markov property is a reasonable modeling assumption. If our \\(S_t\\) omits important information, the process may fail to be Markov (this motivates POMDPs and state augmentation, which will be discussed in a later chapter).</p>"},{"location":"RL-2-Markov%20Decision%20Processes/#state-transition-matrix","title":"State Transition Matrix","text":"<p>In a Markov process, state changes are described using transition probabilities. Assume a finite state space \\(\\mathcal{S}=\\{1,2,\\dots,n\\}\\) and (time-homogeneous) transitions. For a current state \\(s\\) and a next state \\(s'\\), the state transition probability is given as</p> \\[   P_{ss'} \\;=\\; \\mathbb{P}(S_{t+1}=s' \\mid S_t=s), \\qquad s,s' \\in \\{1,\\dots,n\\}.   \\] <p>Collecting all transition probabilities gives the state transition matrix \\(P \\in \\mathbb{R}^{n\\times n}\\).</p> \\[   P = \\begin{bmatrix} P_{11} &amp; P_{12} &amp; \\cdots &amp; P_{1n} \\\\ P_{21} &amp; P_{22} &amp; \\cdots &amp; P_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ P_{n1} &amp; P_{n2} &amp; \\cdots &amp; P_{nn} \\end{bmatrix}.   \\] <p>Here, row \\(s\\) is the conditional distribution over the next state given the current state \\(s\\), so its entries satisfy \\(P_{ss'} \\ge 0\\) and</p> \\[   \\sum_{s'=1}^n P_{ss'} = 1.   \\] <p>Meanwhile, column \\(s'\\) collects the probabilities of transitioning into \\(s'\\) from each possible current state,</p> \\[   (P_{1s'}, P_{2s'}, \\dots, P_{ns'})^\\top,   \\] <p>so columns are not required to sum to \\(1\\) in general. Instead, they indicate how likely it is to arrive at \\(s'\\) from each origin, and in stationary analysis they help interpret how probability mass moves under repeated application of \\(P\\).</p>"},{"location":"RL-2-Markov%20Decision%20Processes/#time-dependent-trasnsition-matrix","title":"Time dependent trasnsition matrix","text":"<p>Note that the above formulation assumes that the transition probabilities are constant over time and is therefore called a time-homogeneous Markov Process. If the transition law depends on time, we have a time-inhomogeneous Markov process given by:</p> \\[   \\Pr(S_{t+1}=s' \\mid S_t=s) = P_t(s' \\mid s),   \\] <p>or in matrix form \\(P_t\\) instead of a single \\(P\\). It is usually handeled in the following two ways:</p> <p>(1) Accept nonstationarity. We explicitly model \\(P_t\\). This is common in nonstationary environments. Many RL algorithms assume stationarity for guarantees. If the environment changes, performance guarantees weaken, but the process can still be treated as Markov-with-time.</p> <p>(2) Augment the state to recover stationarity. If the change is systematic and depends on something observable (like time of day, season, or a known mode), we can include that in the state and define an augmented state as:</p> \\[   \\tilde{S}_t = (S_t, t).   \\] <p>Then the process can become time-homogeneous in the augmented space:</p> \\[   \\Pr(\\tilde{S}_{t+1} \\mid \\tilde{S}_t) \\;\\text{does not need explicit dependence on } t \\text{ anymore}   \\] <p>because \\(t\\) is now part of the state. More generally, if the environment has a hidden ``mode'' \\(M_t\\) that drives changes, we can try to infer and include (an estimate of) \\(M_t\\) in the state.</p> <p>Key idea. Nonstationarity often signals that our current \\(S_t\\) is not capturing all relevant variables. ``Fixing'' it is usually done by adding missing variables to the state representation.</p>"},{"location":"RL-2-Markov%20Decision%20Processes/#markov-reward-process","title":"Markov Reward Process","text":"<p>A Markov chain models how states evolve. To reason about good and bad states, we add rewards. A Markov Reward Process (MRP) extends a Markov process by attaching a numerical reward and is defined by the tuple \\((\\mathcal{S}, P, R, \\gamma)\\), where:</p> <ul> <li>\\(\\mathcal{S}\\) is a finite set of states (the state space).</li> <li>\\(P\\) is the state transition model (matrix form), with entries</li> </ul> \\[   P_{ss'} = \\mathbb{P}(S_{t+1} = s' \\mid S_t = s).   \\] <ul> <li>\\(R\\) is the reward model. A common convention is state-based reward,</li> </ul> \\[   R_s = \\mathbb{E}[R_{t+1} \\mid S_t = s],   \\] <p>which gives the expected immediate reward when in state \\(s\\). Another common convention is transition-based reward,</p> \\[   R(s,s') = \\mathbb{E}[R_{t+1}\\mid S_t=s,\\, S_{t+1}=s'],   \\] <p>which allows the reward to depend on the transition.</p> <ul> <li>\\(\\gamma \\in [0,1]\\) is the discount factor, which controls how much future rewards are valued relative to immediate rewards.</li> </ul> <p>An MRP can be seen as a Markov chain with rewards attached to states (or, under the transition-based convention, attached to transitions).</p> <p>Remark (model-based vs. model-free). In the definition of an MRP, the tuple \\((\\mathcal S,P,R,\\gamma)\\) specifies the environment, so \\(P\\) and the reward model \\(R\\) are treated as given. In many RL settings, however, \\(P\\) and the expected reward model (e.g. \\(R_s=\\mathbb{E}[R_{t+1}\\mid S_t=s]\\)) are unknown. We only observe sampled transitions and realized rewards \\((S_t,R_{t+1},S_{t+1})\\). The discount factor \\(\\gamma\\) is typically chosen by the practitioner.</p>"},{"location":"RL-2-Markov%20Decision%20Processes/#return-total-reward-over-time","title":"Return: total reward over time","text":"<p>In reinforcement learning we often consider an episode (also called a trajectory), which is a single sampled sequence of states, actions, and rewards generated by interacting with the environment:</p> \\[   S_0, A_0, R_1, S_1, A_1, R_2, \\dots   \\] <p>The episode may be finite (ending at a terminal time \\(T\\)) or infinite. To compute the return at time \\(t\\), we take the discounted sum of rewards received from time \\(t+1\\) onward:</p> \\[   G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+1+k}.   \\] <p>Special cases and common settings:</p> <ul> <li>Undiscounted return (\\(\\gamma=1\\)):</li> </ul> \\[   G_t = \\sum_{k=0}^{\\infty} R_{t+1+k}.   \\] <p>This is used when we want to value rewards at all future times equally (no time preference). In practice it is most common in episodic tasks with terminal states, where the episode ends so the sum is over finitely many rewards and stays well-defined.</p> <ul> <li>Finite-horizon return (horizon \\(T\\)):</li> </ul> \\[   G_t^{(T)} = \\sum_{k=0}^{T-t-1} \\gamma^k R_{t+1+k}.   \\] <p>This is used when only rewards up to a deadline matter. For example, in problems with a fixed-length episode, limited budget of steps, or a task where performance is evaluated over the next \\(T-t\\) time steps, rewards after time \\(T\\) are not counted at all.</p>"},{"location":"RL-2-Markov%20Decision%20Processes/#why-discounting","title":"Why discounting?","text":"<p>Discounting is often motivated by three (compatible) reasons:</p> <p>1) Keep returns bounded in cyclic processes. In a Markov process with cycles, we can revisit rewarding states infinitely often. If rewards are nonnegative and we use \\(\\gamma=1\\), the infinite sum can diverge.</p> <p>Example: if \\(R_{t+1}=1\\) forever, then</p> \\[   \\sum_{k=0}^{\\infty} 1 = \\infty,   \\] <p>but with \\(\\gamma \\in (0,1)\\),</p> \\[   \\sum_{k=0}^{\\infty} \\gamma^k = \\frac{1}{1-\\gamma} &lt; \\infty.   \\] <p>So discounting makes infinite-horizon problems mathematically well-behaved.</p> <p>2) Present value (finance analogy). A reward received later is worth less today. Discounting models this by down-weighting future outcomes.</p> <p>3) Immediate vs delayed under uncertainty / imperfect models. When the world is uncertain (and our model isn't perfect), far-future predictions are less reliable. Discounting implicitly says: optimize what we can predict/control more confidently.</p>"},{"location":"RL-2-Markov%20Decision%20Processes/#value-function-expected-return-from-a-state","title":"Value function: expected return from a state","text":"<p>The state-value function (or simply value function) of an MRP is the expected discounted return when starting in state \\(s\\):</p> \\[   V(s) = \\mathbb{E}\\!\\left[G_t \\mid S_t=s\\right], \\qquad G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+1+k}.   \\] <p>Equivalently, \\(V(s)\\) is a function \\(V:\\mathcal S\\to\\mathbb R\\) that assigns to each state \\(s\\) the long-term utility of being in \\(s\\) under the MRP dynamics (no actions), with future rewards geometrically discounted by \\(\\gamma\\). This expectation is over the randomness of future transitions (and rewards).</p>"},{"location":"RL-2-Markov%20Decision%20Processes/#bellman-expectation-equation-mrp","title":"Bellman expectation equation (MRP)","text":"<p>We can split the return into the next reward plus the rest:</p> \\[   G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+1+k} = R_{t+1} + \\sum_{k=1}^{\\infty} \\gamma^k R_{t+1+k} = R_{t+1} + \\gamma \\sum_{j=0}^{\\infty} \\gamma^j R_{t+2+j} = R_{t+1} + \\gamma G_{t+1}.   \\] <p>Take conditional expectation given \\(S_t=s\\):</p> \\[   V(s) = \\mathbb{E}\\left[G_t \\mid S_t=s\\right]= \\mathbb{E}[R_{t+1}\\mid S_t=s] + \\gamma\\,\\mathbb{E}[G_{t+1}\\mid S_t=s].   \\] <p>Now use the law of total expectation (proof shown in appendix) over the next state \\(S_{t+1}\\):</p> \\[   \\mathbb{E}[G_{t+1}\\mid S_t=s]= \\sum_{s'} \\Pr(S_{t+1}=s'\\mid S_t=s)\\,\\mathbb{E}[G_{t+1}\\mid S_{t+1}=s',\\,S_t=s].   \\] <p>Since under the Markov property (and a fixed stationary policy) the future return \\(G_{t+1}\\) depends on the past only through the current state \\(S_{t+1}\\), we have</p> \\[   \\mathbb{E}[G_{t+1}\\mid S_{t+1}=s',\\,S_t=s] =\\mathbb{E}[G_{t+1}\\mid S_{t+1}=s'] =V(s').   \\] <p>Then we can write:</p> \\[   \\mathbb{E}[G_{t+1}\\mid S_t=s] =\\sum_{s'} \\Pr(S_{t+1}=s'\\mid S_t=s)\\,V(s').   \\] <p>Substituting this back into</p> \\[   V(s)=\\mathbb{E}[R_{t+1}\\mid S_t=s]+\\gamma\\,\\mathbb{E}[G_{t+1}\\mid S_t=s],   \\] <p>gives</p> \\[   V(s)=\\mathbb{E}[R_{t+1}\\mid S_t=s]+\\gamma\\sum_{s'} \\Pr(S_{t+1}=s'\\mid S_t=s)\\,V(s').   \\] <p>Identifying</p> \\[   \\mathcal{R}_s := \\mathbb{E}[R_{t+1}\\mid S_t=s] \\quad \\text{and} \\quad \\mathcal{P}_{ss'} := \\Pr(S_{t+1}=s' \\mid S_t=s),   \\] <p>this becomes</p> \\[   v(s) = \\mathcal{R}_s + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}\\, v(s'),   \\] <p>which is the Bellman expectation equation.</p> <p>In vector form (finite state space \\(\\mathcal S\\) with \\(|\\mathcal S|=n\\)), let \\(v\\in\\mathbb{R}^n\\) with entries \\(v_s=v(s)\\), let \\(\\mathcal R\\in\\mathbb{R}^n\\) with entries \\(\\mathcal R_s\\), and let \\(\\mathcal P\\in\\mathbb{R}^{n\\times n}\\) with entries \\(\\mathcal P_{ss'}\\). Then</p> \\[   v = \\mathcal R + \\gamma \\mathcal P\\,v \\quad\\Longrightarrow\\quad (I-\\gamma \\mathcal P)\\,v=\\mathcal R \\quad\\Longrightarrow\\quad v=(I-\\gamma \\mathcal P)^{-1}\\mathcal R,   \\] <p>where \\(I\\) is the \\(n\\times n\\) identity matrix. For \\(\\gamma\\in[0,1)\\) and finite MRPs, \\((I-\\gamma\\mathcal P)\\) is invertible.</p>"},{"location":"RL-2-Markov%20Decision%20Processes/#bellman-equation-for-mrps-recursive-form","title":"Bellman equation for MRPs (recursive form)","text":"<p>The Bellman equation expresses the value function as a recursion: the value of a state \\(s\\) is written in terms of the values of its possible successor states. Starting from \\(s\\), the process transitions to a random next state \\(s'\\), yields an immediate reward, and then continues recursively from \\(s'\\). Taking expectations over all possible next states yields a recursive equation for \\(V(s)\\).</p> <p>Recursive form (transition-based reward, most general) Starting from the definition</p> \\[   V(s)=\\mathbb{E}[G_t\\mid S_t=s],\\qquad  G_t=R_{t+1}+\\gamma G_{t+1},   \\] <p>take conditional expectation given \\(S_t=s\\):</p> \\[   V(s)=\\mathbb{E}[R_{t+1}\\mid S_t=s]+\\gamma\\,\\mathbb{E}[G_{t+1}\\mid S_t=s].   \\] <p>Apply the law of total expectation over \\(S_{t+1}\\):</p> \\[   \\mathbb{E}[R_{t+1}\\mid S_t=s] =\\sum_{s'}P(s'\\mid s)\\,\\mathbb{E}[R_{t+1}\\mid S_t=s,S_{t+1}=s'] =\\sum_{s'}P(s'\\mid s)\\,R(s,s'),   \\] <p>and (Markov property)</p> \\[   \\mathbb{E}[G_{t+1}\\mid S_t=s] =\\sum_{s'}P(s'\\mid s)\\,\\mathbb{E}[G_{t+1}\\mid S_{t+1}=s',S_t=s] =\\sum_{s'}P(s'\\mid s)\\,V(s').   \\] <p>Substituting both back gives</p> \\[   V(s)=\\sum_{s'}P(s'\\mid s)\\Big(R(s,s')+\\gamma V(s')\\Big).   \\] <p>This equation shows that \\(V(s)\\) is obtained as a probability-weighted average of the quantities \\(R(s,s')+\\gamma V(s')\\) associated with each possible successor state \\(s'\\).</p> <p>Special case (state-based reward) If rewards depend only on the current state, i.e. \\(R(s,s')\\equiv R(s)\\), the equation reduces to</p> \\[   V(s)=R(s)+\\gamma\\sum_{s'\\in\\mathcal S}P(s'\\mid s)\\,V(s').   \\] <p>Worked example (recursive value computation) Suppose from state \\(s\\) the next states are \\(\\{a,b,c\\}\\) with</p> \\[   P(a\\mid s)=0.2,\\qquad P(b\\mid s)=0.5,\\qquad P(c\\mid s)=0.3.   \\] <p>Using the transition-based Bellman equation,</p> \\[   V(s) =0.2\\big(R(s,a)+\\gamma V(a)\\big) +0.5\\big(R(s,b)+\\gamma V(b)\\big) +0.3\\big(R(s,c)+\\gamma V(c)\\big).   \\] <p>Equivalently, this means compute the quantity \\(R(s,s')+\\gamma V(s')\\) for each successor \\(s'\\in\\{a,b,c\\}\\) and take their probability-weighted average.</p>"},{"location":"RL-2-Markov%20Decision%20Processes/#complexity-of-solving-the-bellman-equation","title":"Complexity of solving the Bellman equation","text":"<p>For a finite MRP with \\(n=|\\mathcal S|\\) states, the Bellman equation in vector form is</p> \\[   v=\\mathcal R+\\gamma \\mathcal P v \\quad\\Longleftrightarrow\\quad (I-\\gamma \\mathcal P)v=\\mathcal R.   \\] <p>Direct (matrix) solution. If we treat \\(\\mathcal P\\) as a dense \\(n\\times n\\) matrix, solving the linear system \\((I-\\gamma \\mathcal P)v=\\mathcal R\\) with standard dense methods typically costs \\(\\mathcal O(n^3)\\) time and storing \\(\\mathcal P\\) costs \\(\\mathcal O(n^2)\\) memory. Thus direct solvers are practical mainly for small MRPs (or when \\(\\mathcal P\\) is very sparse and we can use sparse linear algebra).</p> <p>Large MRPs: iterative and sample-based methods. When \\(n\\) is large, we usually avoid forming and inverting matrices explicitly. Instead we use iterative methods that repeatedly apply the Bellman recursion (e.g. value iteration / iterative policy evaluation), or model-free methods that estimate \\(v\\) from sampled trajectories (e.g. Monte Carlo and temporal-difference learning) without needing an explicit \\(\\mathcal P\\).</p>"},{"location":"RL-2-Markov%20Decision%20Processes/#evaluation-methods-for-large-mrps","title":"Evaluation methods for large MRPs","text":"<p>When the state space is large, directly solving the Bellman equation is often impractical. Instead, value functions are computed approximately using iterative or sample-based methods. The three most common approaches for evaluating the value function of an MRP differ in what is assumed to be known and how the Bellman recursion is used (these will be discussed in detail in later chapters).</p> <p>1) Dynamic Programming (DP) / iterative evaluation. Assume the MRP model \\((\\mathcal P,\\mathcal R)\\) is known. Starting from an arbitrary initial estimate \\(v^{(0)}\\), repeatedly apply the Bellman recursion</p> \\[   v^{(k+1)} = \\mathcal R + \\gamma \\mathcal P\\, v^{(k)}.   \\] <p>Component-wise,</p> \\[   v^{(k+1)}(s)=\\mathcal R_s+\\gamma\\sum_{s'}\\mathcal P_{ss'}\\,v^{(k)}(s').   \\] <p>Under standard conditions, \\(v^{(k)}\\) converges to the true value function \\(v\\).</p> <p>2) Monte Carlo (MC) evaluation. Assume the model \\((\\mathcal P,\\mathcal R)\\) is unknown, but sample episodes can be generated. Since \\( v(s)=\\mathbb E[G_t\\mid S_t=s], \\) estimate \\(v(s)\\) by averaging observed returns. If \\(m\\) visits to state \\(s\\) are observed, the Monte Carlo estimate is</p> \\[   \\hat v(s)=\\frac{1}{m}\\sum_{i=1}^{m} G^{(i)}, \\qquad G^{(i)}=\\sum_{k=0}^{\\infty}\\gamma^k R^{(i)}_{t_i+1+k}.   \\] <p>Monte Carlo methods are purely sample-based and do not require explicit knowledge of \\(\\mathcal P\\) or \\(\\mathcal R\\).</p> <p>3) Temporal-Difference (TD) learning. Temporal-Difference methods combine ideas from DP (bootstrapping) and MC (sampling). From a single observed transition \\( S_t=s \\to S_{t+1}=s' \\) with reward \\(R_{t+1}\\), define the TD error</p> \\[   \\delta_t = R_{t+1} + \\gamma v(s') - v(s).   \\] <p>The value estimate is updated incrementally using a step size \\(\\alpha\\in(0,1]\\):</p> \\[   v(s)\\leftarrow v(s)+\\alpha\\,\\delta_t.   \\] <p>TD methods update values online, do not require full episodes, and do not assume knowledge of the transition model.</p>"},{"location":"RL-2-Markov%20Decision%20Processes/#markov-decision-processes-mdps","title":"Markov Decision Processes (MDPs)","text":"<p>So far we considered Markov Reward Processes (MRPs), where the dynamics are fixed and there is no control: the process moves according to \\(\\mathcal P\\) and generates rewards according to \\(\\mathcal R\\) (or \\(R(s,s')\\)). A Markov Decision Process (MDP) generalizes an MRP by introducing actions, so that the transition and reward distributions can depend on the agent's choice. A (finite, discounted) MDP is a tuple</p> \\[   (\\mathcal S,\\mathcal A, P, R, \\gamma),   \\] <p>where \\(\\mathcal S\\) is the finite state space, \\(\\mathcal A\\) is a finite action set, \\(\\gamma\\in[0,1)\\) is the discount factor, and:</p> \\[   P(s'\\mid s,a) = \\Pr(S_{t+1}=s'\\mid S_t=s, A_t=a)   \\] <p>is the action-dependent transition model. In the finite case, it is convenient to view \\(P(\\cdot\\mid\\cdot,a)\\) as a matrix for each action:</p> \\[   \\mathcal P^{a}_{ss'} = P(s'\\mid s,a), \\qquad a\\in\\mathcal A,   \\] <p>i.e., an MDP has a collection of transition matrices \\(\\{\\mathcal P^{a}\\}_{a\\in\\mathcal A}\\) rather than a single \\(\\mathcal P\\) as in an MRP. Rewards may also depend on the chosen action. Common conventions are</p> \\[   \\mathcal R(s,a)= \\mathbb E[R_{t+1}\\mid S_t=s, A_t=a], \\qquad\\text{or}\\qquad \\mathcal R(s,a,s')= \\mathbb E[R_{t+1}\\mid S_t=s, A_t=a, S_{t+1}=s'],   \\] <p>Thus, compared to an MRP, the new ingredient in an MDP is that both the transition dynamics and the reward distribution can change with the action \\(a\\).</p>"},{"location":"RL-2-Markov%20Decision%20Processes/#policies-how-the-agent-behaves","title":"Policies: how the agent behaves","text":"<p>In an MDP, the environment specifies what can happen given a state and an action via \\(P(s'\\mid s,a)\\) and how rewards are generated via \\(R(\\cdot)\\). What remains is to specify the agent's behavior: how actions are selected in each state. This is captured by a policy.</p> <p>Definition (deterministic policy). A deterministic policy chooses a single action in each state:</p> \\[   \\mu:\\mathcal S\\to\\mathcal A,\\qquad A_t=\\mu(S_t).   \\] <p>Definition (stochastic policy). A stochastic policy is a mapping from states to distributions over actions:</p> \\[   \\pi(a\\mid s)=\\Pr(A_t=a\\mid S_t=s), \\qquad s\\in\\mathcal S,\\ a\\in\\mathcal A.   \\] <p>Thus, for each state \\(s\\), \\(\\pi(\\cdot\\mid s)\\) is a probability distribution on \\(\\mathcal A\\).</p> <p>Stationary vs. non-stationary. A policy is stationary if it does not explicitly depend on time:</p> \\[   \\pi_t(a\\mid s)=\\pi(a\\mid s)\\quad \\forall t,   \\] <p>and non-stationary if it can vary with time:</p> \\[   \\pi_t(a\\mid s)\\neq \\pi_{t'}(a\\mid s)\\ \\text{for some }t\\neq t'.   \\] <p>Non-stationary policies are most common in finite-horizon settings, while discounted infinite-horizon problems typically focus on stationary policies.</p>"},{"location":"RL-2-Markov%20Decision%20Processes/#induced-markov-processes-under-a-policy","title":"Induced Markov processes under a policy","text":"<p>An MDP specifies how the environment responds to state--action pairs. Once a policy is fixed, the agent's behavior is fully determined, and the remaining randomness comes only from the environment. In this sense, a policy ``removes control'' from the MDP and induces a Markov process. In this section we formalize this idea and show that any fixed policy transforms an MDP into a Markov reward process (MRP).</p> <p>State dynamics under a fixed policy. Fix an MDP \\((\\mathcal S,\\mathcal A,\\mathcal P,\\mathcal R,\\gamma)\\) and a (stationary) policy \\(\\pi\\). The resulting state sequence \\(\\{S_t\\}\\) is a Markov chain with transition probabilities</p> \\[   \\mathcal P^\\pi_{ss'} \\;= \\Pr(S_{t+1}=s'\\mid S_t=s).   \\] <p>By marginalizing over the action chosen by the policy,</p> \\[   \\mathcal P^\\pi_{ss'} = \\sum_{a\\in\\mathcal A} \\Pr(S_{t+1}=s'\\mid S_t=s,A_t=a)\\Pr(A_t=a\\mid S_t=s) = \\sum_{a\\in\\mathcal A}\\pi(a\\mid s)\\,\\mathcal P^{a}_{ss'}.   \\] <p>Thus, a fixed policy induces a single state-transition matrix \\(\\mathcal P^\\pi\\).</p> <p>Rewards under a fixed policy. Similarly, the expected one-step reward under policy \\(\\pi\\) is</p> \\[   \\mathcal R^\\pi_s \\;=\\; \\mathbb E[R_{t+1}\\mid S_t=s] = \\sum_{a\\in\\mathcal A}\\pi(a\\mid s)\\,\\mathcal R(s,a),   \\] <p>or, under a transition-based reward convention,</p> \\[   \\mathcal R^\\pi_s = \\sum_{a\\in\\mathcal A}\\pi(a\\mid s)\\sum_{s'\\in\\mathcal S}\\mathcal P^{a}_{ss'}\\,\\mathcal R(s,a,s').   \\] <p>Combining the induced transition matrix \\(\\mathcal P^\\pi\\) and reward function \\(\\mathcal R^\\pi\\), a fixed policy \\(\\pi\\) turns the MDP into a Markov reward process</p> \\[   (\\mathcal S,\\mathcal P^\\pi,\\mathcal R^\\pi,\\gamma).   \\] <p>This induced MRP allows us to evaluate the policy using the value-function machinery developed earlier.</p>"},{"location":"RL-2-Markov%20Decision%20Processes/#value-functions-in-mdps-state-value-and-action-value","title":"Value functions in MDPs: state-value and action-value","text":"<p>We keep the same notion of discounted return as in MRPs:</p> \\[   G_t = \\sum_{k=0}^{\\infty}\\gamma^k R_{t+1+k}.   \\] <p>Given a fixed policy \\(\\pi\\), we evaluate behavior using two closely related value functions.</p> <p>State-value function. The state-value function under policy \\(\\pi\\) is the expected return when starting in state \\(s\\) and then following \\(\\pi\\):</p> \\[   V^\\pi(s)= \\mathbb{E}_\\pi\\!\\left[G_t \\mid S_t=s\\right].   \\] <p>Action-value function. The action-value function under policy \\(\\pi\\) is the expected return when starting in state \\(s\\), taking action \\(a\\) at time \\(t\\), and then following \\(\\pi\\) thereafter:</p> \\[   Q^\\pi(s,a)= \\mathbb{E}_\\pi\\!\\left[G_t \\mid S_t=s,\\,A_t=a\\right].   \\]"},{"location":"RL-2-Markov%20Decision%20Processes/#bellman-expectation-equations-in-mdps","title":"Bellman expectation equations in MDPs","text":"<p>The derivations mirror the MRP case: the return satisfies the recursion \\(G_t=R_{t+1}+\\gamma G_{t+1}\\), and we take conditional expectations. The difference is that in an MDP the next state and reward depend on the action, and actions are chosen according to the policy \\(\\pi\\).</p> <p>Bellman expectation equation for \\(V^\\pi\\). Start from</p> \\[   V^\\pi(s)= \\mathbb E_\\pi[G_t\\mid S_t=s],\\qquad G_t=R_{t+1}+\\gamma G_{t+1}.   \\] <p>Taking \\(\\mathbb E_\\pi[\\cdot\\mid S_t=s]\\) gives</p> \\[   V^\\pi(s)=\\mathbb E_\\pi[R_{t+1}\\mid S_t=s]+\\gamma\\,\\mathbb E_\\pi[G_{t+1}\\mid S_t=s].   \\] <p>Condition on the first action \\(A_t\\) and use \\(\\Pr(A_t=a\\mid S_t=s)=\\pi(a\\mid s)\\):</p> \\[   \\mathbb E_\\pi[R_{t+1}\\mid S_t=s] =\\sum_{a\\in\\mathcal A}\\pi(a\\mid s)\\,\\mathbb E[R_{t+1}\\mid S_t=s,A_t=a] =\\sum_{a}\\pi(a\\mid s)\\,R(s,a).   \\] <p>Similarly, expand \\(\\mathbb E_\\pi[G_{t+1}\\mid S_t=s]\\) by conditioning on the first action and next state. First apply the law of total expectation over \\(A_t\\) (proof given in appendix):</p> \\[   \\mathbb E_\\pi[G_{t+1}\\mid S_t=s] =\\sum_{a\\in\\mathcal A}\\Pr_\\pi(A_t=a\\mid S_t=s)\\,\\mathbb E_\\pi[G_{t+1}\\mid S_t=s,A_t=a] =\\sum_{a\\in\\mathcal A}\\pi(a\\mid s)\\,\\mathbb E_\\pi[G_{t+1}\\mid S_t=s,A_t=a].   \\] <p>Next, for each fixed \\((s,a)\\), apply the law of total expectation over \\(S_{t+1}\\):</p> \\[   \\mathbb E_\\pi[G_{t+1}\\mid S_t=s,A_t=a] =\\sum_{s'\\in\\mathcal S}\\Pr(S_{t+1}=s'\\mid S_t=s,A_t=a)\\, \\mathbb E_\\pi[G_{t+1}\\mid S_t=s,A_t=a,S_{t+1}=s'].   \\] <p>Using the Markov property, once we condition on \\(S_{t+1}=s'\\), the future return from time \\(t+1\\) onward does not depend on \\((S_t,A_t)\\), so</p> \\[   \\mathbb E_\\pi[G_{t+1}\\mid S_t=s,A_t=a,S_{t+1}=s'] =\\mathbb E_\\pi[G_{t+1}\\mid S_{t+1}=s'] =V^\\pi(s').   \\] <p>Therefore,</p> \\[   \\mathbb E_\\pi[G_{t+1}\\mid S_t=s,A_t=a] =\\sum_{s'\\in\\mathcal S}P(s'\\mid s,a)\\,V^\\pi(s'),   \\] <p>and substituting back yields</p> \\[   \\mathbb E_\\pi[G_{t+1}\\mid S_t=s] =\\sum_{a\\in\\mathcal A}\\pi(a\\mid s)\\sum_{s'\\in\\mathcal S}P(s'\\mid s,a)\\,V^\\pi(s').   \\] <p>Substituting these two expressions yields the Bellman expectation equation:</p> \\[   V^\\pi(s) = \\sum_{a\\in\\mathcal A}\\pi(a\\mid s)\\left[\\,R(s,a)+\\gamma\\sum_{s'\\in\\mathcal S}P(s'\\mid s,a)\\,V^\\pi(s')\\right].   \\] <p>Bellman expectation equation for \\(Q^\\pi\\). Define</p> \\[   Q^\\pi(s,a)= \\mathbb E_\\pi[G_t\\mid S_t=s,A_t=a].   \\] <p>Using \\(G_t=R_{t+1}+\\gamma G_{t+1}\\) and conditioning on \\(S_{t+1}\\),</p> \\[   Q^\\pi(s,a) =\\mathbb E[R_{t+1}\\mid S_t=s,A_t=a]+\\gamma\\,\\mathbb E_\\pi[G_{t+1}\\mid S_t=s,A_t=a] =R(s,a)+\\gamma\\sum_{s'}P(s'\\mid s,a)\\,V^\\pi(s').   \\] <p>Using \\(V^\\pi(s')=\\sum_{a'}\\pi(a'\\mid s')Q^\\pi(s',a')\\), this can be written as a recursion in \\(Q^\\pi\\) alone:</p> \\[   Q^\\pi(s,a)=R(s,a)+\\gamma\\sum_{s'}P(s'\\mid s,a)\\sum_{a'}\\pi(a'\\mid s')\\,Q^\\pi(s',a').   \\] <p>Relationship between \\(V^\\pi\\) and \\(Q^\\pi\\). Probability-weighted sum of \\(Q^\\pi(s,a)\\) over the action drawn from \\(\\pi(\\cdot\\mid s)\\) recovers \\(V^\\pi(s)\\):</p> \\[   V^\\pi(s)=\\sum_{a\\in\\mathcal A}\\pi(a\\mid s)\\,Q^\\pi(s,a).   \\] <p>These equations reduce to the original MRP Bellman recursion when there is only one available action (no control).</p>"},{"location":"RL-2-Markov%20Decision%20Processes/#matrix-form-of-vpi-and-qpi-finite-mdp","title":"Matrix form of \\(V^\\pi\\) and \\(Q^\\pi\\) (finite MDP)","text":"<p>Assume \\(|\\mathcal S|=n\\) and \\(|\\mathcal A|=m\\). Stack the state-values into a vector \\(v^\\pi\\in\\mathbb R^n\\) with entries \\(v^\\pi_s=V^\\pi(s)\\).</p> <p>Induced MRP (state-value). Define the policy-induced transition matrix \\(\\mathcal P^\\pi\\in\\mathbb R^{n\\times n}\\) and reward vector \\(\\mathcal R^\\pi\\in\\mathbb R^n\\) by</p> \\[   \\mathcal P^\\pi_{ss'}= \\sum_{a\\in\\mathcal A}\\pi(a\\mid s)\\,\\mathcal P^{a}_{ss'}, \\qquad \\mathcal R^\\pi_s= \\sum_{a\\in\\mathcal A}\\pi(a\\mid s)\\,\\mathcal R(s,a),   \\] <p>(analogously if rewards depend on \\((s,a,s')\\)). Then the Bellman expectation equation is</p> \\[   v^\\pi=\\mathcal R^\\pi+\\gamma \\mathcal P^\\pi v^\\pi \\quad\\Longrightarrow\\quad v^\\pi=(I-\\gamma \\mathcal P^\\pi)^{-1}\\mathcal R^\\pi,   \\] <p>when \\((I-\\gamma \\mathcal P^\\pi)\\) is invertible (e.g. for \\(\\gamma\\in[0,1)\\) in the finite case).</p> <p>Action-value (state--action form). Stack action-values into a vector \\(q^\\pi\\in\\mathbb R^{nm}\\) indexed by \\((s,a)\\) with entries \\(q^\\pi_{(s,a)}=Q^\\pi(s,a)\\). Define</p> \\[   r_{(s,a)}= \\mathcal R(s,a), \\qquad \\mathcal P_{(s,a),s'}= P(s'\\mid s,a),   \\] <p>and let \\(\\Pi\\in\\mathbb R^{n\\times (nm)}\\) be the policy-averaging matrix with entries</p> \\[   \\Pi_{s,(s,a)}= \\pi(a\\mid s).   \\] <p>Then \\(v^\\pi=\\Pi q^\\pi\\) and the Bellman equation \\(Q^\\pi(s,a)=\\mathcal R(s,a)+\\gamma\\sum_{s'}P(s'\\mid s,a)V^\\pi(s')\\) becomes</p> \\[   q^\\pi = r + \\gamma\\,\\mathcal P\\,v^\\pi       = r + \\gamma\\,\\mathcal P\\,\\Pi\\,q^\\pi \\quad\\Longrightarrow\\quad q^\\pi=(I-\\gamma \\mathcal P\\Pi)^{-1}r,   \\] <p>when the inverse exists.</p>"},{"location":"RL-2-Markov%20Decision%20Processes/#policy-evaluation","title":"Policy evaluation","text":"<p>In the previous section we derived the Bellman expectation equations for an MDP under a fixed policy \\(\\pi\\), and we ended by writing them in matrix form. At this point, it helps to pause and make explicit what we are actually doing: we are no longer making decisions. The policy \\(\\pi\\) has already committed to how actions are chosen in each state, so the only thing left is to measure how good that committed behavior is. This is called Policy evaluation:</p> \\[   \\text{given }\\pi,\\ \\text{compute }V^\\pi\\text{ and }Q^\\pi.   \\] <p>In other words, we are simply asking what long-run discounted return this particular policy achieves. In the finite discounted setting, the matrix form is not just a compact way to write the Bellman equations: it also tells us something reassuring. There is exactly one function \\(V^\\pi\\) that satisfies them, so policy evaluation is not ambiguous.</p> <p>Existence and uniqueness of \\(V^\\pi\\) (finite discounted case) \\label{prop:eval-unique} Assume \\(|\\mathcal S|&lt;\\infty\\), \\(|\\mathcal A|&lt;\\infty\\), and \\(\\gamma\\in[0,1)\\). For any fixed policy \\(\\pi\\), the Bellman expectation equations admit a unique solution \\(V:\\mathcal S\\to\\mathbb R\\). This solution is exactly the value function \\(V^\\pi\\). The same statement can be made for \\(Q^\\pi\\). (Proof is beyond our scope)</p> <p>So far, we have been answering: \"If I behave according to \\(\\pi\\), what do I get?\" The next step is the real control question: \"Can we do better?\" That means comparing policies and pushing performance as high as possible. This is where optimal value functions and an optimal policy enter, and where the Bellman equations change from averaging over actions to taking a maximum.</p>"},{"location":"RL-2-Markov%20Decision%20Processes/#optimality-in-mdps-from-evaluation-to-control","title":"Optimality in MDPs: from evaluation to control","text":"<p>So far we have focused on policy evaluation: given a fixed policy \\(\\pi\\), we quantify its long-term performance via</p> \\[ V^\\pi(s)=\\mathbb{E}_\\pi[G_t\\mid S_t=s], \\qquad Q^\\pi(s,a)=\\mathbb{E}_\\pi[G_t\\mid S_t=s,A_t=a]. \\] <p>The control problem asks the complementary question: among all policies, which behavior is best?</p>"},{"location":"RL-2-Markov%20Decision%20Processes/#optimal-value-functions-and-optimal-policies","title":"Optimal value functions and optimal policies","text":"<p>Let \\(\\Pi\\) denote the set of all (possibly stochastic) policies. Define the optimal state-value function and optimal action-value function by</p> \\[ \\begin{aligned} V^*(s)&amp;= \\sup_{\\pi\\in\\Pi} V^\\pi(s),\\\\ Q^*(s,a)&amp;= \\sup_{\\pi\\in\\Pi} Q^\\pi(s,a). \\end{aligned} \\] <p>Equivalently,</p> \\[ V^*(s)=\\sup_{\\pi\\in\\Pi}\\mathbb{E}_\\pi\\!\\left[\\sum_{k=0}^{\\infty}\\gamma^k R_{t+1+k}\\,\\middle|\\,S_t=s\\right], \\] <p>and</p> \\[ Q^*(s,a)=\\sup_{\\pi\\in\\Pi}\\mathbb{E}_\\pi\\!\\left[\\sum_{k=0}^{\\infty}\\gamma^k R_{t+1+k}\\,\\middle|\\,S_t=s,A_t=a\\right]. \\] <p>Intuitively, \\(Q^*(s,a)\\) is the best achievable return if we force the first action to be \\(a\\) in state \\(s\\) and then behave optimally thereafter.</p> <p>Optimal policy. A policy \\(\\pi^*\\) is optimal if it achieves the optimal value from every state:</p> \\[ V^{\\pi^*}(s)=V^*(s)\\quad\\text{for all }s\\in\\mathcal S. \\] <p>Equivalently,</p> \\[ Q^{\\pi^*}(s,a)=Q^*(s,a)\\quad\\text{for all }(s,a)\\in\\mathcal S\\times\\mathcal A. \\] <p>Existence (finite discounted case). In finite discounted MDPs (\\(|\\mathcal S|&lt;\\infty\\), \\(|\\mathcal A|&lt;\\infty\\), \\(\\gamma\\in[0,1)\\)), there exists at least one optimal policy. Moreover, there always exists an optimal deterministic stationary policy. Optimal policies need not be unique: it is common to have multiple distinct policies that all achieve \\(V^*\\). (Proof is beyond our scope)</p> <p>In particular, in this setting the suprema above are attained, so we may write</p> \\[ V^*(s)=\\max_{\\pi\\in\\Pi}V^\\pi(s),\\qquad Q^*(s,a)=\\max_{\\pi\\in\\Pi}Q^\\pi(s,a). \\] <p>Why \\(Q^*\\) is especially useful. Since \\(Q^*(s,a)\\) already accounts for optimal future behavior, choosing an action that maximizes \\(Q^*(s,a)\\) is optimal for the current state:</p> \\[ a^*(s)\\in\\arg\\max_{a\\in\\mathcal A} Q^*(s,a), \\qquad \\pi^*(a\\mid s)= \\begin{cases} 1, &amp; a\\in\\arg\\max_{a'} Q^*(s,a'),\\\\ 0, &amp; \\text{otherwise}. \\end{cases} \\]"},{"location":"RL-2-Markov%20Decision%20Processes/#bellman-optimality-equations","title":"Bellman optimality equations","text":"<p>The derivation mirrors the Bellman expectation equations. Start from the return recursion</p> \\[ G_t = R_{t+1}+\\gamma G_{t+1}, \\] <p>take conditional expectations, and apply the law of total expectation over the next state. The only conceptual change is optimality: instead of averaging over actions using \\(\\pi(\\cdot\\mid s)\\), we select the action that maximizes the expected continuation value.</p> <p>Optimal state-value recursion. If we take action \\(a\\) now in state \\(s\\) and then behave optimally thereafter, the expected return is</p> \\[ R(s,a)+\\gamma\\sum_{s'}P(s'\\mid s,a)\\,V^*(s'). \\] <p>Choosing the best action gives</p> \\[ V^*(s)=\\max_{a\\in\\mathcal A}\\left[R(s,a)+\\gamma\\sum_{s'}P(s'\\mid s,a)\\,V^*(s')\\right]. \\] <p>Optimal action-value recursion. If the first action is fixed to be \\(a\\) in state \\(s\\), then after transitioning to \\(s'\\) optimal behavior chooses the best next action:</p> \\[ Q^*(s,a)=R(s,a)+\\gamma\\sum_{s'}P(s'\\mid s,a)\\,\\max_{a'}Q^*(s',a'). \\] <p>Consistency and greedy optimality. The link between \\(V^*\\) and \\(Q^*\\) is immediate:</p> \\[ V^*(s)=\\max_{a\\in\\mathcal A}Q^*(s,a), \\qquad \\pi^*(s)\\in\\arg\\max_{a\\in\\mathcal A}Q^*(s,a). \\] <p>and plugging this into the \\(Q^*\\) equation gives the full recursion:</p> \\[ Q^*(s,a) = R(s,a) + \\gamma \\sum_{s'} P(s'\\mid s,a)\\,V^*(s') \\quad\\text{with}\\quad V^*(s')=\\max_{a'}Q^*(s',a'). \\] <p>Indeed, by definition \\(Q^*(s,a)\\) is the optimal return conditional on taking \\(a\\) first, so the best achievable value from \\(s\\) is obtained by selecting the best first action, which gives \\(V^*(s)=\\max_a Q^*(s,a)\\). Conversely, any policy that always picks an action attaining this maximum in each state is optimal, because \\(Q^*\\) already bakes in optimal behavior after the first step. The Bellman optimality equations therefore differ from the expectation equations in one key way: the expectation over actions is replaced by a maximization, reflecting the agent's ability to choose.</p>"},{"location":"RL-2-Markov%20Decision%20Processes/#solving-bellman-optimality","title":"Solving Bellman optimality","text":"<p>The optimality equations include \\(\\max\\) operators, e.g.</p> \\[ V = \\max_a \\left(R^a + \\gamma P^a V\\right), \\] <p>where the max is applied component-wise across actions. This makes the system not linear in \\(V\\) (unlike the MRP case \\((I-\\gamma P)V=R\\)). So, in general, there is no single matrix inverse expression like \\((I-\\gamma P)^{-1}R\\).</p>"},{"location":"RL-2-Markov%20Decision%20Processes/#when-is-a-closed-form-possible","title":"When is a closed form possible?","text":"<p>Closed form becomes possible once the \\(\\max\\) is removed, which effectively happens when:</p> <p>(1) The optimal action is known in advance. If we already know which action \\(a^*(s)\\) is optimal in each state, then the MDP reduces to the MRP induced by the deterministic policy \\(\\pi^*(s)=a^*(s)\\):</p> \\[ P^{\\pi^*}(s'\\mid s)=P(s'\\mid s,a^*(s)),\\qquad R^{\\pi^*}(s)=R(s,a^*(s)). \\] <p>Then</p> \\[ V^* = V^{\\pi^*} = (I-\\gamma P^{\\pi^*})^{-1} R^{\\pi^*}. \\] <p>But notice: knowing \\(a^*(s)\\) is basically the problem we were trying to solve.</p> <p>(2) Tiny MDPs with simple structure. For very small state spaces, we can solve piecewise: assume a maximizing action in each state, solve the corresponding linear system, then check whether the assumed actions are truly maximizing. This is feasible only for small problems because the number of action-combinations grows exponentially.</p> <p>(3) Special cases / restricted classes. Some structured MDPs admit analytic solutions (certain deterministic shortest-path forms, special linear-quadratic control in continuous settings, etc.), but for general finite stochastic MDPs the standard approach is iterative computation.</p>"},{"location":"RL-2-Markov%20Decision%20Processes/#iterative-solution-methods","title":"Iterative solution methods","text":"<p>Model-based vs model-free</p> <ul> <li>Model-based: uses \\(P\\) and \\(R\\) explicitly (value iteration, policy iteration, DP).</li> <li>Model-free: learns from sampled transitions without explicit \\(P\\) (Q-learning, SARSA).</li> </ul> <p>Value Iteration (VI)</p> <p>Value iteration applies the Bellman optimality backup repeatedly:</p> \\[ V_{k+1}(s) \\leftarrow \\max_a\\left[ R(s,a)+\\gamma\\sum_{s'}P(s'\\mid s,a)V_k(s') \\right]. \\] <p>Intuition: repeatedly perform one-step lookahead improvements until the values stabilize.</p> <p>Policy Iteration (PI)</p> <p>Policy iteration alternates two steps:</p> <ol> <li>Policy evaluation: compute \\(V^{\\pi}\\) for the current policy \\(\\pi\\) (this is linear, like an MRP),</li> <li>Policy improvement: make the policy greedy w.r.t. current values:</li> </ol> \\[ \\pi_{\\text{new}}(s)\\in\\arg\\max_a\\left[R(s,a)+\\gamma\\sum_{s'}P(s'\\mid s,a)V^\\pi(s')\\right]. \\] <p>Repeat until the policy stops changing (then it is optimal).</p> <p>Q-learning (off-policy, model-free)</p> <p>Q-learning aims to learn \\(Q^*\\) directly from samples \\((S_t,A_t,R_{t+1},S_{t+1})\\):</p> \\[ Q(S_t,A_t)\\leftarrow Q(S_t,A_t) + \\alpha\\Big(R_{t+1}+\\gamma\\max_{a'}Q(S_{t+1},a')-Q(S_t,A_t)\\Big). \\] <p>It uses a greedy max in the target, so it can learn the optimal greedy behavior even if the behavior policy explores.</p> <p>SARSA (on-policy, model-free)</p> <p>SARSA learns the value of the current behavior policy:</p> \\[ Q(S_t,A_t)\\leftarrow Q(S_t,A_t) + \\alpha\\Big(R_{t+1}+\\gamma Q(S_{t+1},A_{t+1})-Q(S_t,A_t)\\Big). \\] <p>Key difference: it uses the next action actually taken \\((A_{t+1})\\), rather than \\(\\max_{a'}\\).</p>"},{"location":"RL-2-Markov%20Decision%20Processes/#appendix","title":"Appendix","text":"<p>Proof of Law of total expectation</p> <p>We want to show that:</p> \\[ \\mathbb{E}[X \\mid Y] = \\sum_{z} \\Pr(Z=z \\mid Y)\\,\\mathbb{E}[X \\mid Z=z, Y]. \\] <p>given \\(Z\\) is discrete (countable range) and \\(\\mathbb{E}[|X|]&lt;\\infty\\). We prove the identity pointwise in \\(y\\). Fix any \\(y\\) with \\(\\Pr(Y=y)&gt;0\\). For simplicity we write conditional probabilities given \\(Y=y\\).</p> <p>Proof:</p> <p>First, by the definition of conditional expectation for a discrete \\(X\\),</p> \\[ \\mathbb{E}[X\\mid Y=y]=\\sum_{x} x\\,\\Pr(X=x\\mid Y=y). \\] <p>Next, since the events \\(\\{Z=z\\}\\) form a disjoint partition, for each \\(x\\) we have</p> \\[ \\Pr(X=x\\mid Y=y)=\\sum_{z}\\Pr(X=x, Z=z\\mid Y=y). \\] <p>We now use the conditional product rule (chain rule). For events \\(A,C,B\\) with \\(\\Pr(B)&gt;0\\) and \\(\\Pr(C\\cap B)&gt;0\\),</p> \\[ \\Pr(A,C\\mid B)=\\Pr(C\\mid B)\\Pr(A\\mid C,B), \\] <p>because</p> \\[ \\Pr(C\\mid B)\\Pr(A\\mid C,B) =\\frac{\\Pr(C\\cap B)}{\\Pr(B)}\\cdot \\frac{\\Pr(A\\cap C\\cap B)}{\\Pr(C\\cap B)} =\\frac{\\Pr(A\\cap C\\cap B)}{\\Pr(B)} =\\Pr(A\\cap C\\mid B). \\] <p>Apply this with \\(A=\\{X=x\\}\\), \\(C=\\{Z=z\\}\\), and \\(B=\\{Y=y\\}\\) to get</p> \\[ \\Pr(X=x, Z=z\\mid Y=y)=\\Pr(Z=z\\mid Y=y)\\,\\Pr(X=x\\mid Z=z, Y=y). \\] <p>Substituting into the previous expression gives</p> \\[ \\Pr(X=x\\mid Y=y)=\\sum_{z}\\Pr(Z=z\\mid Y=y)\\,\\Pr(X=x\\mid Z=z, Y=y). \\] <p>Plug this into the definition of \\(\\mathbb{E}[X\\mid Y=y]\\):</p> \\[ \\mathbb{E}[X\\mid Y=y] =\\sum_x x\\left[\\sum_{z}\\Pr(Z=z\\mid Y=y)\\,\\Pr(X=x\\mid Z=z, Y=y)\\right]. \\] <p>Rearrange the sums (justified by integrability; e.g. by applying the argument to \\(X^+\\) and \\(X^-\\) separately):</p> \\[ \\mathbb{E}[X\\mid Y=y] =\\sum_{z}\\Pr(Z=z\\mid Y=y)\\left[\\sum_x x\\,\\Pr(X=x\\mid Z=z, Y=y)\\right]. \\] <p>The bracketed term is the definition of \\(\\mathbb{E}[X\\mid Z=z, Y=y]\\), so we conclude</p> \\[ \\mathbb{E}[X\\mid Y=y]=\\sum_{z}\\Pr(Z=z\\mid Y=y)\\,\\mathbb{E}[X\\mid Z=z, Y=y]. \\] <p>Since this holds for every \\(y\\) with \\(\\Pr(Y=y)&gt;0\\), replacing \\(y\\) by the random variable \\(Y\\) yields the desired identity</p> \\[ \\mathbb{E}[X\\mid Y]=\\sum_{z}\\Pr(Z=z\\mid Y)\\,\\mathbb{E}[X\\mid Z=z, Y]. \\]"},{"location":"RL-2-Markov%20Decision%20Processes/#references","title":"References","text":"<ul> <li>https://github.com/zyxue/youtube_RL_course_by_David_Silver</li> </ul>"},{"location":"RL-3-Partially%20Observable%20MDP/","title":"Partially observable Markov decision process (POMDP)","text":"<p>A POMDP is an MDP in which the true state is hidden. The underlying system still evolves as a Markov chain controlled by actions, but the agent does not observe \\(S_t\\) directly. Instead it receives an observation \\(O_t\\) that provides only partial information about the state. Equivalently, a POMDP can be viewed as a hidden Markov model (HMM) with actions.</p>"},{"location":"RL-3-Partially%20Observable%20MDP/#definition","title":"Definition","text":"<p>A POMDP is a tuple</p> \\[ \\langle \\mathcal S,\\mathcal A,\\mathcal O,\\mathcal P,\\mathcal R,\\mathcal Z,\\gamma\\rangle, \\] <p>where</p> <ul> <li>\\(\\mathcal S\\) is a finite set of (hidden) states,</li> <li>\\(\\mathcal A\\) is a finite set of actions,</li> <li>\\(\\mathcal O\\) is a finite set of observations,</li> <li>\\(\\mathcal P\\) is the controlled transition model:</li> </ul> \\[ \\mathcal P^{a}_{ss'} \\;=\\; \\mathbb P\\!\\bigl(S_{t+1}=s'\\mid S_t=s, A_t=a\\bigr), \\] <ul> <li>\\(\\mathcal R\\) is the reward model (expected one-step reward):</li> </ul> \\[ \\mathcal R^{a}_{s} \\;=\\; \\mathbb E\\!\\bigl[R_{t+1}\\mid S_t=s, A_t=a\\bigr], \\] <ul> <li>\\(\\mathcal Z\\) is the observation (emission) model:</li> </ul> \\[ \\mathcal Z^{a}_{s'o} \\;=\\; \\mathbb P\\!\\bigl(O_{t+1}=o\\mid S_{t+1}=s', A_t=a\\bigr), \\] <ul> <li>\\(\\gamma\\in[0,1]\\) is the discount factor.</li> </ul>"},{"location":"RL-3-Partially%20Observable%20MDP/#what-changes-relative-to-an-mdp","title":"What changes relative to an MDP","text":"<p>The dynamics and rewards are still defined on the (hidden) state \\(S_t\\), but the agent only observes the stream \\(O_t\\). Consequently, policies are typically defined on the agent's information (e.g. the observation--action history, or a belief state), rather than directly on \\(S_t\\).</p>"},{"location":"RL-3-Partially%20Observable%20MDP/#agent-information-histories-and-belief-states","title":"Agent information: histories and belief states","text":"<p>Histories.</p> <p>Because the true state is not directly observed in a POMDP, the agent\u2019s information at time \\(t\\) is the entire sequence of past interactions. A history \\(H_t\\) is the sequence of actions, observations, and rewards up to time \\(t\\):</p> \\[ H_t = (A_0, O_1, R_1, \\ldots, A_{t-1}, O_t, R_t). \\] <p>In general, optimal decisions may depend on the full history, not just the most recent observation.</p> <p>Belief states.</p> <p>Rather than working directly with histories, it is convenient to summarize all relevant information in a single object. The belief state associated with a history \\(h\\) is the conditional distribution of the current (hidden) state given that history:</p> \\[ b(h) \\;=\\; \\bigl(\\mathbb P[S_t=s^1\\mid H_t=h],\\,\\ldots,\\,\\mathbb P[S_t=s^n\\mid H_t=h]\\bigr). \\] <p>When \\(\\mathcal S=\\{s^1,\\ldots,s^n\\}\\) is finite, \\(b(h)\\) is a probability mass function (PMF) over \\(\\mathcal S\\): it assigns a probability to each discrete state and satisfies \\(b_i(h)\\ge 0\\) and \\(\\sum_{i=1}^n b_i(h)=1\\). Writing</p> \\[ b(h)=(b_1(h),\\ldots,b_n(h)), \\qquad b_i(h)\\;=\\;\\mathbb P\\!\\bigl(S_t=s^i \\mid H_t=h\\bigr),\\ i=1,\\ldots,n, \\] <p>the belief represents the agent's uncertainty about the true state after observing \\(h\\).</p> <p>Why belief states matter.</p> <p>The belief state is a sufficient statistic for the history: for any action, the distribution of future states/observations/rewards depends on the past only through \\(b(h)\\). This lets us treat a POMDP as a fully observable MDP whose ``state'' is the belief. Consequently, policies can be taken to depend on \\(b(h)\\) rather than on the entire history.</p>"},{"location":"RL-3-Partially%20Observable%20MDP/#long-run-behavior-under-a-fixed-policy-how-ergodicity-shows-up","title":"Long-run behavior under a fixed policy: how ergodicity shows up","text":"<p>Once a policy \\(\\pi\\) is fixed, the interaction between the agent and the environment induces a time-homogeneous stochastic process. In particular:</p> <ul> <li>in an MDP, the state sequence \\((S_t)\\) becomes a Markov chain under \\(\\pi\\);</li> <li>in a POMDP, although \\(S_t\\) is hidden, the underlying system still evolves Markovly, and it is   often useful to analyze the long-run behavior of the induced controlled dynamics (e.g. via the   belief-MDP viewpoint).</li> </ul> <p>This is why concepts like ergodicity and stationary distributions are natural: they formalize when the process ``forgets'' its initial condition and admits well-defined long-run averages (such as average reward).</p> <p>Ergodic Markov processes.</p> <p>A Markov process is called ergodic if it satisfies:</p> <ul> <li>Recurrence: every state is visited infinitely often (with probability one).</li> <li>Aperiodicity: returns to each state do not occur in a fixed cycle or period.</li> </ul> <p>Intuitively, an ergodic Markov process keeps exploring the entire state space forever and does not get trapped in cyclic behavior.</p> <p>Stationary distribution.</p> <p>A probability distribution \\(d^\\pi\\) over \\(\\mathcal S\\) is stationary if it is invariant under the transition dynamics:</p> \\[ d^\\pi(s) \\;=\\; \\sum_{s'\\in\\mathcal S} d^\\pi(s')\\,P_{s's}, \\qquad P_{s's}=\\mathbb P(S_{t+1}=s \\mid S_t=s'). \\] <p>Fundamental theorem.</p> <p>If a Markov process is ergodic, then it admits a unique stationary distribution \\(d^\\pi\\), and the state distribution converges to it regardless of the initial state:</p> \\[ \\mathbb P(S_t=s) \\;\\longrightarrow\\; d^\\pi(s)\\quad\\text{as }t\\to\\infty. \\] <p>Interpretation.</p> <p>The stationary distribution \\(d^\\pi(s)\\) describes the long-run fraction of time the process spends in state \\(s\\). In reinforcement learning, this distribution is central for defining long-run averages and understanding the behavior induced by a fixed policy (including, via the belief-MDP view, in POMDPs).</p>"},{"location":"RL-3-Partially%20Observable%20MDP/#ergodic-mdps-and-the-average-reward-objective","title":"Ergodic MDPs and the average-reward objective","text":"<p>Ergodic MDPs.</p> <p>An MDP is ergodic if, for every policy \\(\\pi\\), the Markov chain over states induced by following \\(\\pi\\) is ergodic. Equivalently, under any fixed policy the resulting state process is recurrent and aperiodic, so it continues to explore the entire state space without getting trapped or cycling deterministically.</p> <p>Average reward.</p> <p>In an ergodic MDP, long-run time averages are well defined. For any policy \\(\\pi\\), the average reward per time step is</p> \\[ \\rho^\\pi \\;=\\; \\lim_{T\\to\\infty} \\frac{1}{T}\\,\\mathbb E\\!\\left[\\sum_{t=1}^{T} R_t\\right]. \\] <p>A key consequence of ergodicity is that this limit exists and is independent of the initial state: because the induced Markov chain forgets where it started, the long-run average reward depends only on the policy \\(\\pi\\).</p> <p>Connection to stationary distributions.</p> <p>For a fixed policy \\(\\pi\\), let \\(d^\\pi\\) denote the stationary distribution of the induced Markov chain. Then the average reward can be written as</p> \\[ \\rho^\\pi \\;=\\; \\sum_{s\\in\\mathcal S} d^\\pi(s)\\sum_{a\\in\\mathcal A}\\pi(a\\mid s)\\,R(s,a). \\] <p>This makes explicit that \\(\\rho^\\pi\\) is the expected reward under the steady-state behavior of the system, and it is the basis of average-reward reinforcement learning methods. In the POMDP setting, the same idea is often applied after reducing to the belief-MDP (where the ``state'' is \\(b(h)\\)).</p>"},{"location":"RL-3-Partially%20Observable%20MDP/#average-reward-value-differential-value-and-bellman-equation","title":"Average-reward value (differential value) and Bellman equation","text":"<p>In the undiscounted setting, a common performance criterion for a policy \\(\\pi\\) is its average reward per time step</p> \\[ \\rho^\\pi \\;=\\; \\lim_{T\\to\\infty}\\frac{1}{T}\\,\\mathbb E_\\pi\\!\\left[\\sum_{t=1}^{T} R_t\\right], \\] <p>(when the limit is well defined, e.g. under ergodicity). This scalar captures steady-state performance, but it does not describe how trajectories from a particular starting state compare to that baseline.</p> <p>Average-reward (differential) value function.</p> <p>To quantify start-dependent deviations from the baseline \\(\\rho^\\pi\\), we define the average-reward value function (or differential value function) as</p> \\[ \\tilde v_\\pi(s) \\;=\\; \\mathbb E_\\pi\\!\\left[ \\sum_{k=1}^{\\infty}\\bigl(R_{t+k}-\\rho^\\pi\\bigr) \\;\\middle|\\; S_t=s \\right]. \\] <p>It is the expected cumulative excess reward when starting from \\(s\\), obtained by subtracting \\(\\rho^\\pi\\) at each step and summing the resulting centered rewards.</p> <p>Interpretation.</p> <p>A positive \\(\\tilde v_\\pi(s)\\) indicates that trajectories starting from \\(s\\) tend to earn above-average reward (relative to \\(\\rho^\\pi\\)) over the transient phase, a negative value indicates below-average transient behavior. Since only these relative deviations matter, \\(\\tilde v_\\pi\\) is defined only up to an additive constant. One typically fixes a convention such as \\(\\tilde v_\\pi(s_0)=0\\) for a reference state \\(s_0\\) (or another normalization) to choose a unique representative.</p>"},{"location":"RL-3-Partially%20Observable%20MDP/#average-reward-bellman-equation","title":"Average-reward Bellman equation","text":"<p>Using the return recursion and subtracting \\(\\rho^\\pi\\), the value function satisfies the average-reward Bellman equation</p> \\[ \\tilde v_\\pi(s)= \\mathbb E_\\pi\\!\\left[ (R_{t+1}-\\rho^\\pi) + \\tilde v_\\pi(S_{t+1}) \\;\\middle|\\; S_t=s \\right]. \\] <p>This plays the same role as the Bellman expectation equation in the discounted setting and forms the basis of average-reward policy evaluation and control.</p>"},{"location":"RL-3-Partially%20Observable%20MDP/#conclusion","title":"Conclusion","text":"<p>A POMDP differs from an MDP only in what the agent observes: the true Markov state \\(S_t\\) is hidden, so policies must act on information such as histories \\(H_t\\) or (more conveniently) beliefs \\(b(h)\\). The belief is a sufficient statistic, which lets us reinterpret the POMDP as an MDP on beliefs. Once a policy is fixed, we obtain an induced Markov process (on states in an MDP, or on beliefs/augmented state in a POMDP viewpoint). If this induced process is ergodic, it has a unique stationary distribution \\(d^\\pi\\), so long-run averages such as the average reward \\(\\rho^\\pi\\) are well defined. The associated differential value function \\(\\tilde v_\\pi\\) then serves to refine this scalar baseline by quantifying, for each state \\(s\\), the expected cumulative deviation from \\(\\rho^\\pi\\) (i.e., which states are transiently better or worse than average). This state-dependent signal is what enables average-reward policy evaluation and policy improvement, and it is characterized by the average-reward Bellman equation.</p>"},{"location":"RL-3-Partially%20Observable%20MDP/#references","title":"References","text":"<ul> <li>https://github.com/zyxue/youtube_RL_course_by_David_Silver</li> </ul>"},{"location":"RL-4-Planning%20with%20Dynamic%20Programming/","title":"4 - Planning with Dynamic Programming","text":""},{"location":"RL-4-Planning%20with%20Dynamic%20Programming/#dynamic-programming","title":"Dynamic Programming","text":"<p>Dynamic programming (DP) is a method for solving optimization problems that unfold over stages (often interpreted as time). The term dynamic refers to this sequential structure: decisions made at one stage affect what is possible and valuable at later stages. The term programming is used in its classical sense from operations research: we are solving an optimization problem by searching over a space of decision rules. In many settings, those decision rules take the form of a policy, a rule that specifies what action to take in each state.</p>"},{"location":"RL-4-Planning%20with%20Dynamic%20Programming/#core-idea","title":"Core idea","text":"<p>DP makes difficult problems tractable by exploiting structure. Rather than optimizing over the entire problem at once, it:</p> <ul> <li>decomposes the problem into smaller subproblems (typically corresponding to different stages),</li> <li>solves those subproblems,</li> <li>and then combines their solutions to construct a solution to the original problem.</li> </ul> <p>The power of DP is not in the act of splitting alone, but in choosing a decomposition where the pieces can be solved efficiently and then reused.</p>"},{"location":"RL-4-Planning%20with%20Dynamic%20Programming/#principle-of-optimality","title":"Principle of Optimality","text":"<p>The principle of optimality states that optimal behavior is self-consistent over time: if we make an optimal decision now, then the remaining decisions must be optimal for the state that results. Equivalently, an optimal solution can be decomposed into an optimal first step and an optimal continuation. Concretely, consider an optimal policy \\(\\pi^*\\). From any state \\(s\\), its behavior can be viewed in two parts:</p> <ul> <li>the first action it takes in \\(s\\), and</li> <li>the continuation policy it follows after transitioning to a successor state \\(s'\\).</li> </ul> <p>Since the continuation problem is the same decision problem but with a new starting state, optimality from \\(s\\) implies optimality from the states that follow when \\(\\pi^*\\) is executed.</p>"},{"location":"RL-4-Planning%20with%20Dynamic%20Programming/#how-this-enables-dp","title":"How this enables DP","text":"<p>The principle of optimality is what makes a recursive decomposition feasible. It yields two structural advantages that DP exploits:</p> <p>Optimal substructure. Because the tail of an optimal solution must itself be optimal, we can solve the original problem by solving its subproblems. The natural subproblem is: starting from a state \\(s\\), what is the best achievable return from here onward? Once we can answer that question for every state, selecting an optimal first action becomes a local comparison of alternatives, each evaluated using the value of an optimal continuation.</p> <p>Overlapping subproblems. Many different histories can lead to the same state, meaning the same continuation problem appears repeatedly. DP avoids recomputing these repeated subproblems by storing their solutions and reusing them.</p>"},{"location":"RL-4-Planning%20with%20Dynamic%20Programming/#mdps-as-a-dp-setting","title":"MDPs as a DP setting","text":"<p>Markov Decision Processes fit this framework exactly. In an MDP, the \u201csituation we are in\u201d can be summarized by the current state \\(s\\) (the Markov property), so the relevant subproblem is well-defined: optimize future return starting from \\(s\\). The value function is therefore a direct representation of subproblem solutions:</p> \\[ v^*(s) \\;=\\; \\text{optimal expected return starting from state } s. \\] <p>The principle of optimality then justifies evaluating actions by combining immediate outcomes with the value of optimal continuation, which leads to the Bellman viewpoint that underlies planning algorithms.</p>"},{"location":"RL-4-Planning%20with%20Dynamic%20Programming/#a-consistency-consequence-along-reachable-states","title":"A consistency consequence along reachable states","text":"<p>A useful implication is that optimality must persist along the trajectories induced by an optimal policy. If a policy \\(\\pi\\) is optimal from a state \\(s\\) in the sense that</p> \\[ v_{\\pi}(s)=v^{*}(s), \\] <p>then it cannot \u201cfall behind\u201d after the first transition. In particular, for any successor state \\(s'\\) that can be reached from \\(s\\) when following \\(\\pi\\), the policy must also achieve optimal value:</p> \\[ v_{\\pi}(s')=v^{*}(s') \\qquad \\text{for every successor state $s'$ reachable from $s$ under $\\pi$.} \\] <p>Informally: if we are optimal now, we must remain optimal on every continuation. This is exactly what Bellman-style updates enforce across the state space. With this principle in place, we can now turn to planning in an MDP and see how DP implements these Bellman updates in practice.</p>"},{"location":"RL-4-Planning%20with%20Dynamic%20Programming/#planning-by-dynamic-programming-in-an-mdp","title":"Planning by Dynamic Programming in an MDP","text":"<p>In reinforcement learning, planning means computing decisions using a model of the environment, rather than updating solely from real experience. Concretely, a planning method takes as input an MDP model, for example the states, actions, a transition model, a reward model, and a discount factor and performs computation (often offline, or via simulated lookahead) to produce a decision rule, i.e., a policy. Dynamic Programming (DP) is the classical planning approach for MDPs when this model is available. Assuming we know all the model parameters (\\(\\mathcal{S}\\), \\(\\mathcal{A}\\), \\(P_{ss'}^{a}\\), \\(R_{s}^{a}\\), \\(\\gamma\\)), DP computes long-term consequences by exploiting Bellman recursions. These recursions express the value of a state (or state--action pair) in terms of the immediate reward and the discounted value of successor states, allowing DP algorithms to improve value estimates and policies through repeated one-step lookahead updates. Within this planning setting, DP algorithms are typically organized around two complementary tasks: prediction and control, which we define precisely next.</p>"},{"location":"RL-4-Planning%20with%20Dynamic%20Programming/#prediction-vs-control","title":"Prediction vs. control","text":"<p>It is useful to distinguish two closely related computational goals when we plan with a known MDP model:</p> <ul> <li>Prediction: the policy \\(\\pi\\) is fixed. The task is to evaluate it by computing its value function \\(v_{\\pi}\\), which gives the expected discounted return from each state when actions are chosen according to \\(\\pi\\).</li> <li>Control: the policy is not fixed. The task is to find the optimal value function \\(v^{*}\\) and an optimal policy \\(\\pi^{*}\\) that achieves it.</li> </ul> <p>These goals correspond to different Bellman operators. Prediction uses Bellman expectation updates, which evaluate a given policy via model-based expectations, whereas control uses Bellman optimality updates, which push values toward the best achievable behavior and thereby support improvement toward an optimal policy which is the ultimate goal of planning.</p>"},{"location":"RL-4-Planning%20with%20Dynamic%20Programming/#iterative-policy-evaluation-prediction","title":"Iterative policy evaluation (prediction)","text":"<p>Assume we are given a policy \\(\\pi\\) and asked: \"If we follow \\(\\pi\\), what long-term return should we expect from each state?\" DP answers this by iteratively refining an estimate of \\(v_{\\pi}\\) using the Bellman expectation equation. One simple procedure is to begin with an initial guess, for instance</p> \\[ v_{1}(s)=0 \\qquad \\text{for all } s\\in\\mathcal{S}, \\] <p>and then repeatedly apply a one-step lookahead update. At iteration \\(k+1\\), for every state \\(s\\),</p> \\[ v_{k+1}(s)=\\sum_{a\\in\\mathcal{A}}\\pi(a\\mid s)\\Bigl(R_{s}^{a}+\\gamma\\sum_{s'\\in\\mathcal{S}}P_{ss'}^{a}\\,v_{k}(s')\\Bigr). \\] <p>This update has a direct interpretation: the value of a state equals the expected immediate reward plus the discounted value of the next state, where the expectation is taken over the action choice under \\(\\pi\\) and the environment dynamics. Repeating this update causes \\(v_k\\) to approach the true value function \\(v_{\\pi}\\).</p>"},{"location":"RL-4-Planning%20with%20Dynamic%20Programming/#synchronous-backups-how-the-updates-are-applied","title":"Synchronous backups (how the updates are applied)","text":"<p>We now describe a standard way to apply the update rule used in iterative policy evaluation in DP.</p> <p>Synchronous backups. At iteration \\(k+1\\), we perform a full sweep over the state space and update every state using only the values from the previous iterate:</p> \\[ \\forall s\\in\\mathcal{S}:\\quad v_{k+1}(s)\\;\\leftarrow\\;\\mathcal{T}^{\\pi}v_k(s), \\] <p>where \\(\\mathcal{T}^{\\pi}\\) is the Bellman expectation operator.</p> <p>It is worth emphasizing, in DP planning, the value function is primarily a tool for improvement: once we have an estimate of how good each state is under the current policy, we can use the model to ask a sharper question, namely whether another action would lead to a higher expected return. This question will be answered in the next section.</p>"},{"location":"RL-4-Planning%20with%20Dynamic%20Programming/#policy-iteration","title":"Policy Iteration","text":"<p>Up to now, we have used DP for prediction: given a fixed policy \\(\\pi\\), compute its value function \\(v_\\pi\\). We now turn to the central goal in an MDP: control, which answers the question of finding the best policy.</p> <p>The main idea behind policy iteration is simple: values enable improvement. If \\(v_\\pi(s)\\) tells us the expected long-term return from each state when following \\(\\pi\\), then we can use the model to ask, in every state, whether some other action would lead to a higher expected return. Policy iteration formalizes this as an alternating loop of evaluation and improvement.</p>"},{"location":"RL-4-Planning%20with%20Dynamic%20Programming/#the-two-step-loop","title":"The two-step loop","text":"<p>Policy iteration maintains a sequence of policies \\(\\pi_0,\\pi_1,\\pi_2,\\dots\\) and alternates between:</p> <ol> <li>Policy evaluation: given the current policy \\(\\pi_k\\), compute (or approximate) its value function \\(v_{\\pi_k}\\), typically by repeated Bellman expectation backups (to near convergence).</li> <li>Policy improvement: construct a new (often deterministic) policy by acting greedily with respect to \\(v_{\\pi_k}\\):</li> </ol> \\[ \\pi_{k+1}(s)\\in \\arg\\max_{a\\in\\mathcal{A}} \\Bigl(R_s^a+\\gamma\\sum_{s'\\in\\mathcal{S}}P_{ss'}^a\\,v_{\\pi_k}(s')\\Bigr). \\] <p>In other words: estimate how good our current policy is, then switch (state-by-state) to the action that looks best under that estimate, and repeat.</p>"},{"location":"RL-4-Planning%20with%20Dynamic%20Programming/#policy-improvement-why-greedy-is-safe","title":"Policy improvement: why greedy is safe","text":"<p>To make the improvement step precise, it is convenient to introduce the action-value function under \\(\\pi\\):</p> \\[ q_{\\pi}(s,a)=\\mathbb{E}_{\\pi}\\!\\left[\\,R_{t+1}+\\gamma v_{\\pi}(S_{t+1}) \\;\\middle|\\; S_t=s,\\;A_t=a\\,\\right]. \\] <p>This quantity means: take action \\(a\\) now, then follow \\(\\pi\\) thereafter.</p> <p>Greedy improvement. Given \\(q_\\pi\\), define a new policy \\(\\pi'\\) that chooses, in each state, an action maximizing \\(q_\\pi\\):</p> \\[ \\pi'(s)\\in \\arg\\max_{a\\in\\mathcal{A}} q_{\\pi}(s,a). \\] <p>Because \\(\\pi'(s)\\) is a maximizer, we immediately have, for every state \\(s\\),</p> \\[ q_{\\pi}\\bigl(s,\\pi'(s)\\bigr)=\\max_{a\\in\\mathcal{A}}q_{\\pi}(s,a)\\;\\ge\\; q_{\\pi}\\bigl(s,\\pi(s)\\bigr)=v_{\\pi}(s). \\] <p>Read this as a one-step statement:</p> <ul> <li>\\(q_{\\pi}(s,\\pi(s))\\) is \"do what \\(\\pi\\) would do now, then keep following \\(\\pi\\)\",</li> <li>\\(q_{\\pi}(s,\\pi'(s))\\) is \"do the greedy action now, then keep following \\(\\pi\\)\",</li> <li>greedy cannot be worse than \\(\\pi\\)'s own action under the same continuation \\(\\pi\\).</li> </ul> <p>From one step to the full return. The key point is that this one-step improvement can be unrolled over time. Starting from</p> \\[ v_{\\pi}(s)\\le q_{\\pi}\\bigl(s,\\pi'(s)\\bigr) =\\mathbb{E}_{\\pi'}\\!\\left[\\,R_{t+1}+\\gamma v_{\\pi}(S_{t+1}) \\;\\middle|\\; S_t=s\\,\\right], \\] <p>We apply the same greedy argument at subsequent states, repeatedly bounding \\(v_\\pi(S_{t+1})\\), then \\(v_\\pi(S_{t+2})\\), and so on.</p> \\[ \\begin{aligned} v_{\\pi}(s) &amp;\\le q_{\\pi}\\bigl(s,\\pi'(s)\\bigr) = \\mathbb{E}_{\\pi'}\\!\\left[\\,R_{t+1}+\\gamma v_{\\pi}(S_{t+1}) \\;\\middle|\\; S_t=s\\,\\right] \\\\ &amp;\\le \\mathbb{E}_{\\pi'}\\!\\left[\\,R_{t+1}+\\gamma q_{\\pi}\\bigl(S_{t+1},\\pi'(S_{t+1})\\bigr) \\;\\middle|\\; S_t=s\\,\\right] \\\\ &amp;\\le \\mathbb{E}_{\\pi'}\\!\\left[\\,R_{t+1}+\\gamma R_{t+2} +\\gamma^{2} q_{\\pi}\\bigl(S_{t+2},\\pi'(S_{t+2})\\bigr) \\;\\middle|\\; S_t=s\\,\\right] \\\\ &amp;\\le \\cdots \\\\ &amp;\\le \\mathbb{E}_{\\pi'}\\!\\left[\\,R_{t+1}+\\gamma R_{t+2}+\\cdots+\\gamma^{n-1}R_{t+n} +\\gamma^{n} v_{\\pi}(S_{t+n}) \\;\\middle|\\; S_t=s\\,\\right] \\\\ &amp;\\le \\mathbb{E}_{\\pi'}\\!\\left[\\,R_{t+1}+\\gamma R_{t+2}+\\cdots \\;\\middle|\\; S_t=s\\,\\right] = v_{\\pi'}(s). \\end{aligned} \\] <p>Taking the limit as \\(n\\to\\infty\\) (the residual term vanishes under discounting), we obtain the policy improvement guarantee:</p> \\[ v_{\\pi'}(s)\\;\\ge\\; v_{\\pi}(s)\\qquad \\forall s\\in\\mathcal{S}. \\] <p>Takeaway. Greedy policy improvement is monotone: it never makes the policy worse. At worst, it leaves values unchanged, otherwise, it strictly improves them in at least one state.</p>"},{"location":"RL-4-Planning%20with%20Dynamic%20Programming/#why-policy-iteration-stops-at-an-optimal-policy","title":"Why policy iteration stops at an optimal policy?","text":"<p>If the greedy improvement step leaves the policy unchanged, then policy iteration has reached a fixed point (and we can regard the algorithm as having converged). Equivalently, for every state \\(s\\in\\mathcal{S}\\),</p> \\[ \\pi(s)\\in \\arg\\max_{a\\in\\mathcal{A}} q_{\\pi}(s,a) \\quad\\Longrightarrow\\quad q_{\\pi}\\bigl(s,\\pi(s)\\bigr)=\\max_{a\\in\\mathcal{A}} q_{\\pi}(s,a). \\] <p>But \\(q_{\\pi}(s,\\pi(s))=v_{\\pi}(s)\\) by definition, so we have</p> \\[ v_{\\pi}(s)=\\max_{a\\in\\mathcal{A}} q_{\\pi}(s,a)\\qquad \\forall s\\in\\mathcal{S}. \\] <p>This is exactly the Bellman optimality condition: in each state, the value equals the best achievable one-step lookahead. A policy that is greedy with respect to its own value function is therefore optimal, which implies</p> \\[ v_{\\pi}(s)=v^{*}(s)\\qquad \\forall s\\in\\mathcal{S}, \\qquad\\text{and}\\qquad \\pi=\\pi^{*}. \\] <p>Intuition. The algorithm stops only when there is no state in which a different action would look better under the policy's own long-term value estimates. At that point the policy already satisfies the optimality equations, so it must be optimal.</p>"},{"location":"RL-4-Planning%20with%20Dynamic%20Programming/#is-this-a-greedy-algorithm","title":"Is this a greedy algorithm?","text":"<p>Policy iteration does include a greedy step, but it is not greedy in the short-sighted sense of maximizing immediate reward.</p> <p>Greedy with respect to long-term return. The improvement step chooses actions using the action-value under the current policy,</p> \\[ q_{\\pi}(s,a)=\\mathbb{E}\\!\\left[\\,R_{t+1}+\\gamma v_{\\pi}(S_{t+1}) \\mid S_t=s,\\;A_t=a\\,\\right], \\] <p>so actions are compared by immediate reward plus discounted future value. This is fundamentally different from the myopic rule</p> \\[ a=\\arg\\max_a \\mathbb{E}[R_{t+1}\\mid s,a], \\] <p>which ignores the effect of actions on future states.</p>"},{"location":"RL-4-Planning%20with%20Dynamic%20Programming/#modified-policy-iteration","title":"Modified Policy Iteration","text":"<p>Classic policy iteration is conceptually elegant: evaluate the current policy \\(\\pi_k\\) until we have its exact value function \\(v_{\\pi_k}\\), then improve greedily to obtain \\(\\pi_{k+1}\\). But this clean separation can be computationally wasteful. Full policy evaluation may require many sweeps over the state space, even though the very next step will replace the policy anyway. This raises a natural question: how accurately do we need to evaluate the current policy before improving it?</p>"},{"location":"RL-4-Planning%20with%20Dynamic%20Programming/#truncating-policy-evaluation","title":"Truncating policy evaluation","text":"<p>Modified policy iteration answers by relaxing the evaluation step. Instead of computing \\(v_{\\pi_k}\\) to convergence, we perform only a limited amount of evaluation, producing an approximation \\(\\tilde v_k\\). Two common truncation choices are:</p> <ul> <li>stop iterative evaluation once successive value estimates change by less than a tolerance \\(\\varepsilon\\) (an \\(\\varepsilon\\)-stopping rule), or</li> <li>run a fixed number of sweeps, say \\(m\\), of the Bellman expectation update.</li> </ul> <p>After this partial evaluation, we perform the same greedy improvement step as in policy iteration, replacing \\(\\pi_k\\) by a policy that is greedy with respect to the current estimate (whether exact or approximate).</p> <p>Why this can still be effective. The improvement step does not require a perfectly accurate value function, it requires a value estimate that is good enough to guide better action choices. If \\(\\tilde v_k\\) already captures the broad shape of long-term returns, then a greedy improvement step often corrects the most obvious suboptimal action choices immediately. As a result, repeatedly doing \"some evaluation + improvement\" can reach a strong policy using fewer total sweeps than insisting on \"perfect evaluation + improvement\" at every iteration.</p>"},{"location":"RL-4-Planning%20with%20Dynamic%20Programming/#a-spectrum-policy-iteration-rightarrow-value-iteration","title":"A spectrum: policy iteration \\(\\rightarrow\\) value iteration","text":"<p>Seen through this lens, policy iteration and value iteration are not fundamentally different algorithms, but endpoints of a continuum determined by how much work we invest in evaluation before improving:</p> <ul> <li>Policy iteration: evaluate to convergence, then improve.</li> <li>Modified policy iteration: evaluate partially, then improve.</li> <li>Value iteration: perform only a single Bellman-style sweep and immediately improve.</li> </ul> <p>In value iteration, evaluation and improvement effectively fuse into a single update that pushes values directly toward optimality. This is the key intuition behind value iteration, which we develop in the next section.</p>"},{"location":"RL-4-Planning%20with%20Dynamic%20Programming/#value-iteration-and-its-relation-to-policy-iteration","title":"Value Iteration and its relation to Policy Iteration","text":"<p>Value iteration is a dynamic programming method for the control problem: it aims to compute the optimal value function \\(v^*\\) and, from it, an optimal policy \\(\\pi^*\\). It is based on the Bellman optimality equation,</p> \\[ v^{*}(s)=\\max_{a\\in\\mathcal{A}} \\Bigl(R_s^a+\\gamma\\sum_{s'\\in\\mathcal{S}}P_{ss'}^a\\,v^{*}(s')\\Bigr), \\] <p>which states that the optimal value of a state equals the best one-step lookahead, immediate reward plus discounted optimal continuation value.</p>"},{"location":"RL-4-Planning%20with%20Dynamic%20Programming/#the-value-iteration-update","title":"The value iteration update","text":"<p>Because \\(v^*\\) is unknown, value iteration begins from an arbitrary initial guess \\(v_0\\) (often \\(v_0\\equiv 0\\)) and repeatedly applies the Bellman optimality backup:</p> \\[ v_{k+1}(s)\\;=\\;\\max_{a\\in\\mathcal{A}} \\Bigl(R_s^a+\\gamma\\sum_{s'\\in\\mathcal{S}}P_{ss'}^a\\,v_k(s')\\Bigr), \\qquad \\forall s\\in\\mathcal{S}. \\] <p>This update has the same form as the Bellman optimality equation, except that it uses the current estimate \\(v_k\\) on the right-hand side rather than the unknown \\(v^{*}\\). In operator notation, it is simply \\(v_{k+1} = \\mathcal{T}^* v_k\\) where \\(\\mathcal{T}^*\\) is the Bellman optimality operator. The optimal value function \\(v^{*}\\) is the unique fixed point of this operator (in the discounted, finite setting), meaning it satisfies \\(v^{*}=\\mathcal{T}^* v^{*}\\). Value iteration repeatedly applies \\(\\mathcal{T}^*\\); if the iterates converge to some limit \\(v_\\infty\\), then necessarily \\(v_\\infty\\) is a fixed point and hence \\(v_\\infty=v^{*}\\).</p>"},{"location":"RL-4-Planning%20with%20Dynamic%20Programming/#how-to-obtain-a-policy","title":"How to obtain a policy","text":"<p>Value iteration updates only values, it does not need to store a policy during the updates. Once a value estimate \\(v_k\\) is available, we can extract a greedy policy by one-step lookahead:</p> \\[ \\pi_k^{\\text{greedy}}(s)\\in\\arg\\max_{a\\in\\mathcal{A}} \\Bigl(R_s^a+\\gamma\\sum_{s'\\in\\mathcal{S}}P_{ss'}^a\\,v_k(s')\\Bigr). \\] <p>In practice, one may compute this greedy policy only at the end (to obtain \\(\\pi^*\\) from \\(v^*\\)), or track it along the way to see how behavior is improving.</p>"},{"location":"RL-4-Planning%20with%20Dynamic%20Programming/#how-value-iteration-differs-from-policy-iteration","title":"How value iteration differs from policy iteration","text":"<p>Policy iteration makes the policy explicit and alternates two operations:</p> \\[ \\pi_k \\;\\xrightarrow{\\ \\text{evaluate}\\ }\\; v_{\\pi_k} \\;\\xrightarrow{\\ \\text{improve}\\ }\\; \\pi_{k+1}, \\] <p>i.e., it evaluates a fixed policy using the Bellman expectation equation and then improves it greedily.</p> <p>Value iteration skips the explicit evaluation of a fixed policy. Instead, each sweep applies the Bellman optimality backup directly to \\(v\\), so the \"improvement\" idea is built into the update itself:</p> \\[ v_k \\;\\xrightarrow{\\ \\text{optimality backup}\\ }\\; v_{k+1}. \\] <p>As a consequence, intermediate value functions \\(v_k\\) are best viewed as improving approximations heading toward \\(v^*\\), they need not equal \\(v_\\pi\\) for any single stationary policy \\(\\pi\\) at that iteration.</p>"},{"location":"RL-4-Planning%20with%20Dynamic%20Programming/#synchronous-dynamic-programming-algorithms","title":"Synchronous Dynamic Programming Algorithms","text":"<p>All of the dynamic programming methods in this chapter rely on the same basic operation: use the model to look one step ahead and update a value estimate. Where they differ is in the objective, prediction versus control, and therefore in which Bellman relationship drives the update. In the tabular setting, these differences lead to three canonical algorithms.</p> <p>1. Prediction \\(\\rightarrow\\) Iterative Policy Evaluation</p> <p>Goal: evaluate a fixed policy \\(\\pi\\) by computing its value function \\(v_{\\pi}\\).</p> <p>Bellman relationship: the Bellman expectation equation, which averages over the actions selected by \\(\\pi\\) (and over next states under the dynamics).</p> <p>Algorithmic pattern: repeatedly update \\(v(s)\\) using the expected immediate reward plus the discounted expected value of successor states under \\(\\pi\\), until the values are consistent with following \\(\\pi\\).</p> <p>2. Control \\(\\rightarrow\\) Policy Iteration</p> <p>Goal: find an optimal policy \\(\\pi^{*}\\).</p> <p>Bellman relationships:</p> <ul> <li>use the Bellman expectation equation to (approximately or exactly) evaluate the current policy, and</li> <li>apply a greedy improvement step to update the policy.</li> </ul> <p>Algorithmic pattern: alternate between (i) evaluating the current \\(\\pi\\) and (ii) improving it by choosing, in each state, an action that maximizes one-step lookahead using the current value estimate. The resulting sequence of policies is monotone: it never gets worse.</p> <p>3. Control \\(\\rightarrow\\) Value Iteration</p> <p>Goal: compute the optimal value function \\(v^{*}\\) and then extract an optimal policy from it.</p> <p>Bellman relationship: the Bellman optimality equation, which takes a max over actions.</p> <p>Algorithmic pattern: repeatedly apply the optimality backup directly to \\(v\\). Informally, each sweep performs an \"improve everywhere\" step, without explicitly storing a policy during the updates (though a greedy policy can be extracted at any time).</p>"},{"location":"RL-4-Planning%20with%20Dynamic%20Programming/#complexity-analysis","title":"Complexity analysis","text":"<p>In their simplest tabular forms, these DP planning methods store state values:</p> \\[ v_{\\pi}(s)\\ \\text{for prediction},\\qquad v^{*}(s)\\ \\text{for control}. \\] <p>Let \\(n=\\lvert\\mathcal{S}\\rvert\\) and \\(m=\\lvert\\mathcal{A}\\rvert\\). A synchronous sweep computes one backup for every stored entry.</p>"},{"location":"RL-4-Planning%20with%20Dynamic%20Programming/#state-value-backups-vs","title":"State-value backups \\(v(s)\\)","text":"<p>In tabular DP control (e.g., value iteration), the backup is</p> \\[ (\\mathcal{T}^* v)(s) =\\max_{a\\in\\mathcal{A}}\\Bigl(R_s^a+\\gamma\\sum_{s'\\in\\mathcal{S}}P_{ss'}^a\\,v(s')\\Bigr). \\] <p>For a fixed state \\(s\\), the work in this formula is:</p> <ul> <li>the \\(\\max_{a\\in\\mathcal{A}}\\) loops over \\(m\\) actions, and</li> <li>for each action, the inner \\(\\sum_{s'\\in\\mathcal{S}}\\) loops over \\(n\\) successor states.</li> </ul> <p>So one state backup costs \\(O(mn)\\). A full sweep updates all \\(n\\) states, hence</p> \\[ \\underbrace{n}_{\\text{states}} \\;\\times\\; \\underbrace{m}_{\\text{actions per state}} \\;\\times\\; \\underbrace{n}_{\\text{successors per action}} \\;=\\; O(mn^2) \\quad \\text{per sweep.} \\]"},{"location":"RL-4-Planning%20with%20Dynamic%20Programming/#action-value-backups-qsa","title":"Action-value backups \\(q(s,a)\\)","text":"<p>If we store action-values, there are \\(mn\\) entries. A common control-style backup is</p> \\[ (\\mathcal{T}^* q)(s,a) =R_s^a+\\gamma\\sum_{s'\\in\\mathcal{S}}P_{ss'}^a\\,\\max_{a'\\in\\mathcal{A}} q(s',a'). \\] <p>For a fixed pair \\((s,a)\\), the work in this formula is:</p> <ul> <li>the \\(\\sum_{s'\\in\\mathcal{S}}\\) loops over \\(n\\) successor states, and</li> <li>inside the sum, the \\(\\max_{a'\\in\\mathcal{A}}\\) loops over \\(m\\) actions.</li> </ul> <p>So one \\((s,a)\\) backup costs \\(O(nm)\\). A full sweep updates all \\(mn\\) pairs, hence</p> \\[ \\underbrace{(mn)}_{\\text{state--action pairs}} \\;\\times\\; \\underbrace{n}_{\\text{successors}} \\;\\times\\; \\underbrace{m}_{\\text{actions in }\\max} \\;=\\; O(m^2n^2) \\quad \\text{per sweep.} \\] <p>Takeaway All synchronous DP methods are one-step lookahead sweeps, they differ mainly in the Bellman operator (expectation for prediction, optimality for control) and in whether a policy is maintained explicitly (policy iteration) or extracted from values (value iteration).</p>"},{"location":"RL-4-Planning%20with%20Dynamic%20Programming/#asynchronous-dynamic-programming","title":"Asynchronous Dynamic Programming","text":"<p>Up to this point, we have treated DP updates as synchronous: at iteration \\(k\\) we compute a complete new table \\(v_{k+1}\\) from \\(v_k\\) by performing a full sweep over all states. This viewpoint is clean and easy to analyze, but it can be wasteful in practice: many states may already be nearly correct, while a small set of states may still have large errors. Synchronous sweeps spend equal effort everywhere, regardless of where the value function most needs improvement.</p> <p>Asynchronous DP keeps the same Bellman backup, but changes how it is applied. Instead of updating every state on every iteration, it updates one (or a few) states at a time and immediately overwrites the stored value:</p> \\[ v(s)\\leftarrow (\\mathcal{T}v)(s). \\] <p>Because updates are not forced to occur uniformly, asynchronous methods can:</p> <ul> <li>avoid recomputing values for states that are already accurate,</li> <li>focus computation where the current approximation is most wrong or most relevant,</li> <li>propagate new information faster (since later updates can use earlier updated values).</li> </ul> <p>In discounted finite MDPs, asynchronous DP still converges as long as every state is updated infinitely often, but it often reaches a useful approximation in far fewer backups than full sweeps.</p>"},{"location":"RL-4-Planning%20with%20Dynamic%20Programming/#three-useful-versions-of-asynchronous-dp","title":"Three useful versions of asynchronous DP","text":""},{"location":"RL-4-Planning%20with%20Dynamic%20Programming/#in-place-dynamic-programming-faster-propagation-less-memory","title":"In-place dynamic programming (faster propagation, less memory)","text":"<p>A minimal change from synchronous DP is to switch from a two-table update to an in-place update. In synchronous value iteration we conceptually separate \"read\" and \"write\" tables:</p> \\[ v_{\\text{new}}(s)\\leftarrow \\max_{a\\in\\mathcal{A}} \\left(\\mathcal{R}_s^a + \\gamma \\sum_{s'\\in\\mathcal{S}}\\mathcal{P}_{ss'}^a\\,v_{\\text{old}}(s')\\right), \\qquad v_{\\text{old}} \\leftarrow v_{\\text{new}}. \\] <p>This ensures each backup uses only stale values from iteration \\(k\\).</p> <p>In in-place value iteration we maintain a single table \\(v\\) and overwrite entries immediately:</p> \\[ v(s)\\leftarrow \\max_{a\\in\\mathcal{A}} \\left(\\mathcal{R}_s^a + \\gamma \\sum_{s'\\in\\mathcal{S}}\\mathcal{P}_{ss'}^a\\,v(s')\\right). \\] <p>The benefit is that information can move through the state space more quickly: later backups in the same sweep can already exploit improvements made earlier. Practically, in-place updates often reduce the number of sweeps needed to achieve a given accuracy, and they also avoid storing a second full table.</p>"},{"location":"RL-4-Planning%20with%20Dynamic%20Programming/#prioritised-sweeping-spend-backups-where-they-matter-most","title":"Prioritised sweeping (spend backups where they matter most)","text":"<p>Synchronous sweeps spend the same effort on every state, even though at a given moment some states violate the Bellman equation much more than others (i.e., their current values are much farther from their one-step backup). Prioritised sweeping takes advantage of this by choosing the next state to update based on its current Bellman error:</p> \\[ \\left| \\max_{a\\in\\mathcal{A}} \\left(\\mathcal{R}_s^a + \\gamma \\sum_{s'\\in\\mathcal{S}}\\mathcal{P}_{ss'}^a\\,v(s')\\right) - v(s) \\right| \\] <p>Intuitively, a large error indicates that the current value at \\(s\\) is far from what the Bellman equation demands, so updating \\(s\\) is likely to produce a meaningful improvement. A typical implementation of this would be:</p> <ul> <li>selects the state with the largest current error and performs its backup,</li> <li>then updates priorities for states likely to be affected next (often predecessor states),</li> <li>and uses a priority queue to make this selection efficient.</li> </ul> <p>Compared to full sweeps, prioritised sweeping can reach an accurate value function with far fewer backups when errors are localized, because it concentrates computation on the \u201chard\u201d parts of the problem.</p>"},{"location":"RL-4-Planning%20with%20Dynamic%20Programming/#real-time-dynamic-programming-update-only-states-you-actually-encounter","title":"Real-time dynamic programming (update only states you actually encounter)","text":"<p>Sometimes the goal is not to make the value function accurate everywhere, but to make it accurate where the agent will actually go. In large MDPs, many states may be irrelevant to near-term decision-making from the current start state. Real-time DP exploits this by backing up only states encountered along simulated (or real) trajectories.</p> <p>After taking a step at time \\(t\\), we update the current state \\(S_t\\):</p> \\[ v(S_t)\\leftarrow \\max_{a\\in\\mathcal{A}} \\left(\\mathcal{R}_{S_t}^a + \\gamma \\sum_{s'\\in\\mathcal{S}}\\mathcal{P}_{S_t s'}^a\\,v(s')\\right). \\] <p>This can be dramatically cheaper than sweeping when the reachable region is a small fraction of the full state space. The tradeoff is that states that are never visited will never be improved\u2014but if they are irrelevant to the task from the current start distribution, this \u201cselective accuracy\u201d is exactly what we want.</p> <p>Takeaway. Synchronous DP spends a fixed budget per sweep by updating everything. Asynchronous DP spends a flexible budget by updating selected states, often achieving good value estimates and policies with substantially less total computation.</p>"},{"location":"RL-4-Planning%20with%20Dynamic%20Programming/#approximate-dynamic-programming","title":"Approximate Dynamic Programming","text":"<p>Tabular DP assumes we can store and update a distinct number for every state \\(s\\in\\mathcal{S}\\). When \\(\\lvert\\mathcal{S}\\rvert\\) is enormous (or continuous), that assumption breaks down. Approximate DP keeps the DP logic, Bellman backups and repeated improvement, but replaces the value table with a function approximator.</p>"},{"location":"RL-4-Planning%20with%20Dynamic%20Programming/#replace-the-table-with-a-function-class","title":"Replace the table with a function class","text":"<p>Instead of representing a value function as \\(v(s)\\) for each state, we represent it by a parameterized approximation</p> \\[ \\hat v(s,\\mathbf{w}), \\] <p>where \\(\\mathbf{w}\\) denotes the parameters (weights). The goal is that a single parameter vector generalizes across many states, allowing compact storage and sharing statistical strength.</p>"},{"location":"RL-4-Planning%20with%20Dynamic%20Programming/#dp-directly-on-the-approximation-fitted-value-iteration","title":"DP directly on the approximation (Fitted Value Iteration)","text":"<p>With function approximation, we can no longer apply the Bellman operator to every state and store the result exactly. A standard workaround is to run DP in a sampled and projected way: we apply a Bellman backup on a set of sampled states, then fit our approximator to match those backed-up values. This template is known as Fitted Value Iteration (FVI).</p> <p>Setup and initialization. We assume an MDP with discount factor \\(\\gamma\\) and access to a generative model (or known dynamics) so that, for any queried pair \\((s,a)\\), we can obtain the immediate reward and either compute or estimate the expected next-state value. Our goal is to approximate the optimal value function \\(v^*\\), but since a tabular representation is infeasible, we restrict ourselves to a parameterized function class</p> \\[ \\{\\hat v(\\cdot,\\mathbf{w}) : \\mathbf{w}\\in\\mathbb{R}^d\\}, \\] <p>(e.g., linear features or a neural network) and seek parameters \\(\\mathbf{w}\\) such that \\(\\hat v(\\cdot,\\mathbf{w})\\approx v^*\\) over the states of interest. To train this approximation we also choose a state-sampling scheme, modeled as sampling \\(s\\sim \\mu\\) for some distribution \\(\\mu\\) over \\(\\mathcal{S}\\) (uniform over a region, concentrated near a start-state distribution, or induced by trajectories under a behavior policy). Finally, we pick an initial parameter vector \\(\\mathbf{w}_0\\) (commonly \\(\\hat v(\\cdot,\\mathbf{w}_0)\\equiv 0\\) or small random weights) and then iteratively improve \\(\\hat v\\) using Bellman targets and regression.</p> <p>Iteration. For \\(k=0,1,2,\\ldots\\) repeat:</p> <p>1. Sample states and form Bellman targets.</p> <p>Sample a finite training set of states \\(\\tilde{\\mathcal{S}}_k=\\{s^{(1)},\\dots,s^{(n)}\\}\\) from \\(\\mu\\). For each sampled state \\(s\\in\\tilde{\\mathcal{S}}_k\\), compute a one-step optimality target by backing up the current approximation:</p> \\[ \\tilde v_k(s)= \\max_{a\\in\\mathcal{A}} \\left(   \\mathcal{R}_s^a   +\\gamma\\sum_{s'\\in\\mathcal{S}}\\mathcal{P}_{ss'}^a\\,\\hat v(s',\\mathbf{w}_k) \\right). \\] <p>(If we only have a simulator rather than an explicit transition matrix, the expectation over \\(s'\\) can be approximated by samples.)</p> <p>2. Fit the approximator to the targets (projection step).</p> <p>Update parameters by supervised regression so that the next approximation matches these targets on the sampled states:</p> \\[ \\mathbf{w}_{k+1} \\in \\arg\\min_{\\mathbf{w}} \\sum_{s\\in\\tilde{\\mathcal{S}}_k} \\big(\\hat v(s,\\mathbf{w})-\\tilde v_k(s)\\big)^2. \\] <p>Equivalently, \\(\\hat v(\\cdot,\\mathbf{w}_{k+1})\\) is trained on the dataset \\(\\{(s,\\tilde v_k(s)):\\ s\\in\\tilde{\\mathcal{S}}_k\\}\\).</p> <p>Interpretation. Each iteration performs two conceptual steps: a Bellman improvement step (create targets using one-step lookahead) followed by a projection step (compress the backed-up values back into the function class by regression). This recovers the DP idea of repeated Bellman updates, but replaces exact tabular storage with approximation and generalization.</p>"},{"location":"RL-4-Planning%20with%20Dynamic%20Programming/#references","title":"References","text":"<ul> <li>https://github.com/zyxue/youtube_RL_course_by_David_Silver</li> </ul>"},{"location":"RL-5-Model%20free%20prediction/","title":"5 - Model Free Prediction","text":""},{"location":"RL-5-Model%20free%20prediction/#model-free-prediction-policy-evaluation","title":"Model-Free prediction (policy evaluation)","text":"<p>Recap. In the last chapter, we planned with dynamic programming to solve a known MDP. In this chapter, we move to model-free prediction, where we estimate a value function in an unknown MDP. In the next chapter, we study model-free control, where we optimize value and aim to find an optimal policy in an unknown MDP.</p>"},{"location":"RL-5-Model%20free%20prediction/#setting-unknown-mdp-fixed-policy","title":"Setting: unknown MDP, fixed policy","text":"<p>In model-free prediction, we do not know the MDP model (transitions and rewards are unknown), but we can sample experience by interacting with the environment. Here, we focus on policy evaluation: given a fixed policy \\( \\pi \\), we estimate its value function. To estimate \\( v_\\pi \\) in an unknown MDP, we mainly use:</p> <ul> <li>Monte Carlo (MC) prediction</li> <li>Temporal Difference (TD) prediction</li> </ul>"},{"location":"RL-5-Model%20free%20prediction/#monte-carlo-prediction","title":"Monte Carlo prediction","text":"<p>Monte Carlo (MC) prediction estimates the value function of a fixed policy \\( \\pi \\) using sampled episodes of experience. We do not need the MDP model (transitions or rewards). Instead, we generate episodes by interacting with the environment, following policy \\( \\pi \\) and learn from what actually happened. We compute returns \\( G_t \\) from visited states using rewards until termination. Therefore, this method fits episodic tasks, where each episode terminates at some time \\( T \\). If an episode never ends, the return is not naturally available in this form.</p>"},{"location":"RL-5-Model%20free%20prediction/#return-and-value-under-a-fixed-policy","title":"Return and value under a fixed policy","text":"<p>Consider one episode generated by following policy \\( \\pi \\):</p> \\[ S_1, A_1, R_2, S_2, A_2, R_3, \\dots, S_T. \\] <p>For a time step \\( t \\), we define the (discounted) return from that time as</p> \\[ G_t = \\sum_{k=0}^{T-t-1}\\gamma^k R_{t+k+1}       = R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{T-1-t}R_T, \\] <p>and the state-value function as the expected return when we start from state \\( s \\) and follow \\( \\pi \\):</p> \\[ v_\\pi(s) = \\mathbb{E}_\\pi\\!\\left[G_t \\mid S_t=s\\right]. \\] <p>Each time we visit a state \\( s \\) while following \\( \\pi \\), we can compute a return sample starting from that visit. Let \\( G^{(1)}(s), G^{(2)}(s), \\dots, G^{(N(s))}(s) \\) be the returns observed from \\( N(s) \\) number of visits to \\( s \\). Then MC estimates the value by the sample mean:</p> \\[ V(s) = \\frac{1}{N(s)}\\sum_{i=1}^{N(s)} G^{(i)}(s) \\approx v_\\pi(s). \\] <p>As \\( N(s) \\) grows, this average becomes a better estimate of the true expectation. In short, MC policy evaluation replaces the expected return in \\( v_\\pi(s) \\) with an empirical mean computed from experience.</p>"},{"location":"RL-5-Model%20free%20prediction/#first-visit-monte-carlo-policy-evaluation","title":"First-Visit Monte Carlo policy evaluation","text":"<p>In first-visit MC, we update each state using the return from its first occurrence in an episode, and then average these returns across episodes to approximate \\( v_\\pi(s) \\). For each state \\( s \\), we maintain:</p> \\[ N(s)\\ \\#\\{\\text{episodes in which } s \\text{ is visited at least once}\\},\\qquad S(s)\\ \\text{(sum of returns)},\\qquad V(s)=\\frac{S(s)}{N(s)}. \\]"},{"location":"RL-5-Model%20free%20prediction/#procedure","title":"Procedure","text":"<p>For each episode generated by following \\( \\pi \\) until termination,</p> \\[ S_1, A_1, R_2, S_2, \\dots, S_T, \\] <p>we do the following for every state \\( s \\) that appears in the episode:</p> <ol> <li>Find the first time step \\( t^\\star \\) in the episode such that \\( S_{t^\\star}=s \\).</li> <li>Compute the return from that first visit:</li> </ol> \\[ G_{t^\\star} = \\sum_{k=0}^{T-t^\\star-1}\\gamma^k R_{t^\\star+k+1}. \\] <ol> <li>Update totals and counts after each episode:</li> </ol> \\[ N(s)\\leftarrow N(s)+1,\\qquad S(s)\\leftarrow S(s)+G_{t^\\star},\\qquad V(s)\\leftarrow \\frac{S(s)}{N(s)}. \\] <p>Practical notes. Episodes start from an initial-state distribution, which is typically set by the environment reset rule (sometimes a fixed start state, sometimes random). During an episode, a state \\( s \\) may appear multiple times, and first-visit MC means we update \\( s \\) using only the first time it appears in that episode (we do not need to return to \\( s \\) again within the same episode). Across episodes, we count how often \\( s \\) appears at least once and form \\( V(s)=S(s)/N(s) \\) by averaging the first-visit returns. This immediately tells us what we can estimate: if \\( s \\) is never visited, then \\( N(s)=0 \\) and we cannot evaluate \\( v_\\pi(s) \\) from data. If \\( s \\) is rarely visited, the estimate has high variance and may be unreliable. If some states are unlikely under \\( \\pi \\), or returns have high variance we do not care much about them, because we mainly care about the states we actually encounter under \\( \\pi \\) (exploration or visiting more states becomes central in later chapters on control), and for any state that is visited infinitely often, the law of large numbers gives convergence:</p> \\[ V(s)\\to v_\\pi(s)\\quad \\text{as}\\quad N(s)\\to\\infty. \\] <p>Note that, we cannot stop the process once all the states have been visited once, as we still need the episode to terminate because the return \\( G_{t^\\star} \\) for a state's first visit depends on rewards after that time step, all the way to the end of the episode.</p>"},{"location":"RL-5-Model%20free%20prediction/#every-visit-monte-carlo-policy-evaluation","title":"Every-Visit Monte Carlo policy evaluation","text":"<p>In every-visit MC, we estimate \\( v_\\pi(s) \\) from complete episodes by averaging all returns observed after every visit to \\( s \\) (not just the first visit within an episode). This usually gives more samples per episode for visited states, at the cost of using correlated returns from the same trajectory.</p>"},{"location":"RL-5-Model%20free%20prediction/#procedure_1","title":"Procedure","text":"<p>Across episodes generated by following policy \\( \\pi \\) to termination, we maintain for each state \\( s \\):</p> \\[ N(s)\\ \\text{(number of visits to state s)},\\qquad S(s)\\ \\text{(sum of returns)},\\qquad V(s)=\\frac{S(s)}{N(s)}. \\] <p>In one episode, for every time step \\( t \\) such that \\( S_t=s \\):</p> \\[ N(s)\\leftarrow N(s)+1,\\qquad S(s)\\leftarrow S(s)+G_t,\\qquad V(s)\\leftarrow \\frac{S(s)}{N(s)}. \\] <p>Here, \\( N(s) \\) counts all visits to \\( s \\) (over all episodes and all time steps). As \\( N(s)\\to\\infty \\), we have \\( V(s)\\to v_\\pi(s) \\) under standard sampling assumptions.</p>"},{"location":"RL-5-Model%20free%20prediction/#incremental-mean-online-update","title":"Incremental mean (online update)","text":"<p>To compute averages from a stream of data without storing all past samples (which is exactly what we need in MC), we use an incremental mean update. If we observe samples \\( x_1,x_2,\\dots \\), the sample mean after \\( k \\) samples is</p> \\[ \\mu_k = \\frac{1}{k}\\sum_{j=1}^k x_j, \\] <p>and we can update it without storing history:</p> \\[ \\mu_k \\leftarrow \\mu_{k-1} + \\frac{1}{k}\\bigl(x_k-\\mu_{k-1}\\bigr). \\] <p>We can read this as an \"error-correction\" update:</p> \\[ \\mu_k \\leftarrow \\mu_{k-1} + \\alpha_k\\cdot \\text{error},\\qquad \\alpha_k=\\frac{1}{k},\\ \\ \\text{error}=x_k-\\mu_{k-1}. \\] <p>Here, \\( k \\) is the number of samples seen so far. We start from an initial \\( \\mu_0 \\) (often \\( 0 \\)) and update as samples arrive. This lets us update \\( V(s) \\) online across episodes without storing past returns: once an episode ends and we compute a new sample \\( G \\), we immediately fold it into \\( V(s) \\) via \\( V(s)\\leftarrow V(s)+\\frac{1}{N(s)}(G-V(s)) \\). So we store only \\( (N(s),V(s)) \\) per state, not the entire history.</p> <p>Incremental Monte Carlo value updates. After the episode ends and all required returns are available:</p> <ul> <li>First-visit MC: for each state \\( s \\) that appears in the episode, take only its earliest occurrence \\( t^\\star \\) and use the sample \\( (s,\\; G_{t^\\star}) \\) to update \\( V(s) \\) once.</li> <li>Every-visit MC: after the episode terminates, compute all \\( G_t \\) and update \\( V(S_t) \\) for each \\( t=1,\\dots,T-1 \\).</li> </ul> <p>Forgetting old episodes. If the environment is non-stationary (its dynamics or reward distribution can change over time), a long-run average may lag behind to what is currently happening. In that case, we use a constant step size \\( \\alpha \\) instead of \\( 1/N(s) \\):</p> \\[ V(s)\\leftarrow V(s)+\\alpha\\bigl(G - V(s)\\bigr). \\] <p>This creates an exponential moving average, so recent returns influence \\( V(s) \\) more than older ones.</p>"},{"location":"RL-5-Model%20free%20prediction/#temporal-difference-learning","title":"Temporal-Difference learning","text":"<p>Temporal-Difference (TD) learning is another model-free way to estimate \\( v_\\pi \\) from experience. Again, we only need sampled interaction trajectories generated by following a fixed policy \\( \\pi \\). Compared to Monte Carlo, the key difference is timing: TD updates during an episode, before termination, unlike MC which requires termination of episodes.</p> <p>The basic TD idea: learn from one transition. At time step \\( t \\), we observe a single transition while following \\( \\pi \\):</p> \\[ S_t,\\ A_t,\\ R_{t+1},\\ S_{t+1}. \\] <p>We then approximate the unknown \"rest of the future\" after \\( S_{t+1} \\) by our current estimate \\( V(S_{t+1}) \\). Therefore, instead of waiting for the full return \\( G_t \\), TD builds a one-step estimate of the return:</p> \\[ \\text{TD target} = R_{t+1}+\\gamma V(S_{t+1}). \\] <p>This target is available immediately: \\( R_{t+1} \\) is observed after the transition, and \\( V(S_{t+1}) \\) is the current stored estimate for the observed next state \\( (S_{t+1}) \\), this allows the update before the episode terminates. We then move the current stored estimate at the present state, \\( V(S_t) \\), toward this target using step size \\( \\alpha \\):</p> \\[ V(S_t)\\leftarrow V(S_t)+\\alpha\\Bigl(\\delta_t\\Bigr), \\qquad \\delta_t = R_{t+1}+\\gamma V(S_{t+1})-V(S_t), \\] <p>where \\( \\delta_t \\) is the TD error.</p>"},{"location":"RL-5-Model%20free%20prediction/#connection-to-the-bellman-expectation-equation","title":"Connection to the Bellman expectation equation","text":"<p>For a fixed policy \\( \\pi \\), the Bellman expectation equation says:</p> \\[ v_\\pi(s)=\\mathbb{E}_\\pi\\!\\left[\\,R_{t+1}+\\gamma v_\\pi(S_{t+1})\\mid S_t=s\\right]. \\] <p>TD(0) is the sample-based version of this identity: we replace the expectation by the single observed reward \\( R_{t+1} \\) and replace the unknown \\( v_\\pi(S_{t+1}) \\) with our current estimate \\( V(S_{t+1}) \\), then update \\( V(S_t) \\) toward that one-step target. This technique of updating is know as bootstrapping (discussed later in more details).</p>"},{"location":"RL-5-Model%20free%20prediction/#mc-vs-td-1-bias-variance-tradeoff","title":"MC vs. TD (1): Bias-variance tradeoff","text":"<p>Both Monte Carlo (MC) and Temporal Difference (TD) methods estimate the same object, \\( v_\\pi \\), from experience under a fixed policy \\( \\pi \\). The difference is the learning signal: MC uses a full-return target and updates after the episode ends, while TD updates during the episode using a bootstrapped one-step target.</p>"},{"location":"RL-5-Model%20free%20prediction/#bias-variance-trade-off","title":"Bias-variance trade-off","text":"<p>Unbiased targets (if we had the truth). For MC, the return is an unbiased sample of the value:</p> \\[ \\mathbb{E}_\\pi\\!\\left[G_t\\mid S_t\\right]=v_\\pi(S_t). \\] <p>Meanwhile for TD, the ideal one-step target would also be unbiased:</p> \\[ \\mathbb{E}_\\pi\\!\\left[R_{t+1}+\\gamma v_\\pi(S_{t+1})\\mid S_t\\right]=v_\\pi(S_t). \\] <p>But, in practice we do not know \\( v_\\pi \\), so TD uses</p> \\[ R_{t+1}+\\gamma V(S_{t+1}), \\] <p>and this is generally a biased target because typically \\( V(S_{t+1})\\neq v_\\pi(S_{t+1}) \\).</p>"},{"location":"RL-5-Model%20free%20prediction/#mc-vs-td-2-certainty-equivalence-in-batch-learning","title":"MC vs. TD (2): Certainty Equivalence in batch learning","text":"<p>So far, we assumed we can keep collecting new experience. With enough data, both MC and TD can converge to the true value:</p> \\[ V(s)\\to v_\\pi(s)\\qquad \\text{as experience }\\to\\infty. \\] <p>In the batch setting, we instead get a fixed dataset of \\( K \\) episodes and we must learn only by reusing it:</p> \\[ \\mathcal{D}=\\{\\tau^{(1)},\\dots,\\tau^{(K)}\\},\\qquad \\tau^{(k)}=(S^{(k)}_1,A^{(k)}_1,R^{(k)}_2,\\dots,S^{(k)}_{T_k}). \\] <p>We repeatedly sweep through \\( \\mathcal{D} \\) (or sample episodes from it) and apply updates to the same data until the values stop changing.</p>"},{"location":"RL-5-Model%20free%20prediction/#a-b-example","title":"A-B example","text":"<p>Environment and dataset. We consider an episodic process with \\( \\gamma=1 \\) and two non-terminal states \\( A \\) and \\( B \\). Our fixed dataset contains \\( 8 \\) episodes:</p> \\[ (A,0,B,0),\\quad (B,1),\\quad (B,1),\\quad (B,1),\\quad (B,1),\\quad (B,1),\\quad (B,1),\\quad (B,0). \\] <p>So:</p> <ul> <li>Episode 1 visits \\( A\\to B \\) and then terminates with reward \\( 0 \\) from \\( B \\).</li> <li>Episodes 2--7 start at \\( B \\) and terminate with reward \\( 1 \\).</li> <li>Episode 8 starts at \\( B \\) and terminates with reward \\( 0 \\).</li> </ul> <p>Batch MC: empirical mean of observed returns. Batch MC converges to the value function that best fits the observed Monte Carlo returns in the dataset, i.e., it treats each sampled return as a supervised target and solves a least-squares regression:</p> \\[ V_{\\text{MC}} \\in \\arg\\min_V \\;\\sum_{k=1}^{K}\\sum_{t=1}^{T_k}\\Bigl(G_t^{(k)} - V(s_t^{(k)})\\Bigr)^2. \\] <p>Here the dataset consists of \\( K \\) recorded episodes (trajectories), indexed by \\( k=1,\\dots,K \\). Episode \\( k \\) has length \\( T_k \\) time steps (i.e., it contains states \\( s^{(k)}_1,\\dots,s^{(k)}_{T_k} \\)), so the inner sum ranges over all time indices within episode \\( k \\) and the outer sum aggregates the squared errors over all time steps in all episodes. In our tabular example, each state value \\( V(s) \\) is an independent parameter, so the objective separates by state. Group all terms with the same state \\( s \\):</p> \\[ \\sum_{k=1}^{K}\\sum_{t=1}^{T_k}\\bigl(G_t^{(k)}-V(s_t^{(k)})\\bigr)^2 =\\sum_{s}\\sum_{i=1}^{n(s)}\\bigl(G_i(s)-V(s)\\bigr)^2 . \\] <p>Thus we can minimize each state\u2019s sum of squares independently:</p> \\[ V_{\\text{MC}}(s)\\in\\arg\\min_{v}\\;\\sum_{i=1}^{n(s)}\\bigl(G_i(s)-v\\bigr)^2 . \\] <p>Differentiate w.r.t. \\( v \\) and set to zero:</p> \\[ \\frac{d}{dv}\\sum_{i=1}^{n(s)}(G_i(s)-v)^2 =-2\\sum_{i=1}^{n(s)}(G_i(s)-v)=0 \\;\\Rightarrow\\; v=\\frac{1}{n(s)}\\sum_{i=1}^{n(s)}G_i(s). \\] <p>Therefore,</p> \\[ V_{\\text{MC}}(s)=\\frac{1}{n(s)}\\sum_{i=1}^{n(s)} G_i(s). \\] <p>where \\( n(s) \\) is the number of occurrences of state \\( s \\) in the dataset and \\( G_i(s) \\) are the corresponding sampled returns. For \\( A \\): \\( n(A)=1 \\) and the single observed return is \\( G_1(A)=0 \\), so</p> \\[ V_{\\text{MC}}(A)=\\frac{1}{1}\\bigl(0\\bigr)=0. \\] <p>For \\( B \\): \\( n(B)=8 \\) and the observed returns are \\( \\{1,1,1,1,1,1,0,0\\} \\), so</p> \\[ V_{\\text{MC}}(B)=\\frac{1}{8}\\bigl(1+1+1+1+1+1+0+0\\bigr) =\\frac{6}{8}=0.75. \\] <p>Batch TD(0): fixed point induced by the dataset. Batch TD(0) behaves differently: with a fixed dataset, it effectively treats the data as an empirical MDP and converges to the value function that satisfies the Bellman equations of that estimated model under \\( \\pi \\). In particular, the dataset induces maximum-likelihood one-step estimates</p> \\[ \\hat{P}^{a}_{s,s'}= \\frac{\\#\\{(s_t=s,\\ a_t=a,\\ s_{t+1}=s')\\}}{\\#\\{(s_t=s,\\ a_t=a)\\}}, \\qquad \\hat{R}^{a}_{s}= \\frac{\\sum_{t:\\,s_t=s,\\ a_t=a} r_{t+1}}{\\#\\{(s_t=s,\\ a_t=a)\\}}, \\] <p>and TD(0) converges to the fixed point of the Bellman expectation equation for the estimated MDP \\( \\langle \\mathcal{S},\\mathcal{A},\\hat{P},\\hat{R},\\gamma\\rangle \\). In our dataset, \\( A \\) always transitions to \\( B \\) with reward \\( 0 \\), so</p> \\[ V_{\\text{TD}}(A)=0+\\gamma V_{\\text{TD}}(B), \\] <p>and from \\( B \\) the empirical mean terminal reward is \\( 0.75 \\) (with terminal value \\( 0 \\)), hence</p> \\[ V_{\\text{TD}}(B)=0.75. \\] <p>Therefore \\( V_{\\text{TD}}(A)=0.75 \\) when \\( \\gamma=1 \\) (and more generally \\( V_{\\text{TD}}(A)=\\gamma\\cdot 0.75 \\)).</p> <p>Why the answers differ. With a finite dataset, MC updates \\( A \\) using only the single observed return following \\( A \\) (here it happened to be \\( 0 \\)), so it keeps \\( V(A)=0 \\), whereas TD bootstraps through \\( B \\) and propagates the dataset's average outcome at \\( B \\) back through the observed transition \\( A\\!\\to\\!B \\), yielding \\( V(A)=V(B)=0.75 \\). In batch learning (no new data), repeated passes over the same \\( K \\) episodes drive the methods to different stable solutions: MC converges to empirical mean returns, while TD converges to the fixed point induced by the dataset's empirical transition and reward estimates.</p>"},{"location":"RL-5-Model%20free%20prediction/#mc-vs-td-3-the-markov-property","title":"MC vs. TD (3): The Markov property","text":"<p>In an MDP, the future is conditionally independent of the past given the current state \\( S_t \\), and TD(0) exploits this one-step Markov structure by updating from a single transition \\( (S_t,R_{t+1},S_{t+1}) \\) via</p> \\[ V(S_t)\\leftarrow V(S_t)+\\alpha\\bigl(R_{t+1}+\\gamma V(S_{t+1})-V(S_t)\\bigr), \\] <p>which lets information propagate efficiently along observed state-to-state transitions without waiting for complete outcomes. In contrast, MC uses the full return \\( G_t \\) as a black-box target that bundles the entire future into one number, this can work well, but it does not explicitly leverage the Markov conditional-independence structure and instead learns only from complete episode outcomes. As a rule of thumb, in standard Markov settings TD is often more data-efficient because it aligns with the environment's one-step dynamics, whereas with non-Markov signals (e.g. partial observability) MC can sometimes help because the full return may implicitly carry information from the episode history when the current state representation is incomplete.</p>"},{"location":"RL-5-Model%20free%20prediction/#method-summaries-mc-vs-td","title":"Method Summaries MC vs TD","text":"<p>Monte Carlo (MC).</p> <ul> <li>Unbiased target: MC uses the actual return \\( G_t \\), so</li> </ul> \\[ \\mathbb{E}_\\pi\\!\\left[G_t\\mid S_t=s\\right]=v_\\pi(s). \\] <ul> <li>Higher variance: \\( G_t \\) aggregates many random future steps (actions, transitions, rewards).</li> <li>Often stable with approximation: MC looks like supervised learning on observed returns, which is usually numerically stable.</li> <li>Less sensitive to initialization: early errors in \\( V \\) do not affect the target (the target is \\( G_t \\)).</li> <li>Simple mental model: we \"average returns\" to estimate value.</li> </ul> <p>Temporal Difference (TD).</p> <ul> <li>Bootstrapped (biased during learning): TD(0) uses the target</li> </ul> \\[ R_{t+1}+\\gamma V(S_{t+1}), \\] <p>which is biased when \\( V(S_{t+1})\\neq v_\\pi(S_{t+1}) \\).</p> <ul> <li>Lower variance: the target depends only on one transition plus a lookup of \\( V(S_{t+1}) \\).</li> <li>More sample-efficient: we update after each step and do not need to wait for termination.</li> <li>Tabular convergence: with a fixed \\( \\pi \\) and suitable step sizes, TD(0) converges to \\( v_\\pi \\).</li> <li>Can be unstable with approximation: bootstrapping + function approximation can cause divergence in some settings.</li> <li>More sensitive to initialization: poor initial \\( V \\) can propagate through the TD target.</li> </ul>"},{"location":"RL-5-Model%20free%20prediction/#convergence-to-the-same-value-function","title":"Convergence to the same value function?","text":"<p>In a stationary tabular setting where each relevant state is visited infinitely often under \\( \\pi \\), MC with sample-mean updates and TD(0) with suitable step-size conditions both converge to the same value function \\( v_\\pi(s) \\). In practice, with finite data, noisy returns, or function approximation, they can yield noticeably different estimates because their targets differ (full returns vs. bootstrapped) and they propagate information differently. An example of this can be seen in the next section.</p>"},{"location":"RL-5-Model%20free%20prediction/#bootstrapping-and-sampling","title":"Bootstrapping and Sampling","text":"<p>Dynamic Programming, TD, and MC can be compared along two independent axes: bootstrapping (whether the update target uses the current estimate \\( V \\)) and sampling (whether expectations are approximated from data or computed exactly from a known model). MC is sampled but non-bootstrapped, using the complete return \\( G_t \\) as its target. TD is sampled and bootstrapped, using the one-step target \\( R_{t+1}+\\gamma V(S_{t+1}) \\), and DP is bootstrapped but not sampled, using a full expected Bellman target such as \\( \\sum_{s'}P(s'|s,a)\\bigl(R(s,a,s')+\\gamma V(s')\\bigr) \\). Equivalently, all three are backups that move \\( V(s) \\) toward a target: DP performs a full expected backup using the model, while TD and MC perform sample backups using observed experience, TD uses a shallow one-step backup that propagates information via bootstrapping, whereas MC uses a deep backup to episode end with no bootstrapping. Many multi-step methods (e.g., TD(\\( \\lambda \\))) interpolate between TD(0) and MC by trading off backup depth against the amount of bootstrapping.</p>"},{"location":"RL-5-Model%20free%20prediction/#n-step-prediction","title":"\\( n \\)-Step Prediction","text":"<p>\\( n \\)-step prediction provides a simple continuum between TD(0) and Monte-Carlo (MC). TD(0) uses a 1-step, bootstrapped target, updating immediately from \\( (R_{t+1},S_{t+1}) \\), while MC avoids bootstrapping by waiting for the episode to finish and using the full observed return. The idea of \\( n \\)-step methods is to look ahead for \\( n \\) steps: use \\( n \\) actual rewards, then bootstrap once at step \\( n \\). As \\( n \\) increases, the target depends less on the current value estimates and more on observed rewards, in the limit \\( n\\to\\infty \\) (for episodic tasks) the method reduces to MC.</p> <p>\\( n \\)-step return. For a trajectory generated under \\( \\pi \\), define</p> \\[ G_t^{(n)}= \\sum_{i=1}^{n}\\gamma^{i-1}R_{t+i} \\;+\\; \\gamma^{n} V(S_{t+n}). \\] <p>This is an \\( n \\)-step backup: accumulate \\( n \\) rewards, then terminate the target by bootstrapping from \\( V \\) at the state reached after \\( n \\) steps. Special cases recover familiar targets:</p> \\[ G_t^{(1)} = R_{t+1} + \\gamma V(S_{t+1}) \\quad (\\text{TD(0)}),\\qquad G_t^{(\\infty)}=\\sum_{i=1}^{T-t}\\gamma^{i-1}R_{t+i}\\quad (\\text{MC}). \\] <p>If the episode terminates before \\( t+n \\), the bootstrap term is omitted and the sum truncates naturally at termination.</p> <p>\\( n \\)-step TD update. Using \\( G_t^{(n)} \\) yields the standard TD update form</p> \\[ V(S_t)\\leftarrow V(S_t)+\\alpha\\bigl(G_t^{(n)}-V(S_t)\\bigr). \\] <p>Choosing \\( n \\) (trade-off). Smaller \\( n \\) means heavier bootstrapping (typically higher bias but lower variance and more online-friendly updates), while larger \\( n \\) uses more real rewards (typically lower bias but higher variance and potentially delayed updates). Intermediate \\( n \\) often works well in practice by propagating multi-step information without the full variance of pure MC.</p>"},{"location":"RL-5-Model%20free%20prediction/#averaging-n-step-returns","title":"Averaging \\( n \\)-Step Returns","text":"<p>Rather than choosing a single horizon \\( n \\), we can form a target by mixing multiple \\( n \\)-step returns: shorter backups tend to give lower-variance, more stable updates, while longer backups propagate credit farther through the episode. For instance,</p> \\[ \\frac{1}{2}G_t^{(2)}+\\frac{1}{2}G_t^{(4)} \\] <p>blends a shallow (2-step) and a deeper (4-step) return. In general, we would like a principled mixture over many \\( n \\) values without explicitly computing every \\( G_t^{(n)} \\).</p>"},{"location":"RL-5-Model%20free%20prediction/#lambda-return","title":"\\( \\lambda \\)-Return","text":"<p>The \\( \\lambda \\)-return provides exactly such a mixture by averaging all \\( n \\)-step returns with geometric weights:</p> \\[ G_t^{\\lambda} \\;=\\; (1-\\lambda)\\sum_{n=1}^{\\infty}\\lambda^{n-1}G_t^{(n)}, \\qquad \\lambda\\in[0,1]. \\] <p>This yields a proper weighted average over backup depths and introduces a single interpolation parameter: \\( \\lambda=0 \\) places all weight on \\( n=1 \\), giving \\( G_t^{\\lambda}=G_t^{(1)} \\) (TD(0)), while \\( \\lambda\\to 1 \\) shifts mass to long horizons and approaches the MC return (less bootstrapping, more reliance on observed rewards).</p>"},{"location":"RL-5-Model%20free%20prediction/#eligibility-traces","title":"Eligibility Traces","text":"<p>The previous section introduced the \\(\\lambda\\)-return as a way to assign credit across multiple time steps by blending \\(n\\)-step returns. The drawback is efficiency: computing it directly would require explicitly forming many \\(G_t^{(n)}\\). We therefore want an online procedure that achieves the same multi-step credit assignment without enumerating these returns and eligibility traces provide exactly this mechanism. Eligibility traces maintain a decaying \"memory\" of recently visited states. Each visit boosts a state's trace, which then fades over time while repeated visits reinforce it. When a TD error occurs, we distribute that error backward in proportion to the current trace, so recent and frequently visited states receive larger updates, while distant one-off states receive little.</p> <p>Rat example: frequency vs. recency.</p> <p>Consider the sequence \\(\\text{lever},\\text{lever},\\text{lever},\\text{bell},\\text{shock}\\). A pure frequency rule would assign most blame to \\(\\text{lever}\\) (it appears three times), while a pure recency rule would assign most blame to \\(\\text{bell}\\) (it is closest to the shock). Eligibility traces do both: \\(\\text{bell}\\) receives a large share because it is most recent, and \\(\\text{lever}\\) can still receive substantial blame because its trace has been \"topped up\" multiple times despite decay.</p> <p>State eligibility trace.</p> <p>Formally, maintain a trace \\(E_t(s)\\) for each state \\(s\\). Each time step applies two operations: (\\(i\\)) decay all traces by a factor \\(\\gamma\\lambda\\), making older visits matter less, and (\\(ii\\)) increment the currently visited state by \\(+1\\), marking it as freshly visited. This yields the standard accumulating trace recursion</p> \\[ E_0(s)=0,\\qquad E_t(s)=\\gamma\\lambda\\,E_{t-1}(s) + \\mathbf{1}(S_t=s). \\] <p>Here \\(\\gamma\\) is the discount factor from the problem definition, and \\(\\lambda\\in[0,1]\\) controls how slowly traces decay: larger \\(\\lambda\\) makes traces persist longer and allows credit to propagate farther into the past. Equivalently, the effective memory length is governed by the product \\(\\gamma\\lambda\\): if \\(\\gamma\\lambda\\) is small, traces vanish quickly, if it is near \\(1\\), traces remain significant for many steps.</p>"},{"location":"RL-5-Model%20free%20prediction/#backward-view-of-tdlambda","title":"Backward view of TD(\\(\\lambda\\))","text":"<p>The update above is called the backward view of TD(\\(\\lambda\\)) because each one-step TD error \\(\\delta_t\\) is computed locally from \\((S_t,R_{t+1},S_{t+1})\\) but its effect is propagated backward through time via the traces: states with larger \\(e_t(s)\\) receive a larger portion of the update \\(\\alpha\\,\\delta_t\\,e_t(s)\\), while states with small traces receive negligible updates. Thus, traces turn a local error signal into multi-step credit assignment.</p>"},{"location":"RL-5-Model%20free%20prediction/#forward-view-of-tdlambda","title":"Forward view of TD(\\(\\lambda\\))","text":"<p>The forward view describes TD(\\(\\lambda\\)) in terms of the target each visited state is trying to match. If we could look ahead, then at time \\(t\\) we would form the \\(\\lambda\\)-return</p> \\[ G_t^\\lambda \\;=\\; (1-\\lambda)\\sum_{n=1}^{T-t}\\lambda^{n-1}G_t^{(n)} \\;+\\; \\lambda^{T-t}G_t^{(T-t)}, \\] <p>a geometrically weighted average of the \\(n\\)-step returns \\(G_t^{(n)}\\). The forward-view update is then simply</p> \\[ V(S_t)\\leftarrow V(S_t)+\\alpha\\bigl(G_t^{\\lambda}-V(S_t)\\bigr). \\] <p>This is conceptually clean, but it has its practical limitation: \\(G_t^\\lambda\\) depends on future rewards and is therefore not directly available at time \\(t\\).</p> <p>Summary.</p> <p>Eligibility traces turn a one-step TD error into a multi-step learning update by keeping a decaying record of recently (and repeatedly) visited states, so \u201crecent and frequent\u201d states get the most credit.</p>"},{"location":"RL-5-Model%20free%20prediction/#td1-as-monte-carlo-and-the-forwardbackward-equivalence","title":"TD(1) as Monte Carlo, and the forward/backward equivalence","text":"<p>Consider an episodic trajectory \\(S_0,R_1,S_1,\\dots,S_T\\) with terminal \\(S_T\\) and discount \\(\\gamma\\in[0,1]\\). Let \\(V(\\cdot)\\) be the current value estimate and define the (one-step) TD error</p> \\[ \\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t), \\qquad t=0,\\dots,T-1, \\] <p>with \\(V(S_T)=0\\) (no bootstrapping past the terminal state).</p>"},{"location":"RL-5-Model%20free%20prediction/#forward-view-lambda-return-and-the-case-lambda1","title":"Forward view: \\(\\lambda\\)-return and the case \\(\\lambda=1\\)","text":"<p>The Monte Carlo return from time \\(t\\) is</p> \\[ G_t =\\; R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{T-t-1}R_T=  \\sum_{i=0}^{T-t-1} \\gamma^i R_{t+1+i}. \\] <p>In the forward view, TD(\\(\\lambda\\)) updates \\(V(S_t)\\) toward the \\(\\lambda\\)-return \\(G_t^\\lambda\\), a geometrically weighted mixture of \\(n\\)-step returns.</p> \\[ G_t^\\lambda \\;=\\; (1-\\lambda)\\sum_{n=1}^{T-t}\\lambda^{n-1}G_t^{(n)} \\;+\\; \\lambda^{T-t}G_t^{(T-t)}, \\] <p>Set \\(\\lambda=1\\):</p> \\[ G_t^{1} = (1-1)\\sum_{n=1}^{T-t}1^{\\,n-1}G_t^{(n)} \\;+\\; 1^{\\,T-t}G_t^{(T-t)} = 0 \\;+\\; G_t^{(T-t)} = G_t^{(T-t)}. \\] <p>Using \\(G_t^{(n)}=\\sum_{i=0}^{n-1}\\gamma^i R_{t+1+i}+\\gamma^n V(S_{t+n})\\), set \\(n=T-t\\):</p> \\[ G_t^{(T-t)}=\\sum_{i=0}^{T-t-1}\\gamma^i R_{t+1+i}+\\gamma^{T-t}V(S_T) =\\sum_{i=0}^{T-t-1}\\gamma^i R_{t+1+i}=G_t \\] <p>since \\(S_T\\) is terminal and \\(V(S_T)=0\\). Therefore, when \\(\\lambda=1\\), the mixture places all its weight on the longest backup, so the \\(\\lambda\\)-return reduces to the full Monte Carlo return \\(G_t^1=G_t^{(T-t)}=G_t\\). Thus the forward-view TD(\\(1\\)) update becomes the standard Monte Carlo update</p> \\[ V(S_t) \\leftarrow V(S_t) + \\alpha\\big(G_t - V(S_t)\\big). \\]"},{"location":"RL-5-Model%20free%20prediction/#backward-view-eligibility-traces-and-equivalence","title":"Backward view: eligibility traces and equivalence","text":"<p>The backward view implements the same overall effect online without explicitly forming \\(G_t^\\lambda\\). Using accumulating traces,</p> \\[ E_t(s) = \\gamma\\lambda\\,E_{t-1}(s) + \\mathbf{1}\\{S_t=s\\}, \\qquad E_{-1}(s)=0, \\] <p>each TD error is distributed to all states proportionally to their current eligibility:</p> \\[ V(s) \\leftarrow V(s) + \\alpha\\,\\delta_t\\,E_t(s). \\] <p>Summing these per-step increments over an episode gives the net change to state \\(s\\),</p> \\[ \\Delta V(s) \\;=\\; \\sum_{t=0}^{T-1}\\alpha\\,\\delta_t\\,E_t(s). \\] <p>A standard way to see the equivalence is to rewrite the forward-view error in terms of TD errors and then swap the order of summation.</p> <p>Step 1: \\(n\\)-step error as a sum of TD errors (telescoping).</p> <p>Starting from the definition,</p> \\[ G_t^{(n)} - V(S_t) = \\sum_{i=0}^{n-1}\\gamma^i R_{t+1+i} + \\gamma^n V(S_{t+n}) - V(S_t). \\] <p>Rewrite the rewards using the TD error identity</p> \\[ \\delta_{t+i}=R_{t+i+1}+\\gamma V(S_{t+i+1})-V(S_{t+i}) \\quad\\Longrightarrow\\quad R_{t+i+1}=\\delta_{t+i}-\\gamma V(S_{t+i+1})+V(S_{t+i}). \\] <p>Substitute into the return:</p> \\[ G_t^{(n)} - V(S_t) = \\sum_{i=0}^{n-1}\\gamma^i\\big(\\delta_{t+i}-\\gamma V(S_{t+i+1})+V(S_{t+i})\\big)     +\\gamma^n V(S_{t+n})-V(S_t) \\] \\[ = \\sum_{i=0}^{n-1}\\gamma^i\\delta_{t+i}    \\;-\\;\\sum_{i=0}^{n-1}\\gamma^{i+1}V(S_{t+i+1})    \\;+\\;\\sum_{i=0}^{n-1}\\gamma^i V(S_{t+i})    \\;+\\;\\gamma^n V(S_{t+n})-V(S_t). \\] <p>Now the value terms telescope: split the last two sums,</p> \\[ \\sum_{i=0}^{n-1}\\gamma^i V(S_{t+i}) = V(S_t)+\\sum_{i=1}^{n-1}\\gamma^i V(S_{t+i}), \\qquad \\sum_{i=0}^{n-1}\\gamma^{i+1}V(S_{t+i+1})=\\sum_{i=1}^{n}\\gamma^i V(S_{t+i}), \\] <p>Consider the value-only terms:</p> \\[ -\\sum_{i=0}^{n-1}\\gamma^{i+1}V(S_{t+i+1}) +\\sum_{i=0}^{n-1}\\gamma^i V(S_{t+i}) +\\gamma^n V(S_{t+n})-V(S_t). \\] <p>Substituting:</p> \\[ -\\sum_{i=1}^{n}\\gamma^i V(S_{t+i}) + V(S_t)+\\sum_{i=1}^{n-1}\\gamma^i V(S_{t+i}) +\\;\\gamma^n V(S_{t+n})-V(S_t) =0 \\] <p>Therefore,</p> \\[ G_t^{(n)} - V(S_t)=\\sum_{i=0}^{n-1}\\gamma^i\\delta_{t+i}. \\] <p>Finally, re-index with \\(k=t+i\\):</p> \\[ \\sum_{i=0}^{n-1}\\gamma^i\\delta_{t+i} =\\sum_{k=t}^{t+n-1}\\gamma^{k-t}\\delta_k, \\] <p>This gives the result:</p> \\[ G_t^{(n)} - V(S_t) =\\sum_{k=t}^{t+n-1}\\gamma^{k-t}\\delta_k. \\] <p>Step 2: \\(\\lambda\\)-return error becomes a discounted sum of TD errors.</p> <p>Plug the identity above into the definition of \\(G_t^\\lambda\\):</p> \\[ G_t^\\lambda - V(S_t) = \\Bigl[(1-\\lambda)\\sum_{n=1}^{T-t}\\lambda^{n-1}G_t^{(n)}+\\lambda^{T-t}G_t^{(T-t)}\\Bigr]-V(S_t) \\] \\[ = (1-\\lambda)\\sum_{n=1}^{T-t}\\lambda^{n-1}\\bigl(G_t^{(n)}-V(S_t)\\bigr) \\;+\\;\\lambda^{T-t}\\bigl(G_t^{(T-t)}-V(S_t)\\bigr) \\] \\[ = (1-\\lambda)\\sum_{n=1}^{T-t}\\lambda^{n-1}\\sum_{k=t}^{t+n-1}\\gamma^{k-t}\\delta_k \\;+\\;\\lambda^{T-t}\\sum_{k=t}^{T-1}\\gamma^{k-t}\\delta_k, \\] <p>where the last equality uses \\(t+(T-t)-1=T-1\\). Fix \\(k\\ge t\\) and collect the coefficient of \\(\\gamma^{k-t}\\delta_k\\).</p> <p>In the first double-sum, \\(\\delta_k\\) appears whenever \\(k\\le t+n-1\\), i.e. \\(n\\ge k-t+1\\), so its total weight there is</p> \\[ (1-\\lambda)\\sum_{n=k-t+1}^{T-t}\\lambda^{n-1}. \\] <p>Adding the second term contributes an additional \\(\\lambda^{T-t}\\). Hence the total coefficient on \\(\\gamma^{k-t}\\delta_k\\) is</p> \\[ (1-\\lambda)\\sum_{n=k-t+1}^{T-t}\\lambda^{n-1}+\\lambda^{T-t}. \\] <p>Evaluate the geometric sum:</p> \\[ \\sum_{n=k-t+1}^{T-t}\\lambda^{n-1} =\\sum_{j=k-t}^{T-t-1}\\lambda^{j} =\\lambda^{k-t}\\frac{1-\\lambda^{T-k}}{1-\\lambda}. \\] <p>Therefore,</p> \\[ (1-\\lambda)\\sum_{n=k-t+1}^{T-t}\\lambda^{n-1}+\\lambda^{T-t} =\\lambda^{k-t}(1-\\lambda^{T-k})+\\lambda^{T-t} =\\lambda^{k-t}. \\] <p>So the coefficient of \\(\\gamma^{k-t}\\delta_k\\) is \\(\\lambda^{k-t}\\), and we obtain</p> \\[ G_t^\\lambda - V(S_t) =\\sum_{k=t}^{T-1}\\gamma^{k-t}\\lambda^{k-t}\\delta_k =\\sum_{k=t}^{T-1}(\\gamma\\lambda)^{k-t}\\delta_k. \\] <p>Step 3: accumulate forward-view updates and swap sums.</p> <p>Sum the forward-view updates affecting a particular state \\(s\\):</p> \\[ \\sum_{t=0}^{T-1}\\alpha\\bigl(G_t^\\lambda - V(S_t)\\bigr)\\mathbf{1}\\{S_t=s\\} = \\sum_{t=0}^{T-1}\\alpha\\,\\mathbf{1}\\{S_t=s\\}\\sum_{k=t}^{T-1}(\\gamma\\lambda)^{k-t}\\delta_k \\] \\[ = \\alpha\\sum_{t=0}^{T-1}\\sum_{k=t}^{T-1} \\mathbf{1}\\{S_t=s\\}(\\gamma\\lambda)^{k-t}\\delta_k \\] <p>Start with the double sum</p> \\[ \\alpha\\sum_{t=0}^{T-1}\\sum_{k=t}^{T-1} \\mathbf{1}\\{S_t=s\\}(\\gamma\\lambda)^{k-t}\\delta_k. \\] <p>The index set here is the triangle</p> \\[ \\{(t,k): 0\\le t\\le T-1,\\; t\\le k\\le T-1\\} \\;=\\; \\{(t,k): 0\\le k\\le T-1,\\; 0\\le t\\le k\\}. \\] <p>So we can swap the order:</p> \\[ \\alpha\\sum_{t=0}^{T-1}\\sum_{k=t}^{T-1} (\\cdots) \\;=\\; \\sum_{k=0}^{T-1}\\alpha\\,\\delta_k \\sum_{t=0}^{k}(\\gamma\\lambda)^{k-t}\\mathbf{1}\\{S_t=s\\}. \\] <p>Step 4: the inner sum is exactly the trace.</p> <p>Unrolling the accumulating-trace recursion</p> \\[ E_k(s)=\\gamma\\lambda E_{k-1}(s)+\\mathbf{1}\\{S_k=s\\},\\qquad E_{-1}(s)=0, \\] <p>gives</p> \\[ E_k(s)=\\sum_{t=0}^{k}(\\gamma\\lambda)^{k-t}\\mathbf{1}\\{S_t=s\\}. \\] <p>Substitute this into the expression above to obtain</p> \\[ \\sum_{t=0}^{T-1}\\alpha\\bigl(G_t^\\lambda - V(S_t)\\bigr)\\mathbf{1}\\{S_t=s\\} \\;=\\; \\sum_{k=0}^{T-1}\\alpha\\,\\delta_k\\,E_k(s), \\] <p>which is the desired forward/backward equivalence.</p> <p>Therefore the forward view explains what is being optimized (learning toward \\(G_t^\\lambda\\)), while the backward view explains how to realize it incrementally via traces. In particular, when \\(\\lambda=1\\) we have \\(G_t^\\lambda = G_t\\), so the episode-level effect of TD(1) matches Monte Carlo.</p>"},{"location":"RL-5-Model%20free%20prediction/#special-case-s-is-visited-only-once","title":"Special case: \\(s\\) is visited only once","text":"<p>Assume a state \\(s\\) appears exactly once, at time \\(k\\) (i.e., \\(S_k=s\\) and \\(S_t\\neq s\\) for \\(t\\neq k\\)). For TD(1), the trace recursion becomes a pure geometric decay after the visit:</p> \\[ E_t(s)=\\gamma E_{t-1}(s)+\\mathbf{1}(S_t=s) \\;=\\; \\begin{cases} 0, &amp; t&lt;k,\\\\[2pt] \\gamma^{t-k}, &amp; t\\ge k. \\end{cases} \\] <p>Hence the total change to \\(V(s)\\) over the episode is a discounted sum of subsequent TD errors:</p> \\[ \\Delta V(s) \\;=\\; \\alpha\\sum_{t=k}^{T-1}\\gamma^{t-k}\\delta_t. \\] <p>This sum telescopes: the \\(-V(S_t)\\) term in \\(\\delta_t\\) cancels with the \\(+\\gamma V(S_{t+1})\\) term from the previous step once we apply the discount powers. What remains is exactly \u201cobserved return minus current estimate\u201d:</p> \\[ \\sum_{t=k}^{T-1}\\gamma^{t-k}\\delta_t = \\Big(\\sum_{i=0}^{T-k-1}\\gamma^i R_{k+1+i}\\Big) - V(S_k) = G_k - V(S_k). \\] <p>Therefore,</p> \\[ \\Delta V(s) \\;=\\; \\alpha\\big(G_k - V(S_k)\\big), \\] <p>which is precisely the Monte Carlo update for the single visit at time \\(k\\).</p>"},{"location":"RL-5-Model%20free%20prediction/#telescoping-in-td1-why-it-matches-mc-at-the-episode-level","title":"Telescoping in TD(1) (why it matches MC at the episode level)","text":"<p>Let's assume we are working in an episodic setting with discount \\(\\gamma\\in[0,1]\\), terminal time \\(T\\), and value estimate \\(V\\) with the terminal convention \\(V(S_T)=0\\). Define the one-step TD error</p> \\[ \\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t), \\qquad t=0,\\dots,T-1. \\] <p>A telescoping identity. From the \\(n\\)-step derivation already shown earlier (the identity \\(G_t^{(n)}-V(S_t)=\\sum_{k=t}^{t+n-1}\\gamma^{k-t}\\delta_k\\)), take \\(n=T-t\\) to obtain</p> \\[ \\sum_{k=t}^{T-1}\\gamma^{k-t}\\delta_k \\;=\\; G_t^{(T-t)} - V(S_t). \\] <p>Since \\(S_T\\) is terminal and \\(V(S_T)=0\\), the \\((T-t)\\)-step return is exactly the Monte Carlo return:</p> \\[ G_t^{(T-t)} \\;=\\; \\sum_{i=0}^{T-t-1}\\gamma^i R_{t+1+i} \\;=\\; G_t. \\] <p>Combining these gives the telescoping relation</p> \\[ \\delta_t + \\gamma\\delta_{t+1} + \\cdots + \\gamma^{T-1-t}\\delta_{T-1} \\;=\\; G_t - V(S_t). \\] <p>(Note: the explicit term-by-term cancellation expansion is the same telescoping argument used in the earlier \\(n\\)-step identity, so it is omitted here.)</p>"},{"location":"RL-5-Model%20free%20prediction/#td1-is-every-visit-mc-when-updates-are-applied-offline","title":"TD(1) is every-visit MC when updates are applied offline","text":"<p>In the backward view, TD(\\(\\lambda\\)) applies per-step increments</p> \\[ \\Delta V(s)\\!\\mid_t \\;=\\; \\alpha\\,\\delta_t\\,E_t(s), \\qquad E_t(s)=\\gamma\\lambda E_{t-1}(s)+\\mathbf{1}\\{S_t=s\\}, \\] <p>and the episode-level (offline) change is the sum of these increments:</p> \\[ \\Delta V(s)\\;=\\;\\sum_{t=0}^{T-1}\\alpha\\,\\delta_t\\,E_t(s). \\] <p>For \\(\\lambda=1\\), unrolling the trace recursion gives the explicit form</p> \\[ E_u(s)\\;=\\;\\sum_{t=0}^{u}\\gamma^{u-t}\\mathbf{1}\\{S_t=s\\}, \\] <p>so</p> \\[ \\Delta V(s) =\\sum_{u=0}^{T-1}\\alpha\\,\\delta_u\\sum_{t=0}^{u}\\gamma^{u-t}\\mathbf{1}\\{S_t=s\\} =\\sum_{t=0}^{T-1}\\alpha\\,\\mathbf{1}\\{S_t=s\\}\\sum_{u=t}^{T-1}\\gamma^{u-t}\\delta_u, \\] <p>where we swapped the order of summation over the region \\(0\\le t\\le u\\le T-1\\).</p> <p>By the telescoping identity,</p> \\[ \\sum_{u=t}^{T-1}\\gamma^{u-t}\\delta_u \\;=\\; G_t - V(S_t), \\] <p>hence</p> \\[ \\Delta V(s) =\\sum_{t=0}^{T-1}\\alpha\\bigl(G_t - V(S_t)\\bigr)\\mathbf{1}\\{S_t=s\\}, \\] <p>which is exactly the state-wise form of the every-visit Monte Carlo update.</p> <p>What differs in practice?</p> <p>The equality above is an episode-level statement (often presented for offline/accumulated updates). Conceptually, MC computes \\(G_t\\) directly from the full reward sequence after the episode ends, while TD(1) constructs the same quantity indirectly: it generates one-step TD errors online and uses traces to \u201croute\u201d those errors back to earlier states. The end result matches, but the bookkeeping and when information is used are different.</p>"},{"location":"RL-5-Model%20free%20prediction/#telescoping-in-tdlambda-and-the-forwardbackward-match","title":"Telescoping in TD(\\(\\lambda\\)) and the forward/backward match","text":"<p>Assume an episodic trajectory \\(S_0,R_1,S_1,\\dots,S_T\\) with discount \\(\\gamma\\in[0,1]\\) and terminal convention \\(V(S_T)=0\\). Define the one-step TD error</p> \\[ \\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t), \\qquad t=0,\\dots,T-1. \\] <p>\\(\\lambda\\)-return as a weighted sum of \\(n\\)-step targets. The \\(n\\)-step return is</p> \\[ G_t^{(n)} = \\sum_{i=0}^{n-1}\\gamma^i R_{t+1+i} + \\gamma^n V(S_{t+n}), \\] <p>and the episodic \\(\\lambda\\)-return can be written (equivalently) as a geometric mixture over horizons, with the remaining probability mass placed on the full return:</p> \\[ G_t^\\lambda = (1-\\lambda)\\sum_{n=1}^{T-t-1}\\lambda^{n-1}G_t^{(n)} \\;+\\; \\lambda^{T-t-1}G_t. \\] <p>TD errors telescope to the \\(\\lambda\\)-error. Using the identity proved earlier,</p> \\[ G_t^{(n)} - V(S_t) \\;=\\; \\sum_{k=t}^{t+n-1}\\gamma^{k-t}\\delta_k, \\] <p>one obtains (by substituting into the \\(\\lambda\\)-return and collecting coefficients) the compact telescoping form</p> \\[ G_t^\\lambda - V(S_t) \\;=\\; \\sum_{k=t}^{T-1}(\\gamma\\lambda)^{k-t}\\delta_k \\;=\\; \\delta_t + (\\gamma\\lambda)\\delta_{t+1} + \\cdots + (\\gamma\\lambda)^{T-1-t}\\delta_{T-1}. \\] <p>(The term-by-term cancellation intuition is the same telescoping argument already used in the \\(n\\)-step case, so we omit a second expansion here.)</p>"},{"location":"RL-5-Model%20free%20prediction/#backward-view-eligibility-traces-recover-the-same-target","title":"Backward view: eligibility traces recover the same target","text":"<p>The backward view implements the update online without explicitly forming \\(G_t^\\lambda\\). It maintains accumulating traces</p> \\[ E_t(s) = \\gamma\\lambda\\,E_{t-1}(s) + \\mathbf{1}\\{S_t=s\\}, \\qquad E_{-1}(s)=0, \\] <p>and applies the per-step increment</p> \\[ \\Delta V(s)\\!\\mid_t \\;=\\; \\alpha\\,\\delta_t\\,E_t(s). \\] <p>Thus each local TD error \\(\\delta_t\\) is distributed across previously visited states, with geometric decay controlled by \\(\\gamma\\lambda\\). To see the link to the forward view without repeating the full derivation, unroll the trace:</p> \\[ E_t(s) \\;=\\; \\sum_{k=0}^{t}(\\gamma\\lambda)^{t-k}\\mathbf{1}\\{S_k=s\\}. \\] <p>Accumulating increments over an episode (offline) gives</p> \\[ \\Delta V(s) =\\sum_{t=0}^{T-1}\\alpha\\,\\delta_t\\,E_t(s) \\] \\[ =\\sum_{t=0}^{T-1}\\alpha\\,\\delta_t\\sum_{k=0}^{t}(\\gamma\\lambda)^{t-k}\\mathbf{1}\\{S_k=s\\} \\] \\[ =\\sum_{k=0}^{T-1}\\alpha\\,\\mathbf{1}\\{S_k=s\\}\\sum_{t=k}^{T-1}(\\gamma\\lambda)^{t-k}\\delta_t, \\] <p>where the last step is just swapping sums over \\(0\\le k\\le t\\le T-1\\). By the telescoping identity,</p> \\[ \\sum_{t=k}^{T-1}(\\gamma\\lambda)^{t-k}\\delta_t \\;=\\; G_k^\\lambda - V(S_k), \\] <p>so</p> \\[ \\Delta V(s) =\\sum_{k=0}^{T-1}\\alpha\\bigl(G_k^\\lambda - V(S_k)\\bigr)\\mathbf{1}\\{S_k=s\\}, \\] <p>which is exactly the episode-level (offline) forward-view update, written state-by-state.</p>"},{"location":"RL-5-Model%20free%20prediction/#online-vs-offline-when-do-forward-and-backward-tdlambda-agree","title":"Online vs. Offline: when do forward and backward TD(\\(\\lambda\\)) agree?","text":"<p>Fix an episodic trajectory \\(S_0,R_1,S_1,\\dots,S_T\\) with discount \\(\\gamma\\in[0,1]\\) and terminal convention \\(V(S_T)=0\\). Let</p> \\[ \\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t), \\qquad t=0,\\dots,T-1. \\] <p>What is being compared? The forward view specifies a target \\(G_t^\\lambda\\) for each visit and updates \\(V(S_t)\\) toward it, the backward view produces updates online by distributing each \\(\\delta_t\\) through eligibility traces. The two views coincide only once we fix when \\(V\\) is allowed to change.</p>"},{"location":"RL-5-Model%20free%20prediction/#offline-frozen-v-updates-exact-equality","title":"Offline (frozen-\\(V\\)) updates: exact equality","text":"<p>If the episode is generated with \\(V\\) held fixed and all changes are applied at the end, then the forward-view and backward-view episode updates are identical:</p> \\[ \\sum_{t=0}^{T-1}\\alpha\\,\\delta_t\\,E_t(s) \\;=\\; \\sum_{t=0}^{T-1}\\alpha\\bigl(G_t^\\lambda - V(S_t)\\bigr)\\mathbf{1}\\{S_t=s\\}. \\] <p>This is exactly the equivalence derived in the previous section (it relies on using the same frozen \\(V\\) inside all bootstrapping terms and TD errors throughout the episode). The endpoint cases discussed earlier follow immediately: \\(\\lambda=0\\) gives TD(0), and \\(\\lambda=1\\) gives TD(1), whose episode-level update matches every-visit Monte Carlo via telescoping.</p>"},{"location":"RL-5-Model%20free%20prediction/#online-updates-why-equality-can-fail","title":"Online updates: why equality can fail","text":"<p>With online learning, \\(V\\) is modified after each transition. Then the forward view becomes ambiguous because its target \\(G_t^\\lambda\\) contains bootstrapping terms \\(V(S_{t+n})\\): under online updates there is no single \u201c\\(V\\)\u201d to plug in\u2014those values depend on which intermediate version of \\(V\\) we use. The backward view, meanwhile, propagates each new \\(\\delta_t\\) through traces computed from past visits, but applies it using the current \\(V\\). Because both the TD errors and the bootstrapping values are now tied to a moving function, the clean algebraic match from the frozen-\\(V\\) case no longer goes through in general. (An exception is \\(\\lambda=0\\), where the target is purely one-step and the forward/backward views coincide in the usual way.)</p> <p>Restoring exact online equivalence: True Online TD(\\(\\lambda\\)). True Online TD(\\(\\lambda\\)) modifies the trace-based update with a small correction so that, even when \\(V\\) changes during the episode, the backward-view recursion matches the corresponding online forward view exactly at each step.</p>"},{"location":"RL-5-Model%20free%20prediction/#summary-of-forward-and-backward-tdlambda","title":"Summary of Forward and Backward TD(\\(\\lambda\\))","text":"Offline updates \\(\\lambda = 0\\) \\(\\lambda \\in (0,1)\\) \\(\\lambda = 1\\) Backward view \\(\\mathrm{TD}(0)\\) \\(\\mathrm{TD}(\\lambda)\\) \\(\\mathrm{TD}(1)\\) \\(\\parallel\\) \\(\\parallel\\) \\(\\parallel\\) Forward view \\(\\mathrm{TD}(0)\\) Forward \\(\\mathrm{TD}(\\lambda)\\) \\(\\mathrm{MC}\\) Online updates \\(\\lambda = 0\\) \\(\\lambda \\in (0,1)\\) \\(\\lambda = 1\\) Backward view \\(\\mathrm{TD}(0)\\) \\(\\mathrm{TD}(\\lambda)\\) \\(\\mathrm{TD}(1)\\) \\(\\parallel\\) \\(\\nparallel\\) \\(\\nparallel\\) Forward view \\(\\mathrm{TD}(0)\\) Forward \\(\\mathrm{TD}(\\lambda)\\) \\(\\mathrm{MC}\\) \\(\\parallel\\) \\(\\parallel\\) \\(\\parallel\\) Exact Online \\(\\mathrm{TD}(0)\\) Exact Online \\(\\mathrm{TD}(\\lambda)\\) Exact Online \\(\\mathrm{TD}(1)\\) <p>\\(\\parallel\\) here indicates equivalence in total update at end of episode.</p>"},{"location":"RL-5-Model%20free%20prediction/#references","title":"References","text":"<ul> <li>https://github.com/zyxue/youtube_RL_course_by_David_Silver</li> </ul>"},{"location":"RL-6-Model%20free%20control/","title":"6 - Model Free Control","text":""},{"location":"RL-6-Model%20free%20control/#introduction-to-model-free-control","title":"Introduction to Model-Free Control","text":"<p>Model-free control means learning to maximise the value function of an unknown MDP. The main idea is simple: we do not build or use an explicit transition/reward model. Instead, we learn directly from experience samples, i.e. observed tuples \\((S_t, A_t, R_{t+1}, S_{t+1})\\).</p> <p>In practice, model-free methods show up in two common regimes. First, the MDP model is unknown but we can sample experience by interacting with the environment, so learning is naturally driven by collected trajectories. Second, the MDP model is known in principle (we could write down \\(P(s'|s,a)\\) and \\(R(s,a)\\)), but the state-action space is so large that exact dynamic programming updates are not feasible. In that case, we still fall back to sampled experiences.</p> <p>On-Policy vs Off-Policy Learning.</p> <p>To talk about how data is collected, it helps to distinguish two policies. The target policy \\(\\pi\\) is the policy we want to evaluate or improve, while the behaviour policy \\(\\mu\\) is the policy that actually generates the data.</p> <ul> <li>In on-policy learning, we \"learn on the job\": we learn about \\(\\pi\\) using experience produced by \\(\\pi\\) itself. Equivalently, the data distribution matches the policy being learned:</li> </ul> \\[   (S_t, A_t) \\sim \\pi(\\cdot \\mid S_t). \\] <ul> <li>In off-policy learning, we \"look over someone's shoulder\": we learn about \\(\\pi\\) using experience generated by a different policy \\(\\mu\\). Here the trajectories come from \\(\\mu\\), but the objective is still to optimise or evaluate \\(\\pi\\):</li> </ul> \\[   (S_t, A_t) \\sim \\mu(\\cdot \\mid S_t), \\quad \\text{while optimising/evaluating } \\pi. \\]"},{"location":"RL-6-Model%20free%20control/#from-dp-policy-iteration-to-mc-control","title":"From DP Policy Iteration to MC Control","text":"<p>In dynamic programming (DP), policy iteration alternates between two ideas: first, policy evaluation, where we compute \\(v_\\pi\\) (for example via iterative policy evaluation), and then policy improvement, where we construct a better policy \\(\\pi' \\ge \\pi\\) (often by making \\(\\pi'\\) greedy with respect to the current value estimate). A natural model-free question is whether we can keep the same loop, but replace DP evaluation with Monte Carlo (MC) evaluation:</p> <p>At a high level, the answer is yes: this fits the idea of Generalised Policy Iteration (GPI), where evaluation and improvement are interleaved even when evaluation is only approximate. With MC, we can estimate the value of \\(\\pi\\) directly from sampled experience, using returns:</p> \\[ v_\\pi(s) = \\mathbb{E}_\\pi\\!\\left[G_t \\mid S_t=s\\right], \\qquad G_t = \\sum_{k=0}^{T-t-1}\\gamma^k R_{t+k+1}. \\] <p>So the evaluation step can be done without knowing the model. The catch is the improvement step. In DP, \"greedy improvement over \\(V\\)\" is typically implemented by a one-step lookahead:</p> \\[ \\pi'(s) \\in \\arg\\max_{a\\in\\mathcal{A}} \\sum_{s'} P(s'\\mid s,a)\\Big(R(s,a,s') + \\gamma V(s')\\Big). \\] <p>Even if MC gives a good estimate \\(V(s)\\approx v_\\pi(s)\\), this greedy update still needs \\(P(s'\\mid s,a)\\) (and the expected rewards). So MC evaluation + DP-style greedy improvement is not fully model-free: the evaluation is sample-based, but the improvement step still assumes access to the dynamics.</p>"},{"location":"RL-6-Model%20free%20control/#model-free-fix-improve-using-the-action-value-function-qsa","title":"Model-Free Fix: Improve Using the Action-Value Function \\(Q(s,a)\\)","text":"<p>A clean way to stay model-free is to improve using the action-value function rather than \\(V(s)\\). We estimate</p> \\[ q_\\pi(s,a) = \\mathbb{E}_\\pi\\!\\left[G_t \\mid S_t=s, A_t=a\\right], \\] <p>and then improve greedily without any one-step lookahead:</p> \\[ \\pi'(s) \\in \\arg\\max_{a\\in\\mathcal{A}} q_\\pi(s,a). \\] <p>This step is model-free because it only compares learned estimates of \\(Q(s,a)\\) across actions, instead of predicting next states via \\(P(\\cdot\\mid s,a)\\). This is the basic motivation for MC control via action-values: estimate \\(Q\\) from experience, and act (approximately) greedily with respect to \\(Q\\).</p> <p>Additional Practical Issues with MC in a Control Loop. Even if we ignore the fact that \\(V\\)-greedy improvement needs a model, MC still sufferes from other problems. Standard MC relies on complete returns, so it fits episodic tasks best, in continuing tasks we usually need truncation or a different setup. Returns \\(G_t\\) can also have high variance, especially when horizons are long, which means value estimates may require many episodes to stabilise. Control adds an exploration headache too: if we become fully greedy too early, we may stop trying other actions and never learn whether they are better. Finally, the process is non-stationary: as the policy changes, the data distribution changes with it, so evaluation is always approximate and intertwined with improvement.</p>"},{"location":"RL-6-Model%20free%20control/#exploration-vs-greedy-improvement-varepsilon-greedy-mc-control","title":"Exploration vs Greedy Improvement: \\(\\varepsilon\\)-Greedy MC Control","text":"<p>If we always act greedily with respect to the current estimate \\(Q(s,a)\\) (or \\(V(s)\\)), we risk stopping exploration. Early estimates are noisy, so the action that looks best after a few trials may not actually be best. Once a greedy policy commits, rarely chosen actions stay poorly estimated, and learning can get \"stuck\" in a suboptimal policy. A tiny example makes this concrete. Imagine a single-state choice with two actions (two doors): left and right. We try left once and see reward \\(0\\), then try right once and see reward \\(+1\\). At this moment,</p> \\[ \\hat V(\\texttt{left}) = 0,\\qquad \\hat V(\\texttt{right}) = 1, \\] <p>so a purely greedy agent will keep choosing right and keep updating its estimate from more samples, e.g.</p> \\[ \\hat V(\\texttt{right}) \\leftarrow 2,\\ \\ldots \\] <p>But the important question is: are we sure we picked the best door? If left has uncertainty or rare high payoffs, never trying it again means we can never correct a wrong early guess.</p> <p>A simple fix for this is doing a \\(\\varepsilon\\)-greedy exploration. It keeps the greedy preference, but forces continued exploration by giving every action non-zero probability. To be more concrete, let \\(m = |\\mathcal{A}|\\) be the number of actions, then we can define a greedy action as</p> \\[ a^*(s) \\in \\arg\\max_{a\\in\\mathcal{A}} Q(s,a). \\] <p>The \\(\\varepsilon\\)-greedy rule is: choose \\(a^*(s)\\) with probability \\(1-\\varepsilon\\), and with probability \\(\\varepsilon\\) choose a random action uniformly from \\(\\mathcal{A}\\). Equivalently for each action,</p> \\[ \\pi(a\\mid s) = \\begin{cases} 1-\\varepsilon + \\dfrac{\\varepsilon}{m}, &amp; \\text{if } a=a^*(s),\\\\ \\dfrac{\\varepsilon}{m}, &amp; \\text{otherwise.} \\end{cases} \\] <p>We add the \\(\\varepsilon/m\\) term in the first case because even in the exploration step (which occurs with probability \\(\\varepsilon\\)), the policy chooses uniformly from all \\(m\\) actions and can still select the greedy action with probability \\(1/m\\), therefore</p> \\[ \\pi(a^*(s)\\mid s)=(1-\\varepsilon)+\\varepsilon\\cdot\\frac{1}{m}=1-\\varepsilon+\\frac{\\varepsilon}{m}. \\] <p>This helps in three straightforward ways: it prevents premature convergence by continuing to sample non-greedy actions, it improves the accuracy of \\(Q(s,a)\\) by collecting more balanced data, and it guarantees non-zero exploration. Informally, if \\(\\varepsilon\\) does not decay too fast, every action keeps getting tried often enough for learning to correct itself (formal convergence needs extra conditions).</p> <p>With this in mind, MC control with action-values becomes a practical on-policy loop: we estimate \\(Q \\approx q_\\pi\\) from sampled returns, then we improve the policy toward greedy with respect to \\(Q\\), while behaving according to an \\(\\varepsilon\\)-greedy version to maintain exploration. This is the standard recipe for \\(\\varepsilon\\)-greedy MC control.</p>"},{"location":"RL-6-Model%20free%20control/#varepsilon-greedy-policy-improvement-proof","title":"\\(\\varepsilon\\)-Greedy Policy Improvement (Proof)","text":"<p>Fix an \\(\\varepsilon\\)-greedy policy \\(\\pi\\) and suppose we can evaluate its action-values \\(q_\\pi(s,a)\\).  We construct an improved policy \\(\\pi'\\) by acting \\(\\varepsilon\\)-greedily with respect to \\(q_\\pi\\): in each state \\(s\\) we prefer actions with maximal \\(q_\\pi(s,a)\\), but keep uniform exploration over all actions.</p> <p>Let</p> \\[ a^*(s)\\in \\arg\\max_{a\\in\\mathcal{A}} q_\\pi(s,a),\\qquad m = |\\mathcal{A}|, \\] <p>and define</p> \\[ \\pi'(a\\mid s)= \\begin{cases} 1-\\varepsilon + \\dfrac{\\varepsilon}{m}, &amp; a=a^*(s),\\\\ \\dfrac{\\varepsilon}{m}, &amp; \\text{otherwise}. \\end{cases} \\] <p>Thus \\(\\pi'\\) concentrates probability on greedy actions while assigning positive probability to every action. We now show that this improvement step cannot decrease performance.</p> <p>[Theorem] \\(\\varepsilon\\)-greedy policy improvement. For any \\(\\varepsilon\\)-greedy policy \\(\\pi\\), the \\(\\varepsilon\\)-greedy policy \\(\\pi'\\) with respect to \\(q_\\pi\\) is an improvement, i.e.</p> \\[ v_{\\pi'}(s)\\ge v_\\pi(s)\\quad \\text{for all } s\\in\\mathcal{S}. \\] <p>Proof. Fix a state \\(s\\). For any policy \\(\\pi'\\) we can write:</p> \\[ q_\\pi(s,\\pi'(s))=\\sum_{a\\in\\mathcal{A}}\\pi'(a\\mid s)\\,q_\\pi(s,a). \\] <p>Step 1: Expand \\(q_\\pi(s,\\pi'(s))\\) using the definition of \\(\\pi'\\). Let \\(a^*(s)\\in\\arg\\max_{a\\in\\mathcal{A}}q_\\pi(s,a)\\) and \\(m=|\\mathcal{A}|\\). Then</p> \\[ \\begin{aligned} q_\\pi(s,\\pi'(s)) &amp;=\\Bigl(1-\\varepsilon+\\frac{\\varepsilon}{m}\\Bigr)q_\\pi\\bigl(s,a^*(s)\\bigr)   \\;+\\;\\sum_{a\\neq a^*(s)}\\frac{\\varepsilon}{m}\\,q_\\pi(s,a)\\\\ &amp;=(1-\\varepsilon)\\,q_\\pi\\bigl(s,a^*(s)\\bigr)   \\;+\\;\\frac{\\varepsilon}{m}\\sum_{a\\in\\mathcal{A}}q_\\pi(s,a)\\\\ &amp;=(1-\\varepsilon)\\max_{a\\in\\mathcal{A}}q_\\pi(s,a)\\;+\\;\\frac{\\varepsilon}{m}\\sum_{a\\in\\mathcal{A}}q_\\pi(s,a). \\end{aligned} \\] <p>This is exactly \"mostly greedy\" plus a uniform exploration average.</p> <p>Step 2: Compare the greedy maximum to the \\(q_\\pi\\)-expectation under \\(\\pi\\). A maximum dominates every convex combination: for any distribution \\(w(\\cdot)\\) on \\(\\mathcal{A}\\),</p> \\[ \\max_{a\\in\\mathcal{A}}q_\\pi(s,a)\\;\\ge\\;\\sum_{a\\in\\mathcal{A}}w(a)\\,q_\\pi(s,a). \\] <p>Define</p> \\[ w(a)\\;=\\frac{\\pi(a\\mid s)-\\varepsilon/m}{1-\\varepsilon}. \\] <p>This is a valid distribution:</p> <ul> <li>Nonnegativity: since \\(\\pi\\) is \\(\\varepsilon\\)-greedy, \\(\\pi(a\\mid s)\\ge \\varepsilon/m\\) for all \\(a\\), hence \\(w(a)\\ge 0\\).</li> <li>Normalization:</li> </ul> \\[ \\sum_{a\\in\\mathcal{A}}w(a) =\\frac{\\sum_a\\pi(a\\mid s)-\\sum_a\\varepsilon/m}{1-\\varepsilon} =\\frac{1-\\varepsilon}{1-\\varepsilon}=1. \\] <p>Plugging this \\(w\\) into the inequality and multiplying by \\((1-\\varepsilon)\\) gives</p> \\[ (1-\\varepsilon)\\max_{a\\in\\mathcal{A}}q_\\pi(s,a) \\;\\ge\\;\\sum_{a\\in\\mathcal{A}}\\bigl(\\pi(a\\mid s)-\\varepsilon/m\\bigr)\\,q_\\pi(s,a). \\] <p>Step 3: Combine with Step 1 to obtain the key statewise inequality. Using the expansion from Step 1,</p> \\[ \\begin{aligned} q_\\pi(s,\\pi'(s)) &amp;=\\frac{\\varepsilon}{m}\\sum_{a\\in\\mathcal{A}}q_\\pi(s,a)\\;+\\;(1-\\varepsilon)\\max_{a\\in\\mathcal{A}}q_\\pi(s,a)\\\\ &amp;\\ge \\frac{\\varepsilon}{m}\\sum_{a\\in\\mathcal{A}}q_\\pi(s,a)  + \\sum_{a\\in\\mathcal{A}}\\bigl(\\pi(a\\mid s)-\\varepsilon/m\\bigr)\\,q_\\pi(s,a)\\\\ &amp;=\\sum_{a\\in\\mathcal{A}}\\pi(a\\mid s)\\,q_\\pi(s,a)\\\\ &amp;=q_\\pi(s,\\pi(s))\\\\ &amp;=v_\\pi(s), \\end{aligned} \\] <p>where the last equality is the standard identity \\(v_\\pi(s)=\\sum_a \\pi(a\\mid s)\\,q_\\pi(s,a)\\).</p> <p>Thus, for every state \\(s\\),</p> \\[ q_\\pi\\bigl(s,\\pi'(s)\\bigr)\\;\\ge\\;v_\\pi(s). \\] <p>Step 4: Conclude \\(v_{\\pi'}\\ge v_\\pi\\) (policy-improvement step). Recall that for any \\(s\\),</p> \\[ (T_{\\pi'}v_\\pi)(s)\\;=\\;\\sum_{a\\in\\mathcal{A}}\\pi'(a\\mid s)\\Bigl[r(s,a)+\\gamma\\sum_{s'}P(s'\\mid s,a)\\,v_\\pi(s')\\Bigr] =\\sum_{a\\in\\mathcal{A}}\\pi'(a\\mid s)\\,q_\\pi(s,a) =q_\\pi(s,\\pi'(s)). \\] <p>So the inequality above is exactly</p> \\[ (T_{\\pi'}v_\\pi)(s)\\;\\ge\\;v_\\pi(s)\\qquad \\forall s. \\] <p>Applying \\(T_{\\pi'}\\) repeatedly and using monotonicity yields</p> \\[ v_\\pi \\;\\le\\; T_{\\pi'}v_\\pi \\;\\le\\; T_{\\pi'}^2v_\\pi \\;\\le\\;\\cdots \\] <p>and since \\(T_{\\pi'}\\) is a \\(\\gamma\\)-contraction, \\(T_{\\pi'}^k v_\\pi \\to v_{\\pi'}\\) as \\(k\\to\\infty\\). Therefore,</p> \\[ v_{\\pi'}(s)\\;\\ge\\;v_\\pi(s)\\qquad \\forall s. \\] <p>Practical Note. The policy-improvement result above assumes access to the true action-values \\(q_\\pi\\). In MC control we only have an estimate \\(Q\\approx q_\\pi\\), so the improvement step is approximate and performance need not increase monotonically after each episode. Moreover, while \\(\\varepsilon\\)-greedy ensures continued exploration in any visited state,</p> \\[ \\pi(a\\mid s)\\ge \\varepsilon/m, \\] <p>it does not provide a useful bound on how quickly the process visits all states or if it tries each action often enough to accurately estimate \\(q_\\pi\\). In large or poorly-connected MDPs, some regions may be reached very rarely, making naive exploration slow and inefficient. In practice, exploration is often made more directed (e.g. optimistic initialisation, UCB-style bonuses, Boltzmann/softmax exploration), and many algorithms switch to TD methods (e.g. SARSA, Q-learning) that learn online without waiting for episode termination.</p>"},{"location":"RL-6-Model%20free%20control/#greedy-in-the-limit-with-infinite-exploration-glie","title":"Greedy in the Limit with Infinite Exploration (GLIE)","text":"<p>A fixed \\(\\varepsilon&gt;0\\) in \\(\\varepsilon\\)-greedy control is great for exploration, but it has an obvious downside: the policy stays random forever, so it never becomes fully greedy. What we usually want is a schedule that explores a lot early (when \\(Q\\) is unreliable) and then gradually exploits more as the estimates improve. GLIE is a standard way to formalise this idea.</p> <p>Definition. A sequence of policies \\(\\{\\pi_k\\}_{k\\ge 1}\\) is Greedy in the Limit with Infinite Exploration (GLIE) if it satisfies two properties. First, it performs infinite exploration: every state-action pair is visited infinitely often,</p> \\[ \\lim_{k\\to\\infty} N_k(s,a) = \\infty \\quad \\text{for all } (s,a)\\in \\mathcal{S}\\times\\mathcal{A}, \\] <p>where \\(N_k(s,a)\\) counts how many times \\((s,a)\\) has appeared up to episode \\(k\\). Second, it becomes greedy in the limit: as \\(k\\) grows, the policy converges to choosing greedy actions with respect to the learned action-values,</p> \\[ \\lim_{k\\to\\infty}\\pi_k(a\\mid s) = \\mathbf{1}\\!\\left(a \\in \\arg\\max_{a'\\in\\mathcal{A}} Q_k(s,a')\\right). \\] <p>A typical example is decaying \\(\\varepsilon\\)-greedy, where \\(\\varepsilon_k\\to 0\\) (for instance \\(\\varepsilon_k=1/k\\)), provided the induced behaviour still visits states often enough. This directly addresses the issue that a fixed \\(\\varepsilon\\) can be too random forever.</p>"},{"location":"RL-6-Model%20free%20control/#glie-monte-carlo-control-episode-by-episode","title":"GLIE Monte-Carlo Control (episode-by-episode)","text":"<p>Control is not an i.i.d. estimation problem: the data are generated by the current policy, and as the policy changes, the state-action distribution of the samples changes as well. GLIE MC control therefore implements generalised policy iteration: it interleaves (approximate) policy evaluation with policy improvement, updating both from the same evolving stream of experience. Concretely, at episode \\(k\\) we roll out a trajectory under the current policy \\(\\pi_k\\),</p> \\[ \\{S_1, A_1, R_2, \\ldots, S_T\\} \\sim \\pi_k, \\] <p>use the resulting returns to update \\(Q\\) by incremental Monte-Carlo averaging for each visited pair \\((S_t,A_t)\\),</p> \\[ N(S_t,A_t) \\leftarrow N(S_t,A_t) + 1,\\qquad Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\frac{1}{N(S_t,A_t)}\\Bigl(G_t - Q(S_t,A_t)\\Bigr), \\] <p>and then improve the policy by making it \\(\\varepsilon_k\\)-greedy with respect to the updated \\(Q\\) while simultaneously reducing exploration according to a GLIE schedule (e.g. \\(\\varepsilon_k=1/k\\)):</p> \\[ \\varepsilon_k \\leftarrow \\frac{1}{k}, \\qquad \\pi_{k+1} \\leftarrow \\varepsilon_k\\text{-greedy}(Q). \\] <p>The decay \\(\\varepsilon_k\\to 0\\) makes the policy greedy in the limit, while keeping \\(\\varepsilon_k&gt;0\\) for all finite \\(k\\) ensures continued exploration. Under standard assumptions for episodic finite MDPs and a GLIE policy sequence, tabular GLIE Monte-Carlo control converges:</p> \\[ Q_k(s,a) \\to q_*(s,a) \\quad \\text{as } k\\to\\infty. \\] <p>Proof sketch. Fix a state-action pair \\((s,a)\\). Under a GLIE policy sequence, every pair is visited infinitely often while the policy becomes greedy in the limit: \\(\\varepsilon_k\\to 0\\) and \\(N_k(s,a)\\to\\infty\\). For first-visit (or every-visit) Monte-Carlo evaluation, the update</p> \\[ Q_{n+1}(s,a) \\;=\\; Q_n(s,a) + \\frac{1}{n}\\bigl(G^{(n)}(s,a)-Q_n(s,a)\\bigr) \\] <p>is the sample-average estimator of \\(\\mathbb{E}_\\pi[G\\mid S=s,A=a]=q_\\pi(s,a)\\), hence by the law of large numbers,</p> \\[ Q_n(s,a)\\to q_{\\pi}(s,a)\\qquad \\text{for fixed }\\pi. \\] <p>In control, \\(\\pi_k\\) changes with \\(k\\), but GLIE interleaves these evaluation updates with \\(\\varepsilon_k\\)-greedy improvement. The improvement step is greedy in the limit, so the sequence of policies approaches a greedy policy w.r.t. the limiting action-values. Combining (i) infinite exploration (so each \\((s,a)\\) is learned) with (ii) greedy-in-the-limit improvement (so suboptimal actions are eventually rejected) yields convergence of the action-value estimates to optimality:</p> \\[ Q_k(s,a)\\to q_*(s,a)\\quad \\text{as }k\\to\\infty. \\] <p>A full proof formalises this using stochastic approximation arguments and the GLIE conditions (see standard RL texts).</p> <p>Additional comments. In tabular GLIE theory, if the GLIE conditions hold (infinite exploration and greedy in the limit), then the choice of initial values does not affect the limit: the estimates still converge to \\(q_*\\). What initialisation does affect is the transient behaviour, how quickly \\(Q_k\\) becomes accurate and how quickly the induced policy becomes good.</p> <p>In practice, initialisation can noticeably change learning speed. With optimistic initialisation (starting with large \\(Q\\) values), actions look promising until they are tried, which can encourage exploration and sometimes allows a smaller \\(\\varepsilon\\) in simple tasks. With pessimistic (or zero) initialisation, many actions may look similarly unappealing, and if exploration is weak the agent may fail to collect enough diverse experience early on. In larger problems, and especially with function approximation, this can interact with limited state-action coverage and further slow learning. Finally, at optimality the limiting action-value function satisfies the Bellman optimality equation:</p> \\[ q_*(s,a) = \\mathbb{E}\\!\\left[ R_{t+1} + \\gamma \\max_{a'\\in\\mathcal{A}} q_*(S_{t+1}, a') \\mid S_t=s, A_t=a \\right]. \\] <p>GLIE MC control targets \\(q_*\\) via sampled returns and \\(\\varepsilon_k\\)-greedy improvement; the GLIE conditions are what ensure we keep visiting enough \\((s,a)\\) pairs to reliably identify the maximising actions.</p>"},{"location":"RL-6-Model%20free%20control/#from-mc-control-to-td-control-sarsa","title":"From MC Control to TD Control: SARSA","text":"<p>Monte-Carlo (MC) control updates \\(Q\\) using full returns, so it must wait until an episode ends and its targets can be noisy for long horizons. Temporal-Difference (TD) control changes the target: it bootstraps from the current value estimate instead of using the complete return. This has a few practical benefits. First, TD targets typically have lower variance than \\(G_t\\) because they depend on one reward plus an estimate, not an entire future sum. Second, TD can update online, step-by-step inside the episode. Third, TD can learn from truncated sequences and continuing tasks since it does not require a terminal state to define an update. The natural control recipe is the same as before: keep \\(\\varepsilon\\)-greedy improvement, but replace MC evaluation with TD updates of \\(Q(S,A)\\) at every time-step.</p>"},{"location":"RL-6-Model%20free%20control/#td-evaluation-on-action-values-the-sarsa-update","title":"TD evaluation on action-values: the SARSA update","text":"<p>SARSA is an on-policy TD control method. Its name comes from the quintuple</p> \\[ (S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}). \\] <p>After taking \\(A_t\\) in \\(S_t\\) and observing \\(R_{t+1}\\) and \\(S_{t+1}\\), we also sample the next action \\(A_{t+1}\\) from the current behaviour policy (typically \\(\\varepsilon\\)-greedy). With \\(S_t=s\\), \\(A_t=a\\), \\(R_{t+1}=r\\), \\(S_{t+1}=s'\\), and \\(A_{t+1}=a'\\), the SARSA update is</p> \\[ Q(s,a)\\;\\leftarrow\\; Q(s,a) + \\alpha\\Bigl(\\underbrace{r + \\gamma Q(s',a')}_{\\text{TD target}} - Q(s,a)\\Bigr). \\] <p>Equivalently, define the TD-error</p> \\[ \\delta_t \\;=\\; R_{t+1} + \\gamma Q(S_{t+1},A_{t+1}) - Q(S_t,A_t), \\] <p>and update</p> \\[ Q(S_t,A_t)\\leftarrow Q(S_t,A_t) + \\alpha\\,\\delta_t. \\]"},{"location":"RL-6-Model%20free%20control/#why-this-td-target-makes-sense","title":"Why this TD target makes sense","text":"<p>Fix a policy \\(\\pi\\). Its action-value function \\(q_\\pi\\) satisfies the Bellman expectation equation</p> \\[ q_\\pi(s,a) = \\mathbb{E}_\\pi\\!\\left[\\,R_{t+1} + \\gamma q_\\pi(S_{t+1},A_{t+1}) \\;\\middle|\\; S_t=s,\\;A_t=a\\right], \\] <p>where \\(A_{t+1}\\sim \\pi(\\cdot\\mid S_{t+1})\\). If \\(q_\\pi\\) were known, then along a trajectory generated by \\(\\pi\\) the random variable</p> \\[ Y_{t} \\;=\\; R_{t+1} + \\gamma q_\\pi(S_{t+1},A_{t+1}) \\] <p>has conditional expectation</p> \\[ \\mathbb{E}_\\pi\\!\\left[\\,Y_t \\mid S_t=s,\\;A_t=a\\right] \\;=\\; q_\\pi(s,a), \\] <p>so \\(Y_t\\) is a one-step, unbiased sample target for \\(q_\\pi(s,a)\\). A natural way to estimate \\(q_\\pi\\) is therefore to repeatedly move our estimate toward this target using stochastic approximation. SARSA follows exactly this logic, but replaces the unknown \\(q_\\pi\\) with the current estimate \\(Q\\):</p> \\[ \\widehat{Y}_t \\;=\\; R_{t+1} + \\gamma Q(S_{t+1},A_{t+1}). \\] <p>This is the TD target. With stepsize \\(\\alpha\\), the SARSA update becomes</p> \\[ Q(S_t,A_t)\\leftarrow Q(S_t,A_t) + \\alpha\\bigl(\\widehat{Y}_t - Q(S_t,A_t)\\bigr), \\] <p>where the TD error</p> \\[ \\delta_t \\;=\\; R_{t+1} + \\gamma Q(S_{t+1},A_{t+1}) - Q(S_t,A_t) \\] <p>measures the current violation of Bellman consistency. When \\(Q\\) matches \\(q_\\pi\\), the expected TD error is zero, so on average the update stops pushing the estimate. In this sense, SARSA updates \\(Q\\) to make it increasingly consistent with the Bellman expectation equation for the policy being followed.</p> <p>Practical Notes. Compared to MC, the TD target typically has lower variance, but it introduces bias because it bootstraps (it uses \\(Q\\) inside the target), in practice this tradeoff is often favourable, especially for long episodes or when we want to learn online. In SARSA control, we continually derive behaviour from the current action-values, most often by acting \\(\\varepsilon\\)-greedily with respect to \\(Q\\),</p> \\[ \\pi(\\cdot\\mid s) \\leftarrow \\varepsilon\\text{-greedy}(Q(\\,s,\\cdot\\,)), \\] <p>because \\(Q\\) is updated at every time-step, the implied policy effectively changes at every time-step as well. This can be understood either through a policy iteration / GPI lens as TD updates act as approximate evaluation toward \\(q_\\pi\\) while \\(\\varepsilon\\)-greedy action selection supplies the improvement step, with the two interleaved continuously, or through an online control lens where behaviour is always generated by the current \\(\\varepsilon\\)-greedy policy implied by \\(Q\\), so revisiting a state can lead to different actions simply because \\(Q\\) has changed since last time. The fact that behaviour changes while learning does matter: the data distribution is non-stationary because the policy evolves, however, SARSA is on-policy and therefore updates \\(Q\\) toward the value of the same policy that generates the data,</p> \\[ \\text{data from } \\pi \\quad \\Longrightarrow \\quad \\text{update toward } q_\\pi, \\] <p>so as \\(\\pi\\) improves and the target \\(q_\\pi\\) moves, SARSA keeps tracking it via continual small updates. Moreover, because TD methods bootstrap, they can immediately use the freshest estimates inside the target, which often speeds up learning relative to MC (which must wait for episode termination). In long-horizon tasks, repeated bootstrapped updates propagate information backward through the value function sooner. Finally, TD ideas extend naturally to off-policy control (e.g. Q-learning), where the target can use a greedy action even while behaviour remains exploratory, which is a major reason TD control sits at the core of many practical RL algorithms.</p>"},{"location":"RL-6-Model%20free%20control/#sarsa-algorithm","title":"SARSA algorithm","text":"<p>Initialize \\(Q(s,a)\\) arbitrarily for all \\(s\\in\\mathcal{S}, a\\in\\mathcal{A}(s)\\) and set \\(Q(\\text{terminal},\\cdot)=0\\).</p> <p>For each episode:</p> <ol> <li>Initialize \\(S\\).</li> <li>Choose \\(A\\) from \\(S\\) using a policy derived from \\(Q\\) (e.g. \\(\\varepsilon\\)-greedy).</li> <li>Repeat (for each step of the episode):<ol> <li>Take action \\(A\\), observe \\(R\\) and next state \\(S'\\).</li> <li>Choose \\(A'\\) from \\(S'\\) using a policy derived from \\(Q\\) (e.g. \\(\\varepsilon\\)-greedy).</li> <li>Update: \\(Q(S,A) \\leftarrow Q(S,A) + \\alpha\\bigl[R + \\gamma Q(S',A') - Q(S,A)\\bigr]\\).</li> <li>\\(S \\leftarrow S'\\), \\(A \\leftarrow A'\\) until \\(S\\) is terminal.</li> </ol> </li> </ol>"},{"location":"RL-6-Model%20free%20control/#convergence-of-sarsa-tabular-setting","title":"Convergence of SARSA (tabular setting)","text":"<p>In the idealised tabular case (finite state-action spaces), SARSA is guaranteed to converge to the optimal action-value function provided (i) we explore forever but become greedy in the limit, and (ii) the step-sizes shrink in a controlled way. Concretely, SARSA converges in the sense that</p> \\[ Q(s,a)\\to q_*(s,a), \\] <p>under the following conditions:</p> <ul> <li>a GLIE sequence of behaviour policies \\(\\{\\pi_t(\\cdot\\mid s)\\}\\) (infinite exploration, greedy in the limit),</li> <li>a Robbins-Monro step-size sequence \\(\\{\\alpha_t\\}\\) satisfying</li> </ul> \\[ \\sum_{t=1}^{\\infty}\\alpha_t = \\infty, \\qquad \\sum_{t=1}^{\\infty}\\alpha_t^2 &lt; \\infty. \\] <p>The Robbins-Monro conditions encode a practical balance between not giving up too early and eventual stability. The requirement \\(\\sum_t \\alpha_t=\\infty\\) means there is enough total \"learning rate\" to keep making progress (the algorithm does not effectively freeze), while \\(\\sum_t \\alpha_t^2&lt;\\infty\\) ensures the steps become small enough that sampling noise averages out and the iterates settle. A standard schedule is \\(\\alpha_t=1/t\\) (more generally \\(\\alpha_t=1/t^\\beta\\) with \\(\\tfrac12&lt;\\beta\\le 1\\)).</p> <p>In real implementations it is common to use a small constant step-size (or a piecewise schedule) to learn faster and to track non-stationarity. This can violate Robbins-Monro, so the strict convergence guarantee no longer applies, but empirically it often works well, the theorem is mainly a clean guarantee for the stationary, tabular regime. These same ingredients also explain a common learning curve pattern in episodic tasks: learning can look extremely slow at first, then suddenly speed up. Early on, \\(Q\\) is uninformative, so an \\(\\varepsilon\\)-greedy policy behaves close to random and may wander for a long time before reaching termination. As SARSA updates \\(Q\\) online, useful actions start receiving higher values, so the same \\(\\varepsilon\\)-greedy rule increasingly prefers them and trajectories shorten dramatically. For example, in a sparse-reward navigation task where reward appears only at a goal state, the first episode might take a lot of steps to stumble into the goal but afterwards, TD bootstrapping propagates positive value back through earlier state-action choices, and later episodes can drop to a significantly lower number of steps even though exploration still occurs with probability \\(\\varepsilon\\).</p> <p>Overall GLIE ensures every \\((s,a)\\) keeps being tried while behaviour becomes greedy in the limit, and Robbins-Monro step-sizes ensure updates keep moving but eventually stabilise, together they yield the tabular convergence of SARSA.</p>"},{"location":"RL-6-Model%20free%20control/#n-step-sarsa-and-sarsalambda-forward-view-vs-backward-view","title":"\\(n\\)-Step SARSA and SARSA(\\(\\lambda\\)): Forward View vs Backward View","text":"<p>A useful way to understand the MC-TD connection is to compare how quickly we bootstrap. Plain SARSA bootstraps after one step: it replaces the unknown future return by the current estimate \\(Q\\) immediately, which typically reduces variance but introduces bias. Monte-Carlo (MC) on the other hand, waits until the end of the episode to form the full return, which is unbiased for \\(q_\\pi\\) (given a fixed policy) but can have high variance and delayed learning. The natural middle ground is to bootstrap after \\(n\\) steps. This yields a family of methods indexed by an integer \\(n\\in\\{1,2,\\dots\\}\\), with \\(n=1\\) recovering SARSA and \\(n=\\infty\\) corresponding to MC. Ideally, however, we would like a mechanism that blends these horizons rather than forcing us to pick a single \"correct\" \\(n\\).</p>"},{"location":"RL-6-Model%20free%20control/#n-step-returns-for-action-values","title":"\\(n\\)-step returns for action-values","text":"<p>Consider a trajectory (episodic case) and fix a time index \\(t\\). The core object is a target for \\(Q(S_t,A_t)\\) that uses the next \\(n\\) rewards explicitly and then bootstraps from \\(Q\\) at time \\(t+n\\). Concretely, the first few \\(n\\)-step action-value targets are</p> \\[ \\begin{aligned} n=1 \\;(\\text{SARSA}):\\quad &amp; q_t^{(1)} = R_{t+1} + \\gamma Q(S_{t+1},A_{t+1}),\\\\ n=2:\\quad &amp; q_t^{(2)} = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 Q(S_{t+2},A_{t+2}),\\\\ n=3:\\quad &amp; q_t^{(3)} = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 Q(S_{t+3},A_{t+3}),\\\\ &amp;\\vdots\\\\ n=\\infty \\;(\\text{MC}):\\quad &amp; q_t^{(\\infty)} = R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{T-t-1}R_T. \\end{aligned} \\] <p>More compactly, for a finite \\(n\\) we define the \\(n\\)-step Q-return</p> \\[ q_t^{(n)} \\;=\\; \\sum_{i=1}^{n}\\gamma^{i-1}R_{t+i} \\;+\\; \\gamma^{n}Q(S_{t+n},A_{t+n}). \\] <p>When the episode terminates before \\(t+n\\), we simply stop at \\(T\\) and there is no bootstrap term; equivalently, the bootstrap term is \\(0\\) after termination. Given this target, \\(n\\)-step SARSA applies the usual incremental update:</p> \\[ Q(S_t,A_t)\\leftarrow Q(S_t,A_t)+\\alpha\\Bigl(q_t^{(n)}-Q(S_t,A_t)\\Bigr). \\] <p>Intuitively, \\(n\\) controls the bias-variance tradeoff: increasing \\(n\\) replaces more of the bootstrapped tail by actual sampled rewards (less bias, more variance and delay), while decreasing \\(n\\) bootstraps sooner (more bias, typically lower variance and faster propagation).</p>"},{"location":"RL-6-Model%20free%20control/#forward-view-sarsalambda-averaging-over-all-horizons","title":"Forward-view SARSA(\\(\\lambda\\)): averaging over all horizons","text":"<p>Picking \\(n\\) can be tricky: the best horizon depends on the problem, and performance can be sensitive to it. SARSA(\\(\\lambda\\)) addresses this by forming a single target that is a weighted average of all \\(n\\)-step targets. We define the \\(\\lambda\\)-return (forward view) by</p> \\[ q_t^\\lambda = (1-\\lambda)\\sum_{n=1}^{\\infty}\\lambda^{n-1}\\,q_t^{(n)}, \\qquad \\lambda\\in[0,1]. \\] <p>The coefficients \\((1-\\lambda)\\lambda^{n-1}\\) form a geometric distribution over \\(n\\) because</p> \\[ \\sum_{n=1}^{\\infty}(1-\\lambda)\\lambda^{n-1}=1, \\] <p>so \\(q_t^\\lambda\\) is literally a convex combination of the \\(n\\)-step returns. The parameter \\(\\lambda\\) controls how much weight is placed on longer horizons:</p> <ul> <li>\\(\\lambda=0\\) puts all weight on \\(n=1\\), so \\(q_t^\\lambda=q_t^{(1)}\\) and we recover one-step SARSA.</li> <li>As \\(\\lambda\\to 1\\), the weights shift toward large \\(n\\), making the target increasingly MC-like.</li> </ul> <p>Using \\(q_t^\\lambda\\) as the target gives the forward-view update rule:</p> \\[ Q(S_t,A_t)\\leftarrow Q(S_t,A_t)+\\alpha\\Bigl(q_t^\\lambda-Q(S_t,A_t)\\Bigr). \\] <p>This view is conceptually clean, it says \"update toward a particular weighted return\", but it is not the most computationally convenient online implementation because it require access to many future rewards to form the sum. In particular, To compute \\(q_t^{(n)}\\) we need information up to time \\(t+n\\), and to evaluate the infinite weighted sum we need the whole future trajectory (up to termination). The backward view (eligibility traces) provides an efficient online mechanism that is equivalent to this forward view under standard conditions, which we discuss next.</p>"},{"location":"RL-6-Model%20free%20control/#backward-view-sarsalambda-eligibility-traces-for-online-learning","title":"Backward-view SARSA(\\(\\lambda\\)): eligibility traces for online learning","text":"<p>The forward view defines an appealing multi-step target, but it is inconvenient online because it depends on future rewards. The backward view achieves the same effect incrementally by maintaining eligibility traces: a table \\(E(s,a)\\) that tracks how much \"credit\" each previously visited pair should receive from the current TD error. This has been discussed in great detail in the previous chapter on Model free prediction.</p> <p>Eligibility traces (accumulating). Initialize traces to zero:</p> \\[ E_0(s,a)=0 \\qquad \\text{for all }(s,a). \\] <p>At each time-step \\(t\\), traces decay and the currently visited pair is reinforced:</p> \\[ E_t(s,a)\\;=\\;\\gamma\\lambda\\,E_{t-1}(s,a)\\;+\\;\\mathbf{1}(S_t=s,\\;A_t=a). \\] <p>Thus all past traces shrink by the factor \\(\\gamma\\lambda\\), while \\((S_t,A_t)\\) receives an additional \\(+1\\). Recent (and repeatedly visited) pairs therefore have larger traces than older pairs.</p> <p>TD error (same as one-step SARSA).</p> \\[ \\delta_t \\;=\\; R_{t+1} + \\gamma Q(S_{t+1},A_{t+1}) - Q(S_t,A_t). \\] <p>Backward-view update (credit assignment). Instead of assigning \\(\\delta_t\\) only to the current pair, SARSA(\\(\\lambda\\)) distributes it across all pairs proportionally to their eligibility:</p> \\[ Q(s,a)\\;\\leftarrow\\; Q(s,a) + \\alpha\\,\\delta_t\\,E_t(s,a) \\qquad \\text{for all }(s,a). \\] <p>Intuitively, the TD error computed now is pushed backward through the trace, so recently visited pairs are updated strongly, and older pairs receive little or no update as their traces decay.</p> <p>How traces remove the need to \"look forward\". The forward view updates \\(Q(S_t,A_t)\\) using a weighted average of future \\(n\\)-step returns. The backward view flips the computation: it stays fully online, forms a one-step TD error at time \\(t\\), and uses \\(E_t\\) to send that information backward to the relevant recent history. The parameter \\(\\lambda\\) controls how far this backward credit assignment reaches: \\(\\lambda=0\\) makes traces vanish immediately (recovering one-step SARSA), while larger \\(\\lambda\\) keeps traces alive longer, producing more multi-step, MC-like behaviour.</p> <p>SARSA vs. SARSA(\\(\\lambda\\)) in practice.</p> <p>SARSA propagates learning one step at a time, so delayed rewards can take many updates to affect earlier decisions. SARSA(\\(\\lambda\\)) uses eligibility traces to push each TD error backward over multiple recent steps (with decaying strength), which often speeds up learning on long-horizon or delayed-reward tasks. SARSA(\\(\\lambda\\)) tends to help most with delayed/sparse rewards and episodic tasks where whole action sequences should share credit, while plain SARSA is often preferable for maximum simplicity, very large spaces where traces are expensive, or very noisy environments where longer-horizon credit assignment inflates variance.</p>"},{"location":"RL-6-Model%20free%20control/#off-policy-learning-and-importance-sampling","title":"Off-Policy Learning and Importance Sampling","text":""},{"location":"RL-6-Model%20free%20control/#on-policy-vs-off-policy-what-changed-and-why-it-matters","title":"On-policy vs. off-policy: what changed, and why it matters","text":"<p>So far we have mostly worked for on-policy methods, where the same policy both generates the data and defines the quantity we evaluate or improve: if we follow \\(\\pi\\), the trajectory distribution is induced by \\(\\pi\\), and estimates like \\(v_\\pi\\) or \\(q_\\pi\\) are built from samples generated under that same \\(\\pi\\). In off-policy learning these roles split: the behaviour policy \\(\\mu(a\\mid s)\\) is what we actually run to collect experience, while the target policy \\(\\pi(a\\mid s)\\) is what we want to evaluate (or improve toward). Concretely, we observe episodes under \\(\\mu\\),</p> \\[ (S_1,A_1,R_2,\\ldots,S_T)\\sim \\mu, \\] <p>but our goal remains to estimate quantities for \\(\\pi\\), such as \\(v_\\pi(s)\\) or \\(q_\\pi(s,a)\\). The motivation is largely data efficiency: interaction is expensive, so off-policy learning lets us learn from demonstrations or another agent (data from \\(\\mu\\), questions about \\(\\pi\\)), reuse trajectories collected under earlier policies instead of discarding them, behave exploratorily for coverage while learning a greedy (or near-greedy) target policy, and even evaluate multiple target policies from a single dataset. But there is a key technical issue with this approach called distribution mismatch discussed below.</p> <p>Distribution mismatch. In off-policy learning we collect experience under a behaviour policy \\(\\mu\\), but we want to evaluate a different target policy \\(\\pi\\). The problem is that value functions are expectations under the target policy's trajectory distribution. For instance,</p> \\[ v_\\pi(s)=\\mathbb{E}_\\pi\\!\\left[G_t \\mid S_t=s\\right], \\] <p>and the subscript \\(\\pi\\) matters: it means the entire future trajectory (actions, next states, rewards, and hence the return \\(G_t\\)) is generated by following \\(\\pi\\). If instead we generate episodes under \\(\\mu\\), then even from the same starting state \\(s\\) we generally obtain a different distribution over trajectories, so the distribution of returns changes as well. Therefore, if we simply average returns observed under \\(\\mu\\) while pretending they came from \\(\\pi\\), we converge to the wrong quantity:</p> \\[ \\mathbb{E}_\\mu[G_t\\mid S_t=s]\\neq \\mathbb{E}_\\pi[G_t\\mid S_t=s]\\quad\\text{in general.} \\] <p>Intuitively, trajectories that are frequent under \\(\\mu\\) but rare under \\(\\pi\\) will be over-represented in the data, and trajectories that \\(\\pi\\) would often generate but \\(\\mu\\) rarely produces will be under-represented. This is a bias issue, not merely a high-variance or small-sample issue: with infinitely many samples from \\(\\mu\\) we would still estimate the expectation under \\(\\mu\\), not under \\(\\pi\\). A correction mechanism is needed to reweight samples so that, in aggregate, they reflect how likely each trajectory would be under \\(\\pi\\) rather than under \\(\\mu\\). This task is achieved by importance sampling.</p>"},{"location":"RL-6-Model%20free%20control/#importance-sampling-correcting-distribution-mismatch","title":"Importance sampling: correcting distribution mismatch","text":"<p>Off-policy evaluation fails if we treat data from \\(\\mu\\) as if it came from \\(\\pi\\), because \\(\\mu\\) and \\(\\pi\\) induce different distributions over outcomes. Importance sampling is the standard correction: it rewrites an expectation under one distribution using samples from another by reweighting each sample by a likelihood ratio. For a random variable \\(X\\) and function \\(f\\),</p> \\[ \\mathbb{E}_{X\\sim P}[f(X)] = \\sum_x P(x)f(x) = \\sum_x Q(x)\\frac{P(x)}{Q(x)}f(x) = \\mathbb{E}_{X\\sim Q}\\!\\left[\\frac{P(X)}{Q(X)}f(X)\\right]. \\] <p>In off-policy RL, \\(P\\) is the trajectory distribution induced by the target policy \\(\\pi\\), and \\(Q\\) is the trajectory distribution induced by the behaviour policy \\(\\mu\\): the ratio \\(\\frac{P(\\tau)}{Q(\\tau)}\\) tells us how much a sampled trajectory \\(\\tau\\) should count if our goal is to estimate an expectation under \\(\\pi\\) while sampling under \\(\\mu\\).</p> \\[ \\mathbb{E}_{\\tau\\sim \\pi}\\bigl[f(\\tau)\\bigr] = \\mathbb{E}_{\\tau\\sim \\mu}\\!\\left[\\frac{P_\\pi(\\tau)}{P_\\mu(\\tau)}\\,f(\\tau)\\right], \\] <p>Coverage (absolute continuity). This correction is only defined if \\(\\mu\\) assigns positive probability wherever \\(\\pi\\) might act. If for some \\((s,a)\\) we have</p> \\[ \\mu(a\\mid s)=0 \\ \\text{and}\\ \\pi(a\\mid s)&gt;0, \\] <p>then \\(\\pi(a\\mid s)/\\mu(a\\mid s)\\) is undefined, and trajectories that \\(\\pi\\) can generate are simply unobservable under \\(\\mu\\). Informally: the behaviour policy must be willing to try any action that the target policy might take.</p>"},{"location":"RL-6-Model%20free%20control/#off-policy-monte-carlo-via-importance-sampling","title":"Off-policy Monte-Carlo via importance sampling","text":"<p>In Monte-Carlo evaluation the return \\(G_t\\) depends on the entire future trajectory, so if data are generated by a behaviour policy \\(\\mu\\) but we want values for a target policy \\(\\pi\\), we must correct the distribution mismatch by reweighting the whole action sequence after time \\(t\\). For an episodic trajectory terminating at \\(T\\), define the (per-decision) importance ratio</p> \\[ \\rho_{t:T-1} = \\prod_{k=t}^{T-1}\\frac{\\pi(A_k\\mid S_k)}{\\mu(A_k\\mid S_k)}. \\] <p>This ratio is the likelihood ratio for producing the same sequence of actions under \\(\\pi\\) instead of \\(\\mu\\) (conditioned on the visited states), and it tells us how much the observed trajectory segment should \"count\" when our goal is an expectation under \\(\\pi\\). The corresponding importance-sampled return is</p> \\[ G_t^{\\pi/\\mu}=\\rho_{t:T-1}\\,G_t, \\] <p>so trajectories that are more likely under \\(\\pi\\) than under \\(\\mu\\) are upweighted and those that are less likely are downweighted, with this reweighting, the estimator has the correct expectation for the target policy. A standard incremental update then takes the usual MC form but uses the corrected return,</p> \\[ V(S_t)\\leftarrow V(S_t) + \\alpha\\Bigl(G_t^{\\pi/\\mu} - V(S_t)\\Bigr). \\] <p>The main practical limitation with this approach is variance: \\(\\rho_{t:T-1}\\) is a product of random ratios, so over long horizons even modest mismatch between \\(\\pi\\) and \\(\\mu\\) can cause the weights to explode (rare trajectories get enormous weight) or collapse toward zero (most trajectories contribute almost nothing), yielding high-variance estimates, this is why pure off-policy MC with importance sampling is often unstable in long tasks and motivates variance-reduction variants (e.g. weighted importance sampling) as well as off-policy TD methods.</p>"},{"location":"RL-6-Model%20free%20control/#off-policy-td-single-step-correction","title":"Off-policy TD: single-step correction","text":"<p>Off-policy learning is valuable because it lets us reuse data (old trajectories, demonstrations, exploratory behaviour) while estimating or improving a different target policy \\(\\pi\\). The core difficulty is still distribution mismatch, so we still need importance sampling, but TD methods make this correction much lighter because their targets are local due to bootstrapping. In TD(0), the target for \\(V\\) uses only the immediate reward and the next state's value estimate,</p> \\[ \\text{TD target:}\\quad R_{t+1}+\\gamma V(S_{t+1}), \\] <p>so to correct for sampling under a behaviour policy \\(\\mu\\) we only need to reweight the one action actually taken at time \\(t\\). Concretely, for off-policy TD(0) evaluation of \\(\\pi\\) using transitions generated by \\(\\mu\\), we apply a single-step importance ratio:</p> \\[ V(S_t)\\leftarrow V(S_t)+ \\alpha\\left( \\frac{\\pi(A_t\\mid S_t)}{\\mu(A_t\\mid S_t)} \\Bigl(R_{t+1}+\\gamma V(S_{t+1})-V(S_t)\\Bigr) \\right), \\] <p>i.e. we multiply the TD error by \\(\\pi(A_t\\mid S_t)/\\mu(A_t\\mid S_t)\\) to account for the fact that \\(A_t\\) was chosen according to \\(\\mu\\) rather than \\(\\pi\\). The practical advantage over off-policy Monte-Carlo is variance: MC must correct the probability of an entire future action sequence and therefore uses a product of many ratios, which can easily explode or collapse on long horizons, whereas off-policy TD uses only a single ratio per update. As a result, TD is typically far more stable and data-efficient off-policy, even when \\(\\pi\\) and \\(\\mu\\) are not extremely close. The remaining caveat is that very large per-step ratios \\(\\pi(A_t\\mid S_t)/\\mu(A_t\\mid S_t)\\) can still introduce instability, so in practice one often chooses \\(\\mu\\) to provide good coverage and keeps \\(\\pi\\) reasonably aligned with it.</p>"},{"location":"RL-6-Model%20free%20control/#q-learning-off-policy-control-with-a-greedy-target","title":"Q-Learning: Off-Policy Control with a Greedy Target","text":"<p>Unlike the off-policy methods above (which correct evaluation of a fixed target policy \\(\\pi\\) using importance sampling), Q-learning builds the target directly from a greedy backup \\(\\max_{a'}Q(S_{t+1},a')\\), so it can learn about an optimal greedy policy even while behaving according to a separate exploratory policy \\(\\mu\\).</p> <p>Q-learning is a canonical off-policy TD control algorithm. We interact with the environment using a behaviour policy \\(\\mu\\) (typically exploratory, e.g. \\(\\varepsilon\\)-greedy) in order to visit many state-action pairs, but we learn action-values for a different target policy: the greedy policy implied by the current action-value estimates. Equivalently, Q-learning aims directly at the optimal action-value function \\(q_\\star\\) while allowing behaviour to remain exploratory.</p>"},{"location":"RL-6-Model%20free%20control/#greedy-target-and-why-the-update-does-not-use-importance-sampling","title":"Greedy target and why the update does not use importance sampling","text":"<p>At time \\(t\\) we choose the executed action from the behaviour policy,</p> \\[ A_t \\sim \\mu(\\cdot\\mid S_t), \\] <p>and we observe a transition \\((S_t,A_t,R_{t+1},S_{t+1})\\). The off-policy aspect is that the update does not try to evaluate the value of the next behaviour action \\(A_{t+1}\\). Instead, it asks a counterfactual question about the next state: if I were greedy at \\(S_{t+1}\\), what value would I obtain? This is implemented by taking a maximum over actions at the next state,</p> \\[ \\max_{a'} Q(S_{t+1},a') \\;=\\; Q\\!\\Bigl(S_{t+1}, \\arg\\max_{a'} Q(S_{t+1},a')\\Bigr), \\] <p>which corresponds to the greedy target policy</p> \\[ \\pi(\\cdot\\mid s)\\in \\arg\\max_{a} Q(s,a). \\] <p>Because the target is computed by an explicit \\(\\max\\) rather than by sampling \\(A_{t+1}\\) from \\(\\pi\\), Q-learning does not need to reweight trajectories to make them look like they were generated by \\(\\pi\\). The update uses behaviour samples only to obtain a transition and reward, then performs a greedy (optimality-style) backup at the next state.</p>"},{"location":"RL-6-Model%20free%20control/#update-rule-optimality-style-td-target","title":"Update rule (optimality-style TD target)","text":"<p>The Q-learning update is</p> \\[ Q(S_t,A_t)\\leftarrow Q(S_t,A_t) +\\alpha\\Bigl(\\underbrace{R_{t+1}+\\gamma\\max_{a'}Q(S_{t+1},a')}_{\\text{TD target}} - Q(S_t,A_t)\\Bigr). \\] <p>The associated TD error is</p> \\[ \\delta_t^{\\text{QL}} \\;=; R_{t+1}+\\gamma\\max_{a'}Q(S_{t+1},a')-Q(S_t,A_t). \\] <p>So each observed transition nudges \\(Q(S_t,A_t)\\) toward \"reward plus discounted value of the best next action.\" Previously, in off-policy TD(0) evaluation, we still bootstrapped but we were trying to match a fixed target policy \\(\\pi\\), so we corrected the TD error using the one-step importance ratio \\(\\pi(A_t\\mid S_t)/\\mu(A_t\\mid S_t)\\). Q-learning instead targets optimality by replacing the next-step value under a policy with a greedy \\(\\max\\) backup, and therefore does not use an explicit importance-sampling correction.</p>"},{"location":"RL-6-Model%20free%20control/#connection-to-the-bellman-optimality-equation","title":"Connection to the Bellman optimality equation","text":"<p>Q-learning is designed to solve the Bellman optimality equation. The optimal action-value function \\(q_\\star\\) is characterised by</p> \\[ q_\\star(s,a) = \\mathbb{E}\\!\\left[ R_{t+1}+\\gamma\\max_{a'}q_\\star(S_{t+1},a') \\;\\middle|\\; S_t=s,\\;A_t=a \\right]. \\] <p>It is convenient to define the right-hand side as the optimal Bellman operator</p> \\[ (T_\\star Q)(s,a)=\\mathbb{E}\\!\\left[ R_{t+1}+\\gamma\\max_{a'}Q(S_{t+1},a') \\mid S_t=s,A_t=a\\right]. \\] <p>Then \\(q_\\star\\) is exactly the fixed point of this operator:</p> \\[ q_\\star = T_\\star q_\\star. \\] <p>The Q-learning update</p> \\[ Q(S_t,A_t)\\leftarrow Q(S_t,A_t) +\\alpha\\Bigl(R_{t+1}+\\gamma\\max_{a'}Q(S_{t+1},a')-Q(S_t,A_t)\\Bigr) \\] <p>is an incremental (sample-based) attempt to enforce this fixed-point condition: it replaces the expectation in \\(T_\\star\\) by the single observed transition and moves \\(Q(S_t,A_t)\\) toward the corresponding target. Under the usual tabular assumptions (every \\((s,a)\\) is updated infinitely often and \\(\\{\\alpha_t\\}\\) satisfies Robbins-Monro), these stochastic updates converge to the unique fixed point, so</p> \\[ Q_t(s,a)\\to q_\\star(s,a). \\] <p>As a result, acting greedily with respect to \\(Q_t\\) becomes optimal in the limit:</p> \\[ \\pi_t(s)\\in\\arg\\max_a Q_t(s,a). \\]"},{"location":"RL-6-Model%20free%20control/#algorithm-sketch","title":"Algorithm sketch","text":"<ol> <li>Initialize \\(Q(s,a)\\) arbitrarily (often \\(0\\)) for all state-action pairs.</li> <li>For each episode:</li> <li>Start in an initial state \\(S\\).</li> <li>Repeat until terminal:<ol> <li>Choose \\(A \\sim \\mu(\\cdot\\mid S)\\) (e.g., \\(\\varepsilon\\)-greedy w.r.t. \\(Q\\)).</li> <li>Execute \\(A\\), observe reward \\(R\\) and next state \\(S'\\).</li> <li>Update: \\(Q(S,A) \\leftarrow Q(S,A) + \\alpha\\bigl(R + \\gamma \\max_{a'} Q(S',a') - Q(S,A)\\bigr)\\).</li> <li>Set \\(S \\leftarrow S'\\).</li> </ol> </li> </ol>"},{"location":"RL-6-Model%20free%20control/#relationship-between-dp-and-td-learning","title":"Relationship Between DP and TD Learning","text":"<p>DP and TD are built on the same Bellman equations, what differs is how they implement the Bellman backup. DP performs a full backup: it explicitly takes an expectation over next states and rewards using the environment dynamics (so it needs a model). TD performs a sample backup: it replaces that expectation by a single observed transition \\((s,a,r,s')\\), which is why it is model-free. The correspondence is easiest to see by lining up the Bellman equation, the DP update, and the TD update.</p>"},{"location":"RL-6-Model%20free%20control/#state-value-prediction-v_pi","title":"State-value prediction (\\(v_\\pi\\))","text":"<p>The Bellman expectation equation for a fixed policy \\(\\pi\\) is</p> \\[ v_\\pi(s)=\\mathbb{E}\\!\\left[R_{t+1}+\\gamma v_\\pi(S_{t+1})\\mid S_t=s\\right]. \\] <p>A DP-style evaluation step updates by taking the full conditional expectation:</p> \\[ V(s)\\leftarrow \\mathbb{E}\\!\\left[R+\\gamma V(S')\\mid S=s\\right]. \\] <p>TD(0) keeps the same target structure but uses one sample transition \\((S_t=s,R_{t+1}=R,S_{t+1}=s')\\):</p> \\[ V(s)\\xleftarrow{\\alpha} R+\\gamma V(s'), \\qquad \\text{where }x\\xleftarrow{\\alpha}y \\equiv x \\leftarrow x+\\alpha(y-x). \\]"},{"location":"RL-6-Model%20free%20control/#action-value-prediction-on-policy-control-q_pi-and-sarsa","title":"Action-value prediction / on-policy control (\\(q_\\pi\\) and SARSA)","text":"<p>For action-values under a fixed policy,</p> \\[ q_\\pi(s,a)=\\mathbb{E}\\!\\left[R_{t+1}+\\gamma q_\\pi(S_{t+1},A_{t+1})\\mid S_t=s,A_t=a\\right], \\qquad A_{t+1}\\sim \\pi(\\cdot\\mid S_{t+1}). \\] <p>DP would again take a full backup (typically inside policy iteration: evaluate \\(q_\\pi\\), then improve \\(\\pi\\)). The TD analogue replaces the expectation with a single sample next state and next action \\((s',a')\\), yielding the SARSA update:</p> \\[ Q(s,a)\\xleftarrow{\\alpha} R+\\gamma Q(s',a'). \\] <p>This is on-policy in the sense that the same policy that generates behaviour also determines the next action \\(a'\\) appearing inside the target.</p>"},{"location":"RL-6-Model%20free%20control/#optimal-control-q_star-and-q-learning","title":"Optimal control (\\(q_\\star\\) and Q-learning)","text":"<p>For optimal control, the relevant Bellman equation is the optimality equation</p> \\[ q_\\star(s,a)=\\mathbb{E}\\!\\left[R_{t+1}+\\gamma\\max_{a'}q_\\star(S_{t+1},a')\\mid S_t=s,A_t=a\\right]. \\] <p>DP implements the corresponding full optimality backup (value iteration / Q-iteration):</p> \\[ Q(s,a)\\leftarrow \\mathbb{E}\\!\\left[R+\\gamma\\max_{a'}Q(S',a')\\mid S=s,A=a\\right]. \\] <p>The TD analogue is Q-learning, which replaces the expectation by a single observed transition:</p> \\[ Q(s,a)\\xleftarrow{\\alpha} R+\\gamma\\max_{a'}Q(s',a'). \\] <p>Here the target is greedy (via the max backup), while the behaviour policy used to collect data can remain exploratory.</p>"},{"location":"RL-6-Model%20free%20control/#references","title":"References","text":"<ul> <li>https://github.com/zyxue/youtube_RL_course_by_David_Silver</li> </ul>"},{"location":"RL-7-Value%20function%20approximation/","title":"7 - Value Function Approximation","text":""},{"location":"RL-7-Value%20function%20approximation/#motivation","title":"Motivation","text":"<p>Reinforcement learning is often used in settings where the state space is enormous (or even continuous). For instance, backgammon has about \\(10^{20}\\) states, computer Go has about \\(10^{170}\\) states, and helicopter control lives in a continuous state space. In such problems, we would like model-free methods to still work well for both prediction and control. However, the standard way we have represented value functions so far with a lookup table with one entry per state \\(V(s)\\), or one entry per state-action pair \\(Q(s,a)\\), does not scale: there are simply too many states and/or actions to store in memory, and learning each value independently is too slow.</p>"},{"location":"RL-7-Value%20function%20approximation/#solution-function-approximation","title":"Solution: function approximation","text":"<p>To handle large (or continuous) state spaces, we stop storing one number per state (or state--action pair) and instead predict values with a parameter vector \\(w\\):</p> \\[ \\hat{v}(s,w) \\approx v_{\\pi}(s), \\qquad \\hat{q}(s,a,w) \\approx q_{\\pi}(s,a). \\] <p>Here \\(w\\) are the model's internal parameters (e.g., weights of a linear model or a neural network). Function approximation also enables generalization because many states share the same parameters \\(w\\): updating \\(w\\) using experience from some states can also change the predicted values of other, possibly unseen, states. We update \\(w\\) using the same ideas as before (Monte Carlo or Temporal-Difference learning), but instead of editing a table entry, we adjust \\(w\\) so that \\(\\hat{v}(s,w)\\) or \\(\\hat{q}(s,a,w)\\) moves toward a chosen target (an MC return or a TD target).</p>"},{"location":"RL-7-Value%20function%20approximation/#types-of-value-function-approximation","title":"Types of value function approximation","text":"<p>A value-function approximator maps inputs (a state, and sometimes an action) to value estimates. The common setups are: (1) state-value function approximation, where the input is \\(s\\) and the output is a single number \\(\\hat{v}(s,w)\\), (2) state-action value function approximation for a single action, where the input is \\((s,a)\\) and the output is one number \\(\\hat{q}(s,a,w)\\), useful when we only care about the value of a specific action at a specific state and (3) state-action values for all actions, where the input is \\(s\\) and the output is a vector \\(\\big(\\hat{q}(s,a_1,w),\\ldots,\\hat{q}(s,a_m,w)\\big)\\), which is especially convenient for discrete action spaces because we get all action-values in one forward pass.</p> <p>Models for function approximation. Many models can approximate value functions, including linear feature combinations, neural networks, decision trees, nearest-neighbour methods, Fourier/wavelet bases, etc. In this chapter we focus on differentiable approximators (like linear models and neural networks). Differentiability matters because it gives us gradients: it tells us how the prediction changes when we change the parameters \\(w\\), which is exactly what we need for gradient-based learning. One extra wrinkle in reinforcement learning is that the data are usually not i.i.d. They come from trajectories, so consecutive samples are temporally correlated. In control, things are even less stable: as the policy improves, the data distribution can shift over time. So our training methods must cope with non-i.i.d. and non-stationary data. Later in this chapter we will look into how we handle these issues.</p>"},{"location":"RL-7-Value%20function%20approximation/#learning-methods-for-value-function-approximation","title":"Learning methods for value function approximation","text":"<p>When learning with function approximation, updates are usually done in one of two ways: batch methods and incremental methods. Batch methods collect many samples and then update using the whole dataset (or large chunks of it). Incremental methods update continuously, using one sample (or a small mini-batch) at a time. Reinforcement learning most often uses the incremental style, since data arrive sequentially from interaction. We therefore begin with incremental, gradient-based updates like gradient descent and its stochastic variants, before returning to batch-style methods.</p>"},{"location":"RL-7-Value%20function%20approximation/#gradient-descent-and-stochastic-gradient-descent-sgd","title":"Gradient descent and stochastic gradient descent (SGD)","text":"<p>To learn the parameter \\(w\\), we need a systematic way to improve our approximation. Concretely, we are fitting a parameterized function that takes a state (or a state and action) as input and produces a value prediction that should match the targets generated from experience. The standard approach is to define an objective function that measures how wrong our current value predictions are, and then adjust \\(w\\) to reduce that error. When the objective is differentiable, gradient-based methods give a simple and effective update rule. Let \\(J(w)\\) be a differentiable objective function of the parameter vector \\(w\\). Its gradient is the vector of partial derivatives</p> \\[ \\nabla_w J(w) = \\begin{pmatrix} \\frac{\\partial J(w)}{\\partial w_1}\\\\ \\vdots\\\\ \\frac{\\partial J(w)}{\\partial w_n} \\end{pmatrix}. \\] <p>To reduce \\(J(w)\\), we move parameters in the direction of the negative gradient:</p> \\[ \\Delta w = -\\frac{1}{2}\\alpha \\nabla_w J(w), \\] <p>where \\(\\alpha&gt;0\\) is the step size.</p> <p>Value function approximation via SGD. We want parameters \\(w\\) that minimize the mean-squared error between the approximation \\(\\hat{v}(s,w)\\) and the true value \\(v_\\pi(s)\\):</p> \\[ J(w) = \\mathbb{E}_{\\pi}\\!\\left[\\left(v_\\pi(S) - \\hat{v}(S,w)\\right)^2\\right]. \\] <p>Gradient descent gives</p> \\[ \\Delta w = -\\frac{1}{2}\\alpha \\nabla_w J(w) = \\alpha\\, \\mathbb{E}_{\\pi}\\!\\left[\\left(v_\\pi(S) - \\hat{v}(S,w)\\right)\\nabla_w \\hat{v}(S,w)\\right]. \\] <p>Incremental (stochastic) update. Computing the expectation in the gradient update is usually infeasible, so we approximate it by sampling. Concretely, we generate a trajectory by following policy \\(\\pi\\), take the current visited state \\(S\\), and use it as a sample from the state distribution induced by \\(\\pi\\). Replacing the expectation with this single sample gives the incremental (stochastic) update:</p> \\[ \\Delta w = \\alpha \\left(v_\\pi(S) - \\hat{v}(S,w)\\right)\\nabla_w \\hat{v}(S,w). \\] <p>If we generate experience by following \\(\\pi\\), then each visited state \\(S\\) can be viewed as a sample from the state distribution induced by \\(\\pi\\). Then SGD replaces this expectation with a single sampled term,</p> \\[ \\alpha\\left(v_\\pi(S)-\\hat{v}(S,w)\\right)\\nabla_w \\hat{v}(S,w), \\] <p>so each step uses only the current state from the trajectory. Individual updates are noisy because \\(S\\) changes from step to step, and some states may appear more often than others. But because the samples come from the same distribution used in the expectation, the sampled update is unbiased: if we average these incremental updates over many time steps, we recover the expected (full-gradient) update direction. This is why SGD still minimizes \\(J(w)\\) in expectation, while avoiding the cost of computing the full average at every step.</p>"},{"location":"RL-7-Value%20function%20approximation/#linear-value-function-approximation","title":"Linear value function approximation","text":"<p>Instead of working with the raw state, we describe each state using a feature vector</p> \\[ x(S)= \\begin{pmatrix} x_1(S)\\\\ \\vdots\\\\ x_n(S) \\end{pmatrix}. \\] <p>We can think of features as measurements that summarize what matters about each state \\(S\\), for example a robot's distance to landmarks, indicators from the stock market, or patterns of pieces in chess. Features can make learning practical, but they may also discard information about the original state. For the remaining of this chapter, we assume we have a reasonably good feature representation.</p> <p>Linear model. A simple and widely used differentiable approximator is a linear model:</p> \\[ \\hat{v}(S,w)=x(S)^\\top w=\\sum_{j=1}^n x_j(S)\\,w_j. \\] <p>Using mean-squared error,</p> \\[ J(w)=\\mathbb{E}_{\\pi}\\!\\left[\\left(v_\\pi(S)-x(S)^\\top w\\right)^2\\right], \\] <p>the objective is quadratic in \\(w\\). This is nice: for linear approximation, SGD converges (under standard conditions) to a global minimizer of \\(J(w)\\). Then the gradient is especially clean:</p> \\[ \\nabla_w \\hat{v}(S,w)=x(S), \\] <p>so the incremental update becomes</p> \\[ \\Delta w=\\alpha\\left(v_\\pi(S)-\\hat{v}(S,w)\\right)x(S), \\qquad\\text{or}\\qquad \\Delta w_j=\\alpha\\left(v_\\pi(S)-\\hat{v}(S,w)\\right)x_j(S). \\] <p>Interpretation of the update. Each step has the intuitive form</p> \\[ \\text{update}=\\text{step size}\\times\\text{prediction error}\\times\\text{feature value}. \\] <p>If a feature is inactive (\\(x_j(S)=0\\)), its weight does not change. If a feature has large magnitude, it drives a larger update. Even when the linear model cannot represent \\(v_\\pi\\) perfectly, good features can still capture enough structure for accurate and useful predictions.</p> <p>Table lookup as a special case. A lookup table is a special case of linear approximation, it is linear approximation with one-hot features. If the finite state space is \\(\\{s_1,\\dots,s_n\\}\\), define</p> \\[ x^{\\text{table}}(S)= \\begin{pmatrix} \\mathbb{I}(S=s_1)\\\\ \\vdots\\\\ \\mathbb{I}(S=s_n) \\end{pmatrix}, \\] <p>where \\(\\mathbb{I}(\\cdot)\\) is \\(1\\) if its argument is true and \\(0\\) otherwise. Then</p> \\[ \\hat{v}(S,w)=\\big(x^{\\text{table}}(S)\\big)^\\top w = \\sum_{i=1}^n \\mathbb{I}(S=s_i)\\,w_i = w_i \\quad \\text{if } S=s_i. \\] <p>In this case we create one feature per state. So the model simply picks the single parameter tied to the current state, which is exactly what a lookup table does.</p>"},{"location":"RL-7-Value%20function%20approximation/#incremental-prediction-with-value-function-approximation","title":"Incremental prediction with value function approximation","text":"<p>Till now, we assumed, the objective used the true value \\(v_\\pi(s)\\) as if an oracle provided it. But, in reinforcement learning we generally do not have such information, we only see rewards (and next state). The practical fix is simple: replace \\(v_\\pi(S_t)\\) with a target computed from experience, and do an incremental SGD update:</p> \\[ \\Delta w = \\alpha\\left(\\text{target}_t-\\hat{v}(S_t,w)\\right)\\nabla_w \\hat{v}(S_t,w). \\] <p>The only thing that changes across methods is how we choose \\(\\text{target}_t\\):</p> <ul> <li>Monte Carlo (MC): use the return \\(G_t\\),</li> </ul> \\[ \\Delta w = \\alpha\\left(G_t-\\hat{v}(S_t,w)\\right)\\nabla_w \\hat{v}(S_t,w). \\] <ul> <li>TD(0): use the one-step TD target \\(R_{t+1}+\\gamma \\hat{v}(S_{t+1},w)\\),</li> </ul> \\[ \\Delta w = \\alpha\\left(R_{t+1}+\\gamma \\hat{v}(S_{t+1},w)-\\hat{v}(S_t,w)\\right)\\nabla_w \\hat{v}(S_t,w). \\] <ul> <li>TD(\\(\\lambda\\)): use the \\(\\lambda\\)-return \\(G_t^\\lambda\\),</li> </ul> \\[ \\Delta w = \\alpha\\left(G_t^\\lambda-\\hat{v}(S_t,w)\\right)\\nabla_w \\hat{v}(S_t,w). \\]"},{"location":"RL-7-Value%20function%20approximation/#monte-carlo-with-value-function-approximation","title":"Monte Carlo with value function approximation","text":"<p>The Monte Carlo return \\(G_t\\) is a noisy but unbiased sample of the true value \\(v_\\pi(S_t)\\) (its expectation equals \\(v_\\pi(S_t)\\)). This makes MC prediction with function approximation look a lot like supervised learning: from each episode we get training pairs</p> \\[ \\langle S_1,G_1\\rangle,\\ \\langle S_2,G_2\\rangle,\\ \\ldots,\\ \\langle S_T,G_T\\rangle. \\] <p>We then apply the generic incremental update with, \\(\\text{target}_t=G_t\\):</p> \\[ \\Delta w=\\alpha\\left(G_t-\\hat{v}(S_t,w)\\right)\\nabla_w \\hat{v}(S_t,w). \\] <p>In this setting, we apply the update once per time step \\(t\\) (for each visited state \\(S_t\\)), using that step's return \\(G_t\\) as the target. In an episodic task, \\(G_t\\) depends on rewards all the way until the end of the episode, so we typically:</p> <ul> <li>run an episode to termination,</li> <li>compute \\(G_t\\) for \\(t=0,1,\\dots,T-1\\),</li> <li>then update \\(w\\) for each time step (either in forward or backward order):</li> </ul> \\[ w \\leftarrow w + \\alpha\\left(G_t-\\hat{v}(S_t,w)\\right)\\nabla_w \\hat{v}(S_t,w). \\] <p>For a linear approximator \\(\\hat{v}(s,w)=x(s)^\\top w\\) (so \\(\\nabla_w \\hat{v}(S_t,w)=x(S_t)\\)), then this becomes</p> \\[ \\Delta w=\\alpha\\left(G_t-\\hat{v}(S_t,w)\\right)x(S_t). \\] <p>With non-linear approximators, the objective is generally non-convex, so MC prediction typically converges to a local optimum rather than guaranteeing a global one.</p>"},{"location":"RL-7-Value%20function%20approximation/#td-learning-with-value-function-approximation","title":"TD learning with value function approximation","text":"<p>Monte Carlo uses the full return \\(G_t\\) as a target, which is unbiased but only available after the episode ends. TD methods trade a bit of bias for faster, online learning by building targets from the next reward and the current value estimate. The TD(0) target</p> \\[ R_{t+1}+\\gamma \\hat{v}(S_{t+1},w) \\] <p>is a biased estimate of \\(v_\\pi(S_t)\\) because it bootstraps from the current approximation \\(\\hat{v}(\\cdot,w)\\). The TD target includes the current value estimate for the next state, and that estimate is generally not exactly correct. As a result, even if we average over many transitions, the target tends to be systematically biased. Still, we can think of TD as supervised learning on moving targets, with training pairs like</p> \\[ \\langle S_t,\\ R_{t+1}+\\gamma \\hat{v}(S_{t+1},w)\\rangle. \\] <p>Notably, the TD target depends on \\(\\hat{v}(\\cdot,w)\\), so the \"label\" changes as \\(w\\) changes. To derive the TD(0) update, we start from the generic incremental SGD rule</p> \\[ \\Delta w=\\alpha\\left(\\text{target}_t-\\hat{v}(S_t,w)\\right)\\nabla_w \\hat{v}(S_t,w). \\] <p>Substituting the value of the TD(0) target into the generic rule gives</p> \\[ \\Delta w =\\alpha\\left(R_{t+1}+\\gamma \\hat{v}(S_{t+1},w)-\\hat{v}(S_t,w)\\right)\\nabla_w \\hat{v}(S_t,w). \\] <p>It is common to name the term in parentheses the TD error,</p> \\[ \\delta_t = R_{t+1}+\\gamma \\hat{v}(S_{t+1},w)-\\hat{v}(S_t,w), \\] <p>so the update can be written compactly as \\(\\Delta w=\\alpha\\,\\delta_t\\,\\nabla_w \\hat{v}(S_t,w)\\). For a linear model \\(\\hat{v}(s,w)=x(s)^\\top w\\) (so \\(\\nabla_w \\hat{v}(S_t,w)=x(S_t)\\)), this becomes</p> \\[ \\Delta w=\\alpha\\,\\delta_t\\,x(S_t), \\qquad \\delta_t=R_{t+1}+\\gamma \\hat{v}(S_{t+1},w)-\\hat{v}(S_t,w). \\] <p>A useful practical note: unlike MC, TD(0) can update immediately at each time step because its target does not require waiting for the episode to end.</p>"},{"location":"RL-7-Value%20function%20approximation/#tdlambda-with-value-function-approximation","title":"TD(\\(\\lambda\\)) with value function approximation","text":"<p>Like TD(0), the \\(\\lambda\\)-return \\(G_t^\\lambda\\) bootstraps from current estimates, so it is also a biased target for \\(v_\\pi(S_t)\\). But, we can still think in supervised-learning terms, using pairs</p> \\[ \\langle S_1,G_1^\\lambda\\rangle,\\ \\langle S_2,G_2^\\lambda\\rangle,\\ \\ldots,\\ \\langle S_{T-1},G_{T-1}^\\lambda\\rangle. \\] <p>Forward view (linear TD(\\(\\lambda\\))). Using the generic SGD rule with \\(\\text{target}_t=G_t^\\lambda\\) gives</p> \\[ \\Delta w =\\alpha\\left(G_t^\\lambda-\\hat{v}(S_t,w)\\right)\\nabla_w \\hat{v}(S_t,w). \\] <p>For a linear approximator \\(\\hat{v}(s,w)=x(s)^\\top w\\) (so \\(\\nabla_w \\hat{v}(S_t,w)=x(S_t)\\)),</p> \\[ \\Delta w=\\alpha\\left(G_t^\\lambda-\\hat{v}(S_t,w)\\right)x(S_t). \\] <p>Backward view (linear TD(\\(\\lambda\\))). The forward view is conceptually clean, but it still refers to \\(G_t^\\lambda\\), which is defined using future rewards. The backward view implements the same idea online by accumulating credit with an eligibility trace. We define the TD error</p> \\[ \\delta_t=R_{t+1}+\\gamma \\hat{v}(S_{t+1},w)-\\hat{v}(S_t,w), \\] <p>and the eligibility trace</p> \\[ E_t=\\gamma\\lambda E_{t-1}+x(S_t), \\qquad E_{-1}=0. \\] <p>Then the update is</p> \\[ \\Delta w=\\alpha\\,\\delta_t\\,E_t. \\]"},{"location":"RL-7-Value%20function%20approximation/#control-with-value-function-approximation","title":"Control with value function approximation","text":"<p>In control, the goal is not just to predict values under a fixed policy, but to improve the policy. A common pattern is to alternate between:</p> <ul> <li>Policy evaluation: learn an approximation to the action-value function,</li> </ul> \\[ \\hat{q}(\\cdot,\\cdot,w)\\approx q_\\pi, \\] <ul> <li>Policy improvement: make \\(\\pi\\) more greedy with respect to \\(\\hat{q}\\) (often via \\(\\epsilon\\)-greedy).</li> </ul>"},{"location":"RL-7-Value%20function%20approximation/#policy-evaluation","title":"Policy Evaluation","text":""},{"location":"RL-7-Value%20function%20approximation/#action-value-function-approximation","title":"Action-value function approximation","text":"<p>For evaluation, we would ideally choose \\(w\\) to minimize the mean-squared error between \\(\\hat{q}(S,A,w)\\) and the true \\(q_\\pi(S,A)\\). Similar to the state-value function approximation we can define:</p> \\[ J(w)=\\mathbb{E}_{\\pi}\\!\\left[\\left(q_\\pi(S,A)-\\hat{q}(S,A,w)\\right)^2\\right]. \\] <p>Applying gradient descent gives</p> \\[ \\Delta w =-\\frac{1}{2}\\alpha \\nabla_w J(w) =\\alpha\\,\\mathbb{E}_{\\pi}\\!\\left[\\left(q_\\pi(S,A)-\\hat{q}(S,A,w)\\right)\\nabla_w \\hat{q}(S,A,w)\\right]. \\] <p>In practice we use a sampled (incremental) version of this update, replacing the expectation and the unknown \\(q_\\pi\\) with targets estimated from experience (e.g., TD targets). With this target-based, incremental update in mind, we now specify how to represent action-values with a parameterized function\u2014starting with the common linear case.</p> <p>Linear action-value function approximation. For action-values, we can describe a state-action pair with a feature vector</p> \\[ x(S,A)= \\begin{pmatrix} x_1(S,A)\\\\ \\vdots\\\\ x_n(S,A) \\end{pmatrix}, \\] <p>and use a linear model</p> \\[ \\hat{q}(S,A,w)=x(S,A)^\\top w=\\sum_{j=1}^n x_j(S,A)\\,w_j. \\] <p>The gradient is immediate:</p> \\[ \\nabla_w \\hat{q}(S,A,w)=x(S,A). \\] <p>So the incremental SGD update becomes</p> \\[ \\Delta w=\\alpha\\left(q_\\pi(S,A)-\\hat{q}(S,A,w)\\right)x(S,A). \\] <p>(In practice, \\(q_\\pi(S,A)\\) is unknown and is replaced by a target estimated from experience.)</p> <p>Incremental control algorithms with function approximation. Just like in prediction, we do not know the true action-value \\(q_\\pi(S,A)\\), so we learn from targets built from experience. With a differentiable approximator \\(\\hat{q}(S,A,w)\\), the generic incremental update is</p> \\[ \\Delta w =\\alpha\\left(\\text{target}_t-\\hat{q}(S_t,A_t,w)\\right)\\nabla_w \\hat{q}(S_t,A_t,w). \\] <p>Different control algorithms mainly differ in how they choose \\(\\text{target}_t\\):</p> <ul> <li>MC control: use the return \\(G_t\\),</li> </ul> \\[ \\Delta w =\\alpha\\left(G_t-\\hat{q}(S_t,A_t,w)\\right)\\nabla_w \\hat{q}(S_t,A_t,w). \\] <ul> <li>TD(0) / SARSA-style: bootstrap from the next state-action,</li> </ul> \\[ \\text{target}_t = R_{t+1}+\\gamma \\hat{q}(S_{t+1},A_{t+1},w), \\] <p>giving</p> \\[ \\Delta w =\\alpha\\left(R_{t+1}+\\gamma \\hat{q}(S_{t+1},A_{t+1},w)-\\hat{q}(S_t,A_t,w)\\right)\\nabla_w \\hat{q}(S_t,A_t,w). \\] <ul> <li>Forward-view TD(\\(\\lambda\\)): use an action-value \\(\\lambda\\)-return \\(q_t^\\lambda\\),</li> </ul> \\[ \\Delta w =\\alpha\\left(q_t^\\lambda-\\hat{q}(S_t,A_t,w)\\right)\\nabla_w \\hat{q}(S_t,A_t,w). \\] <ul> <li>Backward-view TD(\\(\\lambda\\)) (eligibility traces): define the TD error</li> </ul> \\[ \\delta_t =R_{t+1}+\\gamma \\hat{q}(S_{t+1},A_{t+1},w)-\\hat{q}(S_t,A_t,w), \\] <p>and an eligibility trace</p> \\[ E_t=\\gamma\\lambda E_{t-1}+\\nabla_w \\hat{q}(S_t,A_t,w), \\qquad E_{-1}=0, \\] <p>then update</p> \\[ \\Delta w=\\alpha\\,\\delta_t\\,E_t. \\]"},{"location":"RL-7-Value%20function%20approximation/#policy-improvement","title":"Policy Improvement","text":"<p>Once we have an updated action-value approximation, we improve the policy by following a \\epsilon-greedy policy with respect to the current estimate: most of the time we choose the action with the highest estimated value under the current approximator, and with small probability epsilon we choose a random action to keep exploring. This couples naturally with the incremental updates above: as the action-value estimates change online, the epsilon-greedy policy tracks them, gradually shifting behavior toward better actions while still occasionally sampling alternatives to avoid getting stuck with a poor early estimate.</p>"},{"location":"RL-7-Value%20function%20approximation/#gradient-td-learning-and-its-convergence","title":"Gradient TD learning and it's convergence","text":"<p>With function approximation, it is tempting to think TD is minimizing a single error measure like</p> \\[ J(w)=\\mathbb{E}_\\pi\\!\\left[\\left(v_\\pi(S)-\\hat{v}(S,w)\\right)^2\\right]. \\] <p>For Monte Carlo, the update can be seen as (stochastic) gradient descent on this objective, because the target \\(G_t\\) is an unbiased sample of \\(v_\\pi(S_t)\\). For TD, however, the target</p> \\[ R_{t+1}+\\gamma \\hat{v}(S_{t+1},w) \\] <p>itself depends on \\(w\\). As a result, the usual TD update</p> \\[ \\Delta w=\\alpha\\left(R_{t+1}+\\gamma \\hat{v}(S_{t+1},w)-\\hat{v}(S_t,w)\\right)\\nabla_w \\hat{v}(S_t,w) \\] <p>is not, in general, the gradient of a single fixed scalar objective in \\(w\\) (especially off-policy, and with non-linear approximation). That is a core reason TD can be unstable or even diverge. The TD update is built from the Bellman equation and a bootstrapped target, e.g.</p> \\[ \\Delta w \\propto \\delta_t \\nabla_w \\hat{v}(S_t,w), \\qquad \\delta_t = R_{t+1}+\\gamma \\hat{v}(S_{t+1},w)-\\hat{v}(S_t,w). \\] <p>If we try to treat TD as \"just doing SGD\" on the squared TD error</p> \\[ \\ell_t(w) = \\frac{1}{2}\\,\\delta_t^2, \\] <p>its true gradient is</p> \\[ \\nabla_w \\ell_t(w)=\\delta_t\\,\\nabla_w \\delta_t =\\delta_t\\Big(\\gamma \\nabla_w \\hat{v}(S_{t+1},w)-\\nabla_w \\hat{v}(S_t,w)\\Big). \\] <p>But the standard TD update keeps only the second term:</p> \\[ \\Delta w \\propto \\delta_t\\,\\nabla_w \\hat{v}(S_t,w), \\] <p>and drops the \\(\\gamma \\nabla_w \\hat{v}(S_{t+1},w)\\) part. This is called a semi-gradient method: it treats the target \\(R_{t+1}+\\gamma \\hat{v}(S_{t+1},w)\\) as if it were a constant, even though it depends on \\(w\\). This keeps the update local and cheap (it avoids \"backpropagation through the target\"), and it matches the Bellman fixed-point view of TD. Because TD is doing a semi-gradient update. We treat the bootstrapped target</p> \\[ R_{t+1}+\\gamma \\hat{v}(S_{t+1},w) \\] <p>as a constant at time \\(t\\) and only differentiate the prediction \\(\\hat{v}(S_t,w)\\).</p>"},{"location":"RL-7-Value%20function%20approximation/#gradient-td","title":"Gradient TD","text":"<p>Gradient TD methods address this by defining an explicit objective and then performing (stochastic) gradient descent on it. A common choice is the mean-squared projected Bellman error (MSPBE). Let \\(T^\\pi\\) be the Bellman operator and let \\(\\Pi\\) denote projection onto the function class (e.g., projection onto the span of features under a chosen norm). For an approximate value \\(\\hat{v}_w\\), define</p> \\[ \\text{MSPBE}(w)\\;=\\big\\|\\Pi T^\\pi \\hat{v}_w-\\hat{v}_w\\big\\|^2. \\] <p>Intuitively, \\(T^\\pi \\hat{v}_w\\) is the one-step Bellman backup, and \\(\\Pi\\) projects it back into the representable set, so we measure how far \\(\\hat{v}_w\\) is from its projected Bellman update. In the linear case \\(\\hat{v}_w(s)=x(s)^\\top w\\), define the TD error</p> \\[ \\delta_t = R_{t+1}+\\gamma x(S_{t+1})^\\top w - x(S_t)^\\top w. \\] <p>This leads to two coupled SGD recursions: one for the main weights \\(w\\) and one for an auxiliary vector \\(h\\) that estimates a correction term needed by the gradient. One popular example is GTD2:</p> \\[ h \\leftarrow h + \\beta\\left(\\delta_t - x(S_t)^\\top h\\right)x(S_t), \\] \\[ w \\leftarrow w + \\alpha\\left(x(S_t)-\\gamma x(S_{t+1})\\right)\\big(x(S_t)^\\top h\\big), \\] <p>with step sizes \\(\\alpha,\\beta&gt;0\\) (often with \\(\\beta\\) larger so \\(h\\) adapts faster). We can think of it as: standard TD uses one set of weights but may be unstable off-policy; Gradient TD adds \\(h\\) so the update truly follows the gradient, which improves stability (especially off-policy with linear approximation).</p>"},{"location":"RL-7-Value%20function%20approximation/#convergence-of-control-algorithms","title":"Convergence of control algorithms","text":"<p>When we say an algorithm \"converges\", we usually mean the parameters \\(w\\) approach a fixed point (and hopefully one that is close to the best possible within the chosen function class). A practical summary for control with function approximation is:</p> <ul> <li>Monte Carlo control: stable with table lookup; often reasonable with linear approximation but may chatter; no general guarantee with non-linear models.</li> <li>SARSA (on-policy TD control): stable with table lookup; can also chatter with linear approximation; no general guarantee with non-linear models.</li> <li>Q-learning (off-policy TD control): stable with table lookup; can diverge with linear or non-linear approximation.</li> <li>Gradient Q-learning: designed for stability with table lookup and linear approximation; still no general guarantee with non-linear models.</li> </ul> <p>Here chattering informally means the parameters keep oscillating around a near-optimal solution rather than settling to an exact fixed point.</p>"},{"location":"RL-7-Value%20function%20approximation/#batch-reinforcement-learning","title":"Batch reinforcement learning","text":"<p>Incremental SGD is convenient, but it often underuses data: a transition is seen once, we update, and then we move on. Batch RL takes the opposite view: collect experience into a dataset and then fit the value function to that dataset as well as possible. This is useful when data are expensive (e.g., real robots) or when we want to squeeze more learning out of past experience.</p> <p>Least-squares prediction. If we can treat learning as supervised regression, a natural goal is to find the parameters that best fit the observed targets in a squared-error sense. Given a dataset</p> \\[ \\mathcal{D}=\\{\\langle s_1,v_1^\\pi\\rangle,\\ \\langle s_2,v_2^\\pi\\rangle,\\ \\ldots,\\ \\langle s_T,v_T^\\pi\\rangle\\}, \\] <p>and an approximator \\(\\hat{v}(s,w)\\), least-squares chooses</p> \\[ w^\\pi=\\arg\\min_w LS(w), \\qquad LS(w)=\\sum_{t=1}^{T}\\left(v_t^\\pi-\\hat{v}(s_t,w)\\right)^2. \\] <p>Compared to one-pass SGD, this objective makes the goal explicit: \"fit the best value function you can, given the data you have.\"</p> <p>Experience replay (SGD on a dataset). In practice, we may not solve the least-squares problem in closed form, but we can optimize it with SGD by repeatedly replaying samples from \\(\\mathcal{D}\\):</p> <ol> <li>Sample \\(\\langle s,v^\\pi\\rangle \\sim \\mathcal{D}\\) (often uniformly at random).</li> <li>Update</li> </ol> \\[ \\Delta w=\\alpha\\left(v^\\pi-\\hat{v}(s,w)\\right)\\nabla_w \\hat{v}(s,w). \\] <p>This is called experience replay: we reuse past experience many times, which improves sample efficiency and (with random sampling) reduces the temporal correlations that appear in online trajectories. So far, this is just \"supervised learning on stored RL experience.\" The next step is to apply the same idea to control with action-values: store transitions \\((s,a,r,s')\\) in a replay buffer and train a Q-function from mini-batches.</p>"},{"location":"RL-7-Value%20function%20approximation/#deep-q-network","title":"Deep Q-network","text":"<p>The idea is simple: replace the lookup table \\(Q(s,a)\\) with a neural network \\(Q(s,a;w)\\), and train it so that its outputs satisfy the Bellman optimality equation. However, naively combining Q-learning (bootstrapping) with a non-linear network can be unstable. To handle this, we can introduce deep Q-networks (DQN), it became successful largely because it adds two stabilizing ideas: experience replay and fixed (or slowly changing) Q-targets.</p>"},{"location":"RL-7-Value%20function%20approximation/#experience-replay","title":"Experience replay","text":"<p>As the agent interacts with the environment, it stores transitions in a replay buffer \\(\\mathcal{D}\\):</p> \\[ (s_t,a_t,r_{t+1},s_{t+1})\\in\\mathcal{D}. \\] <p>Training then samples mini-batches uniformly at random from \\(\\mathcal{D}\\). Random sampling breaks strong temporal correlations in consecutive experience and makes learning closer to i.i.d. supervised training, which typically stabilizes SGD and improves data efficiency (each transition can be reused many times).</p>"},{"location":"RL-7-Value%20function%20approximation/#fixed-q-targets","title":"Fixed Q-targets","text":"<p>A core instability in bootstrapping is the moving target problem: the target depends on the same parameters we are updating. If \\(w\\) changes, then both the prediction and the target can change at the same time, which can cause oscillations or divergence.</p> <p>DQN reduces this by using a separate set of (older) parameters \\(w^{-}\\) to compute targets, while the current network uses \\(w\\). For a sampled transition \\((s,a,r,s')\\), the DQN target is</p> \\[ y = r + \\gamma \\max_{a'} Q(s',a';w^{-}), \\] <p>and we fit \\(Q(s,a;w)\\) to this target by minimizing the squared error</p> \\[ \\mathcal{L}(w) =\\mathbb{E}_{(s,a,r,s')\\sim\\mathcal{D}}\\Big[\\big(y-Q(s,a;w)\\big)^2\\Big] = \\mathbb{E}_{(s,a,r,s')\\sim\\mathcal{D}} \\Big[\\big(r+\\gamma \\max_{a'} Q(s',a';w^{-})-Q(s,a;w)\\big)^2\\Big]. \\] <p>Stability intuition. Putting it together, DQN repeatedly:</p> <ol> <li>collects transitions and stores them in \\(\\mathcal{D}\\) (often using \\(\\epsilon\\)-greedy w.r.t. \\(Q(\\cdot,\\cdot;w)\\)),</li> <li>samples random mini-batches from \\(\\mathcal{D}\\),</li> <li>treats \\(y=r+\\gamma \\max_{a'} Q(s',a';w^{-})\\) as a (nearly) fixed label and does supervised-style regression,</li> <li>updates the target network occasionally (hard update \\(w^{-}\\leftarrow w\\) every \\(K\\) steps) or slowly (soft updates).</li> </ol>"},{"location":"RL-7-Value%20function%20approximation/#linear-least-squares-prediction","title":"Linear Least Squares Prediction","text":"<p>In the previous section we motivated batch RL: once we store experience in a replay buffer, learning becomes \"fit a function to a dataset.\" Before going deeper into DQN, it is helpful to look at the simplest batch case: linear function approximation. If we use a linear value function,</p> \\[ \\hat{v}(s,w)=x(s)^\\top w, \\] <p>then least-squares prediction becomes a standard linear regression problem. Experience replay + SGD can eventually reach the best-fitting parameters, but it may require many passes over the data. In case of a linear model, we can often compute the least-squares solution directly (or update it efficiently as new data arrive). We start from the least-squares objective on a dataset \\(D=\\{(s_t,v_t^\\pi)\\}_{t=1}^T\\):</p> \\[ LS(w)=\\sum_{t=1}^{T}\\bigl(v_t^\\pi-x(s_t)^\\top w\\bigr)^2. \\] <p>At the minimizer, the gradient must be zero:</p> \\[ \\nabla_w LS(w)=0. \\] <p>Differentiate:</p> \\[ \\nabla_w LS(w) = -2\\sum_{t=1}^{T} x(s_t)\\bigl(v_t^\\pi-x(s_t)^\\top w\\bigr)=0. \\] <p>Rearranging gives the normal equations:</p> \\[ \\sum_{t=1}^{T} x(s_t)\\,v_t^\\pi = \\sum_{t=1}^{T} x(s_t)\\,x(s_t)^\\top w. \\] <p>Define</p> \\[ A\\;=\\sum_{t=1}^{T} x(s_t)\\,x(s_t)^\\top, \\qquad b\\;=\\sum_{t=1}^{T} x(s_t)\\,v_t^\\pi, \\] <p>so the solution (when \\(A\\) is invertible) is</p> \\[ w=A^{-1}b. \\] <ul> <li>If there are \\(N\\) features, solving via a matrix inverse is typically \\(O(N^3)\\).</li> <li>If data arrive sequentially, we can maintain \\(A^{-1}\\) incrementally using Sherman-Morrison updates, giving roughly \\(O(N^2)\\) per new sample.</li> </ul> <p>Linear Least Squares Prediction Algorithms. In the batch setting we want to fit a value function from stored experience. The catch is that we still do not observe the true values \\(v_\\pi(S_t)\\). Instead, we build targets from experience (returns or TD-style bootstraps) and then solve for the parameters that best match those targets under a linear model. With linear approximation \\(\\hat{v}(s,w)=x(s)^\\top w\\), common batch targets are:</p> <ul> <li>LSMC (Least-Squares Monte Carlo): use returns,</li> </ul> \\[ v_\\pi(S_t)\\approx G_t. \\] <ul> <li>LSTD (Least-Squares TD): use the one-step TD target,</li> </ul> \\[ v_\\pi(S_t)\\approx R_{t+1}+\\gamma \\hat{v}(S_{t+1},w). \\] <ul> <li>LSTD(\\(\\lambda\\)): use the \\(\\lambda\\)-return,</li> </ul> \\[ v_\\pi(S_t)\\approx G_t^\\lambda. \\] <p>In each case, the algorithm finds parameters \\(w\\) that satisfy the corresponding MC/TD fixed-point condition on the dataset.</p> <p>Fixed-point equations and closed forms. Assume a dataset of transitions \\((S_t,R_{t+1},S_{t+1})\\) for \\(t=1,\\dots,T\\) and linear features \\(x(S_t)\\).</p> <p>LSMC.</p> <p>Treat \\(G_t\\) as the label and solve the normal equations:</p> \\[ 0=\\sum_{t=1}^{T}\\bigl(G_t-\\hat{v}(S_t,w)\\bigr)x(S_t). \\] <p>This yields</p> \\[ w = \\left(\\sum_{t=1}^{T} x(S_t)x(S_t)^\\top\\right)^{-1} \\left(\\sum_{t=1}^{T} x(S_t)G_t\\right). \\] <p>LSTD.</p> <p>Use the TD target. The fixed-point condition is</p> \\[ 0=\\sum_{t=1}^{T}\\Bigl(R_{t+1}+\\gamma \\hat{v}(S_{t+1},w)-\\hat{v}(S_t,w)\\Bigr)x(S_t), \\] <p>which can be written as \\(Aw=b\\) with</p> \\[ A=\\sum_{t=1}^{T} x(S_t)\\bigl(x(S_t)-\\gamma x(S_{t+1})\\bigr)^\\top, \\qquad b=\\sum_{t=1}^{T} x(S_t)R_{t+1}. \\] <p>So</p> \\[ w=A^{-1}b = \\left(\\sum_{t=1}^{T} x(S_t)\\bigl(x(S_t)-\\gamma x(S_{t+1})\\bigr)^\\top\\right)^{-1} \\left(\\sum_{t=1}^{T} x(S_t)R_{t+1}\\right). \\] <p>LSTD(\\(\\lambda\\)).</p> <p>Let the TD error be</p> \\[ \\delta_t = R_{t+1}+\\gamma \\hat{v}(S_{t+1},w)-\\hat{v}(S_t,w), \\] <p>and define eligibility vectors (backward view)</p> \\[ E_t=\\gamma\\lambda E_{t-1}+x(S_t), \\qquad E_0=0. \\] <p>The fixed-point condition is</p> \\[ 0=\\sum_{t=1}^{T}\\delta_t\\,E_t, \\] <p>which again gives \\(Aw=b\\) with</p> \\[ A=\\sum_{t=1}^{T} E_t\\bigl(x(S_t)-\\gamma x(S_{t+1})\\bigr)^\\top, \\qquad b=\\sum_{t=1}^{T} E_t R_{t+1}, \\] <p>and therefore</p> \\[ w= \\left(\\sum_{t=1}^{T} E_t\\bigl(x(S_t)-\\gamma x(S_{t+1})\\bigr)^\\top\\right)^{-1} \\left(\\sum_{t=1}^{T} E_t R_{t+1}\\right). \\]"},{"location":"RL-7-Value%20function%20approximation/#least-squares-control-policy-iteration","title":"Least Squares Control (Policy Iteration)","text":"<p>For control, the goal is to improve the policy, not just evaluate it. A standard template is generalized policy iteration: repeatedly (i) estimate action-values under the current policy, then (ii) make the policy more greedy with respect to those estimates. Here we focus on doing the evaluation step with a least-squares fit of a linear \\(Q\\)-function.</p>"},{"location":"RL-7-Value%20function%20approximation/#least-squares-action-value-function-approximation","title":"Least-squares action-value function approximation","text":"<p>We approximate the action-value function with a linear model over state-action features:</p> \\[ \\hat{q}(s,a,w)=x(s,a)^\\top w \\approx q_\\pi(s,a). \\] <p>In a batch setting, we treat collected experience under policy \\(\\pi\\) as a dataset of supervised-style pairs,</p> \\[ D=\\Big\\{\\langle (s_1,a_1),v_1^\\pi\\rangle,\\ \\langle (s_2,a_2),v_2^\\pi\\rangle,\\ \\ldots,\\ \\langle (s_T,a_T),v_T^\\pi\\rangle\\Big\\}, \\] <p>and choose \\(w\\) to best fit these targets in a least-squares sense (with \\(v_t^\\pi\\) supplied by returns or TD-style targets in practice).</p> <p>Least-squares control idea. Least-squares control combines two goals: use all stored data efficiently (batch evaluation) and improve the policy (control). In practice, the replay buffer often contains data generated by many past policies, so learning is naturally off-policy. The core idea mirrors Q-learning. From stored experience generated by an old behaviour policy,</p> \\[ (S_t,A_t,R_{t+1},S_{t+1}) \\sim \\pi_{\\text{old}}, \\] <p>we imagine acting according to an improved policy at the next state, for example the greedy policy w.r.t. the current estimate,</p> \\[ A'=\\pi_{\\text{new}}(S_{t+1}) \\;\\approx\\; \\arg\\max_{a'} \\hat{q}(S_{t+1},a',w). \\] <p>Then we push the current estimate \\(\\hat{q}(S_t,A_t,w)\\) toward the bootstrapped target</p> \\[ R_{t+1}+\\gamma \\hat{q}(S_{t+1},A',w). \\] <p>Least-squares methods differ from vanilla Q-learning mainly in how they fit this target: instead of a one-step SGD update, they fit \\(w\\) using (approximate) least-squares over the whole dataset.</p>"},{"location":"RL-7-Value%20function%20approximation/#least-squares-q-learning-lstdq","title":"Least Squares Q-Learning (LSTDQ)","text":"<p>Start from the usual one-step Q-learning-style TD error (using the greedy/improved action at the next state):</p> \\[ \\delta_t =R_{t+1}+\\gamma \\hat{q}\\bigl(S_{t+1},\\pi(S_{t+1}),w\\bigr)-\\hat{q}(S_t,A_t,w), \\] <p>and the linear TD update</p> \\[ \\Delta w_t=\\alpha\\,\\delta_t\\,x(S_t,A_t). \\] <p>LSTDQ replaces many small SGD steps with a single batch solution by asking for a fixed point: across the dataset, the updates should balance out, i.e.,</p> \\[ 0=\\sum_{t=1}^{T}\\Delta w_t =\\sum_{t=1}^{T}\\alpha\\,\\delta_t\\,x(S_t,A_t). \\] <p>Dropping the common factor \\(\\alpha\\) and substituting the linear form \\(\\hat{q}(s,a,w)=x(s,a)^\\top w\\) gives</p> \\[ 0=\\sum_{t=1}^{T}x(S_t,A_t)\\Bigl(R_{t+1}+\\gamma x\\bigl(S_{t+1},\\pi(S_{t+1})\\bigr)^\\top w-x(S_t,A_t)^\\top w\\Bigr). \\] <p>Rearrange into the standard linear system \\(Aw=b\\) by grouping terms in \\(w\\):</p> \\[ \\left(\\sum_{t=1}^{T}x(S_t,A_t)\\Bigl(x(S_t,A_t)-\\gamma x\\bigl(S_{t+1},\\pi(S_{t+1})\\bigr)\\Bigr)^\\top\\right)w = \\sum_{t=1}^{T}x(S_t,A_t)\\,R_{t+1}. \\] <p>So, when the matrix is invertible,</p> \\[ w= \\left( \\sum_{t=1}^{T} x(S_t,A_t)\\Bigl(x(S_t,A_t)-\\gamma x\\bigl(S_{t+1},\\pi(S_{t+1})\\bigr)\\Bigr)^\\top \\right)^{-1} \\left( \\sum_{t=1}^{T}x(S_t,A_t)\\,R_{t+1} \\right). \\]"},{"location":"RL-7-Value%20function%20approximation/#least-squares-policy-iteration-algorithm","title":"Least Squares Policy Iteration Algorithm","text":"<p>LSPI is the batch, least-squares analogue of policy iteration: it uses a fixed dataset \\(D\\) and alternates between (i) evaluating the current policy using LSTDQ, and (ii) improving the policy by acting greedily with respect to the learned \\(Q\\)-function. The key point is that \\(D\\) can be collected off-policy (from older behaviour policies), while LSPI keeps reusing it to evaluate newer, improved policies.</p> <p>Algorithm (pseudocode).</p> <pre><code>function LSPI(D, \u03c0_0)\n    \u03c0' &lt;- \u03c0_0\n    repeat\n        \u03c0 &lt;- \u03c0'\n        Q  &lt;- LSTDQ(\u03c0, D)            # policy evaluation on the fixed batch\n        for all s in S do             # policy improvement\n            \u03c0'(s) &lt;- argmax_{a in A} Q(s,a)\n        end for\n    until (\u03c0 approx \u03c0')\n    return \u03c0\nend function\n</code></pre> <p>Notes.</p> <ul> <li>Policy evaluation: fit \\(\\hat{q}^\\pi(s,a)=x(s,a)^\\top w\\) from the dataset \\(D\\) using LSTDQ (off-policy data is allowed).</li> <li>Policy improvement: update greedily:</li> </ul> \\[ \\pi_{\\text{new}}(s)\\in\\arg\\max_{a\\in\\mathcal{A}}\\hat{q}(s,a,w). \\] <ul> <li>Stopping: stop when the policy stops changing (or changes are below a chosen threshold).</li> </ul>"},{"location":"RL-7-Value%20function%20approximation/#convergence-of-control-algorithms_1","title":"Convergence of control algorithms","text":"<p>A rough, practical guide to convergence under different approximations is:</p> Algorithm Table lookup Linear Non-linear Monte-Carlo control \\(\\checkmark\\) (\\(\\checkmark\\)) \\(\\times\\) SARSA (on-policy TD) \\(\\checkmark\\) (\\(\\checkmark\\)) \\(\\times\\) Q-learning (off-policy TD) \\(\\checkmark\\) \\(\\times\\) \\(\\times\\) LSPI (via LSTDQ) \\(\\checkmark\\) (\\(\\checkmark\\)) -- <p>Here \\((\\checkmark)\\) means the method often behaves well but may chatter (oscillate around a near-optimal solution) rather than converge cleanly. \"-\" means we are not claiming a general guarantee in that setting.</p>"},{"location":"RL-7-Value%20function%20approximation/#references","title":"References","text":"<ul> <li>https://github.com/zyxue/youtube_RL_course_by_David_Silver</li> </ul>"},{"location":"Transformers-1-Introduction/","title":"Transformers - Introduction","text":""},{"location":"Transformers-1-Introduction/#introduction","title":"Introduction","text":"<p>Transformers are a major breakthrough in deep learning. They use attention as a way for the network to give different weights to different inputs, where those weights are determined by the inputs themselves. This lets transformers naturally capture useful patterns in sequences and other kinds of data. They\u2019re called transformers because they take a set of vectors in one space and turn them into a new set of vectors (with the same size) in another space. This new space is designed such that it holds a richer internal representation that makes it easier to solve downstream tasks.</p> <p>One big strength of transformers is that transfer learning works really well with them. We can first train a transformer on a huge amount of data, then fine-tune that same model for many different tasks. When a large model like this can be adapted to lots of tasks, we call it a foundation model.</p> <p>Transformers can also be trained in a self-supervised way on unlabeled data, which is perfect for language, since there\u2019s so much text available on the internet and elsewhere. The scaling hypothesis says that if we just make the model bigger (more parameters) and train it on more data, we get much better performance even without changing the architecture. Transformers also run very efficiently on GPUs, which support massive parallel processing, so we can train huge language models with around a trillion parameters in a reasonable time. Now we will go through the different components of the Transformer architecture:</p>"},{"location":"Transformers-1-Introduction/#attention","title":"Attention","text":"<p>The core idea behind transformers is attention. It was first created to improve RNNs for machine translation but later, it was shown that we could remove recurrence entirely and rely only on attention, getting much better results. Today, attention-based transformers have largely replaced RNNs in almost all applications. For example, consider the following sentences:</p> <ul> <li>The baseball player gripped the bat.</li> <li>A small animal that hangs upside-down might be a bat.</li> </ul> <p>Here the word \u201cbat\u201d has different meanings in the two sentences. However, this can be detected only by looking at the context provided by the other words in the sequence. We also see that some words are more important than others in determining the interpretation of \u201cbat.\u201d In the first sentence, the words \u201cbaseball player\u201d and \u201cgripped\u201d strongly indicate that \u201cbat\u201d refers to a piece of sports equipment, whereas in the second sentence, the words \u201csmall animal\u201d and \u201changs upside-down\u201d indicate that \u201cbat\u201d refers to a flying mammal. Thus, to determine the appropriate interpretation of \u201cbat,\u201d a neural network processing such a sentence should attend to\u2014i.e., rely more heavily on these specific words from the rest of the sequence. </p> <p>In a standard neural network, each input affects the output based on its weight, and once the network is trained, these weights stay fixed. But based on the above examples, we want the model to focus on different words in different positions for each new input. Attention makes this possible by using weights that change depending on the specific input data.</p> <p>In natural language processing we will see that word embeddings map each word to a vector in an embedding space. These vectors are then used as inputs to neural networks. The embeddings capture basic meaning: words with similar meanings end up close together in this space. A key point is that each word always maps to the same vector (for example, \u201cbat\u201d always has one fixed embedding even when the context is different, like in our example above). Meanwhile, a transformer can be seen as a more powerful kind of embedding. It maps each word\u2019s vector to a new vector that depends on the other words in the sequence. That means the word \u201cbat\u201d can end up in different places depending on the sentence: near \u201cbaseball\u201d or near \u201canimal\u201d.</p>"},{"location":"Transformers-1-Introduction/#transformer-processing","title":"Transformer processing","text":"<p>In a transformer, the input is a set of vectors \\(\\{x_n\\}\\) of dimensionality \\(D\\), for \\(n = 1, \\dots, N\\). Each vector is called a token. A token might correspond to a word in a sentence or a patch in an image.</p> <p>The individual components \\(x_{ni}\\) of each token are called features. A key advantage of transformers is that we do not need to design a different neural network architecture for each data type. Instead, we simply convert the different kinds of data into a shared set of tokens and feed them into the same model.</p>"},{"location":"Transformers-1-Introduction/#notations","title":"Notations","text":"<p>We stack the token vectors for one sequence into a data matrix \\(\\mathbf{X}\\) of shape \\(\\mathbf{N} \\times \\mathbf{D}\\), where each row (\\(x_n^T\\)) is a token with \\(\\mathbf{D}\\) columns or features. In real tasks, we have many such sequences (for example, many text passages,  with each word represented as one token). </p> <p>A fundamental building block of the transformer is the transformer layer, a function that takes \\(\\mathbf{X}\\) as input  and outputs a new matrix \\(\\tilde{\\mathbf{X}}\\) of the same size:</p> \\[ \\tilde{\\mathbf{X}} = \\mathrm{TransformerLayer}[\\mathbf{X}] \\, . \\] <p>By stacking several transformer layers, we obtain a deep network that can learn  rich internal representations. Each transformer layer has its own weights and  biases, which are learned using gradient descent with an appropriate cost function.</p> <p>A single transformer layer has two stages. The first stage implements the attention mechanism, which, for each feature column, forms a weighted sum over all tokens (rows), thereby mixing information between the token vectors. The second stage then acts on each row  independently and further transforms the features within each token vector.</p>"},{"location":"Transformers-1-Introduction/#attention-coefficients","title":"Attention coefficients","text":"<p>Suppose we have a set of input token vectors (rows)</p> \\[ \\mathbf{x}_1, \\dots, \\mathbf{x}_N \\] <p>and we want to map them to a new set of output vectors</p> \\[ \\mathbf{y}_1, \\dots, \\mathbf{y}_N \\] <p>in a new embedding space that captures richer semantic structure.</p> <p>For any particular output vector \\(\\mathbf{y}_n\\), we want it to depend not only on its corresponding input \\(\\mathbf{x}_n\\) but on all input token vectors (rows) \\(\\mathbf{x}_1, \\dots, \\mathbf{x}_N\\). A simple way to achieve this is to define \\(\\mathbf{y}_n\\) as a weighted sum of the inputs:</p> \\[ \\mathbf{y}_n = \\sum_{m=1}^{N} a_{nm}\\,\\mathbf{x}_m, \\] <p>where the coefficients \\(a_{nm}\\) are called attention weights.</p> <p>We require these coefficients to satisfy</p> \\[ a_{nm} \\ge 0 \\quad \\text{for all } m \\] <p>and</p> \\[ \\sum_{m=1}^{N} a_{nm} = 1. \\] <p>These constraints ensure that the weights form a partition of unity, so that each coefficient lies in the range \\(0 \\le a_{nm} \\le 1\\). Thus, each output vector \\(\\mathbf{y}_n\\) is a convex combination (a weighted average) of the input vectors, with some inputs receiving larger weights than others as we wanted.</p> <p>Note that we have a different set of coefficients \\(\\{a_{n1}, \\dots, a_{nN}\\}\\) for each output index \\(n\\), and the above constraints apply separately for each \\(n\\). The coefficients \\(a_{nm}\\) depend on the input data, later we will see how we compute them in practice.</p>"},{"location":"Transformers-1-Introduction/#self-attention","title":"Self attention","text":"<p>We wish to determine the coefficients \\(a_{nm}\\) used in</p> \\[ \\mathbf{y}_n = \\sum_{m=1}^{N} a_{nm}\\,\\mathbf{x}_m. \\] <p>Query, Key and Value analogy. In information retrieval (e.g. a movie streaming service), each movie is described by an attribute vector called a key, while the movie file itself is a value. A user specifies their preferences as a query vector. The system compares the query with all keys, finds the best match, and returns the corresponding value. Focusing on a single best-matching movie would be called hard attention.</p> <p>In transformers we use soft attention: instead of returning a single value, we compute continuous weights that tell us how strongly each value should influence the output. This keeps the whole mapping differentiable, so it can be trained by gradient descent.</p> <p>Applying this to tokens. For each input token we start from its embedding \\(\\mathbf{x}_n\\), but we use three conceptually different copies of it (in this simple version we set \\(\\mathbf{q}_n = \\mathbf{k}_n = \\mathbf{v}_n = \\mathbf{x}_n\\) for all \\(n\\)).</p> <ul> <li>Value \\(\\mathbf{v}_n\\) (movie file): the actual content to return or mix into the output.</li> <li>Key \\(\\mathbf{k}_n\\) (movie's attribute profile): a summary describing that movie (genre, actors, length) used for matching.</li> <li>Query \\(\\mathbf{q}_n\\) (user's wish list of attributes): what the output position is looking for and this is compared against all keys.</li> </ul> <p>To decide how much the output at position \\(n\\) should attend to token \\(m\\), we measure the similarity between the corresponding query and key vectors.  A simple similarity measure is the dot product</p> \\[ \\mathbf{x}_n^\\top \\mathbf{x}_m. \\] <p>Attention weights via softmax. To enforce the constraints</p> \\[ a_{nm} \\ge 0, \\qquad \\sum_{m=1}^{N} a_{nm} = 1 \\quad \\text{for each fixed } n, \\] <p>we define the attention weights by a softmax over \\(m\\):</p> \\[ a_{nm} = \\frac{\\exp\\big(\\mathbf{x}_n^\\top \\mathbf{x}_m\\big)}        {\\sum_{m'=1}^{N} \\exp\\big(\\mathbf{x}_n^\\top \\mathbf{x}_{m'}\\big)}. \\] <p>Thus, for each \\(n\\), the row \\((a_{n1},\\dots,a_{nN})\\) forms a set of non-negative coefficients that sum to one, assigning larger weights to inputs whose keys are more similar to the query.</p> <p>Matrix form and self-attention. Let \\(\\mathbf{X} \\in \\mathbb{R}^{N \\times D}\\) be the input matrix whose \\(n\\)-th row is \\(\\mathbf{x}_n\\), and let \\(\\mathbf{Y} \\in \\mathbb{R}^{N \\times D}\\) be the output matrix whose \\(n\\)-th row is \\(\\mathbf{y}_n\\). We first compute all pairwise dot products:</p> \\[ \\mathbf{L} = \\mathbf{X}\\mathbf{X}^\\top \\in \\mathbb{R}^{N \\times N}. \\] <p>We then apply the softmax row-wise to obtain the attention matrix</p> \\[ \\mathbf{A} = \\mathrm{Softmax}[\\mathbf{X}\\mathbf{X}^\\top], \\] <p>and finally compute the outputs as</p> \\[ \\mathbf{Y} = \\mathbf{A}\\mathbf{X}            = \\mathrm{Softmax}[\\mathbf{X}\\mathbf{X}^\\top]\\,\\mathbf{X}. \\] <p>Because the queries, keys, and values are all derived from the same sequence \\(\\mathbf{X}\\), this mechanism is known as self-attention, and since the similarity is given by a dot product, it is specifically dot-product self-attention.</p>"},{"location":"Transformers-1-Introduction/#network-parameters","title":"Network parameters","text":"<p>We currently have two problems: - Vanilla self-attention contains no trainable parameters - Treat all feature values within a token equally in determining the attention coefficients.</p> <p>Introducing a single projection (learnable weight matrix) \\(\\mathbf{U}\\in\\mathbb{R}^{D\\times D}\\),</p> \\[ \\tilde{\\mathbf{X}}=\\mathbf{X}\\mathbf{U},\\qquad \\mathbf{Y}=\\mathrm{Softmax}\\!\\big[\\mathbf{X}\\mathbf{U}\\mathbf{U}^\\top\\mathbf{X}^\\top\\big]\\;\\mathbf{X}\\mathbf{U}, \\] <p>adds learnability but yields a symmetric score matrix \\(\\mathbf{X}\\mathbf{U}\\mathbf{U}^\\top\\mathbf{X}^\\top\\) and ties the value and similarity parameters.</p> <p>To obtain a more flexible, asymmetric mechanism (for example, bat should be strongly associated with animal but animal should not be strongly associated with bat as there are different kinds of animals), therefore, we use independent projections for queries, keys, and values:</p> \\[ \\mathbf{Q}=\\mathbf{X}\\mathbf{W}^{(q)},\\quad \\mathbf{K}=\\mathbf{X}\\mathbf{W}^{(k)},\\quad \\mathbf{V}=\\mathbf{X}\\mathbf{W}^{(v)}, \\] <p>with linear trasnformations \\(\\mathbf{W}^{(q)},\\mathbf{W}^{(k)}\\in\\mathbb{R}^{D\\times D_k}\\) and \\(\\mathbf{W}^{(v)}\\in\\mathbb{R}^{D\\times D_v}\\) (typically \\(D_k=D\\), \\(D_v=D\\) as this helps to stack multiple Transformer layers on top of each other). The resulting dot-product self-attention is</p> \\[ \\mathbf{Y}=\\mathrm{Softmax}\\!\\big[\\mathbf{Q}\\mathbf{K}^\\top\\big]\\;\\mathbf{V}, \\] <p>with \\(\\mathbf{Q}\\mathbf{K}^{T}\\in\\mathbb{R}^{N\\times N}\\) and \\(\\mathbf{Y}\\in\\mathbb{R}^{N\\times D_v}\\), which is trainable, reweighs features, and supports asymmetric token relationships.</p> <p>Bias absorption. Add a column of ones to the data and a row for biases to the weights, so that</p> \\[ XW + \\mathbf{1} b^\\top = \\underbrace{\\big[\\,X\\;\\;\\mathbf{1}\\,\\big]}_{X_{\\text{aug}}} \\underbrace{\\begin{bmatrix} W \\\\ b^\\top \\end{bmatrix}}_{W_{\\text{aug}}}. \\] <p>Hence, biases can be treated as implicit via augmentation.</p> <p>Fixed vs data dependent weights - Simple NN vs Transformer. For a simple NN let a single input vector be</p> \\[ \\mathbf{x}\\in\\mathbb{R}^{D_{\\text{in}}} \\quad\\text{and the layer have } D_{\\text{out}} \\text{ output units.} \\] <p>The layer has a weight matrix \\(W\\in\\mathbb{R}^{D_{\\text{in}}\\times D_{\\text{out}}}\\). The output is</p> \\[ \\mathbf{y}=\\mathbf{x}W \\quad(\\text{or } \\mathbf{y}=\\mathbf{x}W). \\] <p>Component-wise, for each output unit \\(n\\in\\{1,\\dots,D_{\\text{out}}\\}\\),</p> \\[ y_n=\\sum_{m=1}^{D_{\\text{in}}} x_m\\,W_{m n}. \\] <p>So each output unit is a weighted sum of all input features. But these weights \\(W_{mn}\\) are fix for all inputs.</p> <p>Transformers data dependent weights. In the standard layer above, once training is done, the weights \\(W_{mn}\\) are fixed. For any new input \\(\\mathbf{x}\\), the contribution of input feature \\(m\\) to output \\(n\\) is always \\(x_m\\,W_{mn}\\).</p> <p>In attention, by contrast, the mixing coefficients are computed from the current input. With queries, keys, and values \\(\\,Q=XW^{(q)},\\,K=XW^{(k)},\\,V=XW^{(v)}\\). Therefore, Q,K and V will be different for different inputs.</p> \\[ Y = \\underbrace{\\mathrm{Softmax}\\!\\big(QK^\\top\\big)}_{\\displaystyle A(X)} \\,V, \\qquad \\mathbf{y}_n=\\sum_{m=1}^{N} a_{nm}(X)\\,\\mathbf{v}_m, \\] <p>and the \u201cweights\u201d \\(a_{nm}(X)\\) depend on the present data (via a softmax over dot products).Thus, the contribution from token \\(m\\) to output \\(n\\) can be nearly zero for one input and large for another a behavior that a standard fixed-weight layer cannot achieve.</p>"},{"location":"Transformers-1-Introduction/#scaled-self-attention","title":"Scaled self attention","text":"<p>Issue. Softmax gradients shrink when its inputs (logits) are large in magnitude (saturation). In dot-product attention the logits are \\(\\ell_{ij}=\\mathbf{q}_i^\\top\\mathbf{k}_j\\), which can grow with vector dimension.</p> <p>Why do they grow? Assume (as a scale reference) that query/key components are independent with mean \\(0\\) and variance \\(1\\): \\(\\mathbf{q}=(q_1,\\dots,q_{D_k})\\), \\(\\mathbf{k}=(k_1,\\dots,k_{D_k})\\). Then</p> \\[ \\mathbf{q}^\\top\\mathbf{k}=\\sum_{t=1}^{D_k} q_t k_t, \\qquad \\mathbb{E}[q_t k_t]=0,\\quad \\mathrm{Var}(q_t k_t)=\\mathbb{E}[q_t^2]\\mathbb{E}[k_t^2]=1\\cdot 1=1, \\] <p>so by independence,</p> \\[ \\mathrm{Var}(\\mathbf{q}^\\top\\mathbf{k}) =\\sum_{t=1}^{D_k}\\mathrm{Var}(q_t k_t)=D_k, \\] <p>and the typical magnitude (std. dev.) is \\(\\sqrt{D_k}\\). Larger \\(D_k\\) therefore pushes logits to larger scales, sharpening the softmax and shrinking gradients.</p> <p>Fix. Normalize logits by their standard deviation:</p> \\[ \\tilde{\\ell}_{ij}=\\frac{\\mathbf{q}_i^\\top\\mathbf{k}_j}{\\sqrt{D_k}}, \\] <p>which makes \\(\\mathrm{Var}(\\tilde{\\ell}_{ij})\\approx 1\\) under the reference assumption, keeping softmax in a stable range.</p> <p>Result. The attention layer is</p> \\[ \\mathbf{Y} = Attention(\\mathbf{Q},\\mathbf{K},\\mathbf{V}) =\\mathrm{Softmax}\\!\\left(\\frac{\\mathbf{Q}\\mathbf{K}^\\top}{\\sqrt{D_k}}\\right)\\mathbf{V}. \\] <p>This is scaled dot-product self-attention. (Even when the independence/variance assumptions are only approximate, this scaling acts like a principled temperature that stabilizes training.)</p>"},{"location":"Transformers-1-Introduction/#multi-head-attention","title":"Multi head attention","text":"<p>A single head can average out distinct patterns. Therefore, we use \\(H\\) parallel heads with separate parameters to attend to different patterns for example in NLP it can be tenses, vocabulary, etc.</p> <p>Setup. Let \\(X\\!\\in\\!\\mathbb{R}^{N\\times D}\\). Each head \\(h\\in\\{1,\\dots,H\\}\\) has its own:</p> \\[ Q_h = X W^{(q)}_h,\\qquad K_h = X W^{(k)}_h,\\qquad V_h = X W^{(v)}_h, \\] <p>with \\(W^{(q)}_h,W^{(k)}_h\\!\\in\\!\\mathbb{R}^{D\\times D_k}\\) and \\(W^{(v)}_h\\!\\in\\!\\mathbb{R}^{D\\times D_v}\\).</p> <p>Per-head attention (scaled).</p> \\[ \\mathbf{H}_h \\;=\\; Attention(\\mathbf{Q_h}, \\mathbf{K_h},\\mathbf{V_h}) \\;=\\; \\mathrm{Softmax}\\!\\left(\\frac{Q_h K_h^\\top}{\\sqrt{D_k}}\\right) V_h \\;\\in\\;\\mathbb{R}^{N\\times D_v}. \\] <p>Combine heads. Concatenate along features to get shape \\((N \\times H D_v)\\) and project:</p> \\[ Y(X) \\;=\\; \\mathrm{Concat}\\,[H_1,\\dots,H_H]\\; W^{(o)},\\qquad W^{(o)} \\in \\mathbb{R}^{H D_v \\times D }\\text{ is a trainable linear matrix: }, \\] <p>so \\(Y\\in\\mathbb{R}^{N\\times D}\\) matches the input width. A common choice is \\(D_k=D_v=D/H\\), making the concatenated matrix \\(N\\times D\\).</p> <p>Redundancy observed. It is due to the reparameterization on the value path. We know for each head,</p> \\[ H_h=\\mathrm{Softmax}\\!\\Big(\\tfrac{Q_h K_h^\\top}{\\sqrt{D_k}}\\Big)\\;V_h, \\qquad V_h = X W^{(v)}_h, \\] <p>and the final combine is</p> \\[ Y=\\mathrm{Concat}[H_1,\\dots,H_H]\\;W^{(o)}. \\] <p>We can write \\(W^{(o)}=\\big[(W^{(o)}_1)^\\top\\;\\cdots\\;(W^{(o)}_H)^\\top\\big]^\\top\\). Then</p> \\[ Y=\\sum_{h=1}^H H_h\\,W^{(o)}_h   =\\sum_{h=1}^H \\mathrm{Softmax}\\!\\Big(\\tfrac{Q_h K_h^\\top}{\\sqrt{D_k}}\\Big)\\;\\underbrace{(X W^{(v)}_h W^{(o)}_h)}_{X\\,\\tilde W^{(v)}_h}. \\] <p>Then we can define \\(\\tilde W^{(v)}_h := W^{(v)}_h W^{(o)}_h\\). The layer becomes</p> \\[ Y=\\sum_{h=1}^H \\mathrm{Softmax}\\!\\Big(\\tfrac{Q_h K_h^\\top}{\\sqrt{D_k}}\\Big)\\;X\\,\\tilde W^{(v)}_h, \\] <p>with no explicit \\(W^{(o)}\\).</p> <p>Therefore the two consecutive linear maps on \\(V\\) (first \\(W^{(v)}_h\\), then the head\u2019s block \\(W^{(o)}_h\\)) can always be merged into a single matrix \\(\\tilde W^{(v)}_h\\). Since the attention weights use only \\(Q\\) and \\(K\\), the value path is purely linear per head: \\(V_h W^{(o)}_h = X\\,W^{(v)}_h W^{(o)}_h\\). Thus we can collapse the two matrices into one \\(\\tilde W^{(v)}_h := W^{(v)}_h W^{(o)}_h\\). This gives two equivalent parameterizations:</p> \\[ \\text{(separate)}\\;\\; (W^{(v)}_h,\\,W^{(o)}_h) \\quad\\longleftrightarrow\\quad \\text{(collapsed)}\\;\\; \\tilde W^{(v)}_h. \\] <p>This non-uniqueness is the redundancy. we keep two matrices even though one combined matrix suffices to represent exactly the same function.</p> <p>Why keep \\(W^{(o)}\\) in practice? It standardizes the output width \\(D\\), keeps per-head value sizes \\(D_v\\) small, and matches common implementations,  but representationally, only the product \\(W^{(v)}_h W^{(o)}_h\\) matters.</p>"},{"location":"Transformers-1-Introduction/#transformer-layers","title":"Transformer layers","text":"<p>The input tokens have the shape:  \\((X\\in\\mathbb{R}^{N\\times D})\\) (where rows are the tokens).</p> <p>Multi-head attention (MHA). For heads \\((h=1,\\dots,H)\\),</p> \\[ Q_h=XW^{(q)}_h,\\quad K_h=XW^{(k)}_h,\\quad V_h=XW^{(v)}_h, \\] \\[ H_h=\\mathrm{Softmax}\\!\\Big(\\tfrac{Q_h K_h^\\top}{\\sqrt{D_k}}\\Big)\\,V_h\\in\\mathbb{R}^{N\\times D_v}. \\] <p>Concatenate and project:</p> \\[ \\mathrm{MHA}(X)=\\mathrm{Concat}[H_1,\\dots,H_H]\\;W^{(o)},\\qquad W^{(o)}\\in\\mathbb{R}^{H D_v\\times D}. \\] <p>The output from MHA layer has the same shape as its input of \\((N \\times D)\\) enabling residuals. MHA gives data-dependent mixing between tokens and helps learn different relations (e.g., syntax vs. semantics) in parallel.</p> <p>Residual + LayerNorm (two variants). Residuals preserve the input signal and enable deep stacks. In addition to this, pre/post norm improve optimization stability (pre-norm is common for very deep models).</p> <p>Post-norm:</p> \\[ Z=\\mathrm{LayerNorm}\\big(\\mathrm{MHA}(X)+X\\big). \\] <p>Pre-norm:</p> \\[ Z=X+\\mathrm{MHA}(\\mathrm{LayerNorm}(X)). \\] <p>Position-wise MLP (shared across tokens). MHA outputs lie in the span of inputs due to linear mixing, the MLP adds nonlinearity and feature-wise transformation per token, boosting expressiveness. A two-layer example with activation \\(\\phi\\) can be denoted as:</p> \\[ \\mathrm{MLP}(U)=\\phi(UW_1+b_1)\\,W_2+b_2,\\quad W_1\\in\\mathbb{R}^{D\\times D_{\\mathrm{ff}}},\\;\\;W_2\\in\\mathbb{R}^{D_{\\mathrm{ff}}\\times D}. \\] <p>Block output (two variants).</p> <p>Post-norm:</p> \\[ \\tilde X=\\mathrm{LayerNorm}\\big(\\mathrm{MLP}(Z)+Z\\big). \\] <p>Pre-norm:</p> \\[ \\tilde X=Z+\\mathrm{MLP}(\\mathrm{LayerNorm}(Z)). \\] <p>Layer normalization is generally used in both sublayers (MHA and MLP) to normalize per token to reduce covariate shift and to keep activations in a well-scaled regime to keep the training process steady.</p> <p>Stacking. Repeat the block \\(L\\) times to form a deep transformer. Note that all mappings preserve shape \\(N\\times D\\), enabling residuals.</p>"},{"location":"Transformers-1-Introduction/#computational-complexity","title":"Computational complexity","text":"<p>Setup. Input \\(X\\in\\mathbb{R}^{N\\times D}\\) (rows = tokens with \\(D\\) features). A transformer block outputs the same shape \\(N\\times D\\) as its input. We use \\(H\\) heads and for each head key/query/value widths (features) are \\(d_k,d_k,d_v\\) (typically \\(d_k=d_v=D/H\\)).</p> <p>Baseline: fully connected on flattened sequence Flatten \\(X\\) to a vector in \\(\\mathbb{R}^{ND}\\) and map to \\(\\mathbb{R}^{ND}\\) with weight matrix \\(W\\in\\mathbb{R}^{ND\\times ND}\\).</p> \\[ \\text{parameters}= \\text{elements in the weight matrix = }N^2D^2,\\qquad \\text{ FLOPs }\\approx  2\\,N^2D^2. \\] <p>Where, FLOPs mean Floating-point Operations.</p> <p>Multi-head self-attention (MHA)</p> <p>1) Linear projections to \\(Q,K,V\\). For each head \\(h\\):</p> \\[ Q_h=XW^{(q)}_h,\\quad K_h=XW^{(k)}_h,\\quad V_h=XW^{(v)}_h. \\] <p>with \\(W^{(q)}_h,W^{(k)}_h\\in\\mathbb{R}^{D\\times d_k}\\), \\(W^{(v)}_h\\in\\mathbb{R}^{D\\times d_v}\\).</p> \\[ \\begin{aligned} \\text{Shapes: }&amp; Q_h,K_h\\in\\mathbb{R}^{N\\times d_k},\\; V_h\\in\\mathbb{R}^{N\\times d_v}. \\end{aligned} \\] \\[ \\begin{aligned} \\text{FLOPs: }&amp; \\underbrace{N D d_k}_{XW^{(q)}_h} +\\underbrace{N D d_k}_{XW^{(k)}_h} +\\underbrace{N D d_v}_{XW^{(v)}_h} = N D\\,(2 d_k + d_v)\\;\\text{ per head}. \\end{aligned} \\] \\[ \\begin{aligned} &amp;\\Rightarrow\\;\\text{Total FLOPs (all \\(H\\) heads)} = N D H\\,(2 d_k + d_v). \\\\ &amp;\\text{If } d_k = d_v = D/H,\\ \\text{then this simplifies to } 3 N D^2. \\end{aligned} \\] \\[ \\text{parameters (projections)}=D(2H d_k + H d_v) = 3D^2. \\] <p>2) Attention scores (scaled dot products). For each head:</p> \\[ S_h=\\frac{Q_h K_h^\\top}{\\sqrt{d_k}}\\in\\mathbb{R}^{N\\times N}. \\] \\[ \\text{FLOPs: }N^2 d_k \\text{ (matrix multiply)}\\quad (\\text{the divide by }\\sqrt{d_k}\\text{ is }O(N^2)). \\] <p>Softmax over rows:</p> \\[ A_h=\\mathrm{Softmax}(S_h)\\in\\mathbb{R}^{N\\times N},\\qquad \\text{FLOPs: }O(N^2). \\] <p>Total FLOPs (all heads): \\(H N^2 d_k\\) for \\(QK^\\top\\) and \\(O(H N^2)\\) for softmax and parameters = 0.</p> <p>3) Value mixing. For each head:</p> \\[ H_h=A_h V_h\\in\\mathbb{R}^{N\\times d_v},\\qquad \\text{FLOPs: }N^2 d_v = \\frac{N^2D}{H}  \\text{ (matrix multiply)}, \\qquad \\text{parameters}=0 \\] <p>4) Concatenate and output projection. Concatenate \\(H_h\\) along features:</p> \\[ H=\\mathrm{Concat}[H_1,\\dots,H_H]\\in\\mathbb{R}^{N\\times (H d_v)}. \\] <p>Project to width \\(D\\):</p> \\[ Y_{\\text{attn}}=H W^{(o)},\\quad W^{(o)}\\in\\mathbb{R}^{H d_v\\times D}. \\] \\[ \\text{FLOPs: }N\\,(H d_v)\\,D,\\qquad \\text{parameters (output proj)}=(H d_v)D. \\] <p>Common setting \\(d_k=d_v=D/H\\). Then</p> \\[ \\begin{aligned} \\text{Parameters (MHA)} &amp;= D(2H\\cdot \\tfrac{D}{H} + H\\cdot \\tfrac{D}{H}) + 0 + 0 + (H\\tfrac{D}{H})D = 4D^2,\\\\ \\text{FLOPs (MHA)} &amp;= \\underbrace{N D (2H d_k + H d_v)}_{=\\,3ND^2} +\\underbrace{H N^2 d_k}_{=\\,N^2 D} +\\underbrace{H N^2 d_v}_{=\\,N^2 D} +\\underbrace{N(H d_v)D}_{=\\,N D^2}\\\\ &amp;= \\boxed{2N^2 D + 4 N D^2}\\quad(\\text{softmax adds }O(N^2)). \\end{aligned} \\] <p>Residual adds \\(X+Y_{\\text{attn}}\\): elementwise add, \\(\\;\\text{FLOPs}=N D, \\qquad \\#\\text{parmas} = 0 \\)</p> <p>Layer normalization Given an input token (row) \\(x \\in \\mathbb{R}^D\\), LayerNorm computes</p> \\[ \\mu = \\frac{1}{D} \\sum_{i=1}^D x_i,  \\qquad \\sigma^2 = \\frac{1}{D} \\sum_{i=1}^D (x_i - \\mu)^2, \\] \\[ \\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\varepsilon}}, \\qquad \\text{final output is given by: } y_i = \\gamma_i \\hat{x}_i + \\beta_i, \\quad i = 1,\\dots,D, \\] <p>where \\(\\gamma, \\beta \\in \\mathbb{R}^D\\) are learnable scale and bias.</p> <p>For a batch of \\(N\\) tokens (matrix \\(X \\in \\mathbb{R}^{N \\times D}\\)), FLOPs (in forward pass):</p> \\[ \\text{FLOPs} \\approx c \\, N D \\] <p>for a small constant \\(c\\) (mean/var + normalize + scale/shift).</p> <p>Parameters per LayerNorm:</p> \\[ \\text{parameters} = 2D \\quad (\\gamma,\\beta). \\] <p>Position-wise MLP (shared across tokens) Two-layer MLP with hidden width \\(D_{\\text{ff}}\\):</p> \\[ \\mathrm{MLP}(U)=\\phi(UW_1+b_1)\\,W_2+b_2,\\quad W_1\\in\\mathbb{R}^{D\\times D_{\\text{ff}}},\\;W_2\\in\\mathbb{R}^{D_{\\text{ff}}\\times D}. \\] \\[ \\text{parameters for }W_1, W_2, b_1, b_2 = D D_{\\text{ff}} + D_{\\text{ff}} D + D_{\\text{ff}} + D \\approx 2 D D_{\\text{ff}}. \\] <p>FLOPs. The two matrix multiplications dominate: \\(\\;UW_1: (N\\times D)(D\\times D_{\\text{ff}})\\) and \\((\\cdot)W_2: (N\\times D_{\\text{ff}})(D_{\\text{ff}}\\times D)\\), each costing \\(\\approx  N D D_{\\text{ff}}\\) FLOPs, so in total \\(\\text{FLOPs} \\approx 2 N D D_{\\text{ff}}\\).</p> <p>Common choice \\(D_{\\text{ff}}=cD\\) (e.g.\\(c{=}4\\)) gives \\(\\text{parameters}\\approx 2c D^2\\) and \\(\\text{FLOPs}=2c N D^2\\).</p> <p>Block totals (one transformer block, pre-/post-norm similar) Ignoring small \\(ND\\) terms from residuals/LayerNorm:</p> \\[ \\boxed{ \\text{FLOPs} \\;\\approx\\; \\underbrace{2 N^2 D}_{\\text{attention mixes}} \\;+\\; \\underbrace{4 N D^2}_{\\text{QKV+out proj}} \\;+\\; \\underbrace{2c N D^2}_{\\text{MLP}} } \\] \\[ \\text{FLOPs} \\;\\approx\\; 2 N^2 D \\;+\\; (4+2c)\\,N D^2. \\] \\[ \\boxed{ \\text{parameters} \\;\\approx\\; \\underbrace{4 D^2}_{\\text{MHA}} \\;+\\; \\underbrace{2c D^2}_{\\text{MLP}} \\;+\\; \\underbrace{4D}_{\\text{two LayerNorms}} } \\] <p>When does which term dominate? Attention dominates for long sequences (\\(N\\gg D\\)) since it scales as \\(N^2 D\\). The MLP dominates for wide models (\\(D\\gg N\\)) since it scales as \\(N D^2\\). Compared to a dense \\(\\mathbb{R}^{ND}\\!\\to\\!\\mathbb{R}^{ND}\\) layer (\\(\\Theta(N^2D^2)\\) params/FLOPs), a transformer block is vastly more efficient.</p>"},{"location":"Transformers-1-Introduction/#positional-encoding","title":"Positional encoding","text":"<p>Why we need it. Transformers have a really cool property, since it shares \\((W_h^{(q)},W_h^{(k)},W_h^{(v)})\\) across input tokens and applies the same computations to every row of \\(X\\in\\mathbb{R}^{N\\times D}\\). This makes permuting the input rows results in the same permutation of the rows of the output matrix (permutation equivariance). Since parameters are shared across inputs, it gives two major benefits to the network, firstly it makes the computation parallel, and secondly it makes long range dependencies just as effective as the short range ones. This is because the same weight matrices (for attention, feed-forward layers, etc.) are shared across all tokens in the sequence, the model doesn\u2019t need different parameters for each position or word. This means every token can be run through the same computations at the same time, letting GPUs/TPUs process all tokens in parallel instead of one after another. But for sequences (language, etc.), order matters, so we need to inject positional information into the data. Since we want to keep the two nice properties of our attention layers discussed above, we\u2019d rather encode token order directly in the input representations, instead of baking it into the network architecture itself.</p> <p>Additive positional encoding. Associate each position \\(n\\) with a vector \\(r_n\\in\\mathbb{R}^{D}\\) as each token has \\(D\\) dimensional features and add it to the token embedding \\(x_n\\):</p> \\[ \\tilde x_n = x_n + r_n \\quad(=\\text{row } n \\text{ of } \\tilde X). \\] <p>This might seem counter-intuitive, as this might mess up the input vector, but in reality this works really well. As in high-dimensional spaces, two unrelated vectors are almost orthogonal, so the model can keep token identity and position information relatively separate even when they\u2019re added. Residual connections across layers help preserve this position information as it flows through the network. And because the layers are mostly linear, using addition of token and position embeddings behaves a lot like concatenating them and then applying a linear layer (add then linear is a special case of concatenation then linear where the bigger weight matrix is a concatenation of two same smaller weight matrices). This also preserves the model\u2019s width \\(D\\) (concatenation would increase cost).</p> <p>Example The simplest example for this is \\(r_n=\\{1,2,3,\\cdots\\}\\). But in this case the magnitude can get very high and start corrupting the input. In addition, it may not generalize well to new input sequences that are longer than samples in training dataset.</p> <p>Design goals. A good positional code should be: (i) unique per position, (ii) bounded, (iii) generalize to longer sequences, and (iv) support relative offsets. Therefore, a good example of it can be positional encoding between (0,1).</p> <p>Learned Encoding Another common way to add position information is with learned positional encodings. Here, each position in the sequence gets its own trainable vector, learned together with the rest of the model instead of being hand-designed. Because these position vectors are not shared across positions, permuting the tokens changes their positions and thus breaks permutation invariance, which is exactly what we want from a positional encoding. But the downside is that this scheme doesn\u2019t naturally generalize to positions beyond those seen in training, meaning newer positions just have untrained embeddings. So, learned positional encoding are mainly a good fit when sequence lengths stay roughly the same during training and inference.</p>"},{"location":"Transformers-1-Introduction/#references","title":"References","text":"<ul> <li>Bishop, C. M., &amp; Bishop, H. (2023). Transformers. In Deep Learning: Foundations and Concepts (pp. 357-406). Cham: Springer International Publishing.</li> </ul>"},{"location":"Transformers-2-NLP/","title":"Natural Language Processing","text":""},{"location":"Transformers-2-NLP/#introduction","title":"Introduction","text":"<p>Transformers can process language data made of words, sentences, and paragraphs. They were first developed for text, but are now state-of-the-art models for many other kinds of input data. Many languages, such as English, are sequences of words separated by white space, together with punctuation symbols making them sequential data. For now we focus only on the words and ignore punctuation.</p> <p>Representation. To use words in a deep neural network, we must first turn each word into numbers. A simple method is to choose a fixed dictionary (vocabulary) of words and represent each word by a vector whose length equals the dictionary size. We then use a one-hot representation: the \\(\\,k\\,\\)th word in the dictionary has a vector with a 1 in position \\(k\\) and 0 in all other positions. For example, if aardwolf is the third word in the dictionary, its vector is \\((0, 0, 1, 0, \\ldots, 0)\\).</p> <p>Two issues. This one-hot scheme has two main problems. First, a realistic dictionary may have several hundred thousand words, so the vectors are very high dimensional. Second, these vectors do not express any similarity or relationship between words. </p>"},{"location":"Transformers-2-NLP/#word-embedding","title":"Word embedding","text":"<p>We address both the above issues using word embeddings, which map each word into a dense vector in a lower-dimensional space, typically of a few hundred dimensions. The embedding process uses a matrix \\(\\mathbf{E}\\) of size \\(D \\times K\\), where \\(D\\) is the dimensionality of the embedding space and \\(K\\) is the size of the dictionary. For each one-hot encoded input vector \\(\\mathbf{x}_n\\) of shape \\(K \\times 1\\), we compute the embedding vector as:</p> \\[ \\mathbf{v}_n = \\mathbf{E}\\mathbf{x}_n . \\] <p>Here, \\(\\mathbf{v}_n \\in \\mathbb{R}^D\\) is the dense embedding (or distributed representation) of the \\(n\\)-th word in the vocabulary. In particular, words that appear in similar contexts in the corpus tend to have embedding vectors that are close to each other in this \\(D\\)-dimensional space, so geometric relationships between vectors reflect semantic similarity. Because \\(\\mathbf{x}_n\\) is one-hot, \\(\\mathbf{v}_n\\) of shape \\(D \\times 1\\) is simply the corresponding column of \\(\\mathbf{E}\\). We learn this \\(\\mathbf{E}\\) from a corpus (a large data set) of text.</p>"},{"location":"Transformers-2-NLP/#methods-for-learning-word2vec","title":"Methods for learning: Word2vec","text":"<p>A simple two-layer neural network. The training set is built by taking a ``window'' of \\(M\\) adjacent words in the text, with a typical value \\(M = 5\\). Each window gives one training sample. The samples are treated as independent, and the overall error is the sum of the error terms for all samples.</p> <p>There are two variants. In continuous bag of words (CBOW), the target to be predicted is the middle word, and the remaining context words are the inputs, so the network is trained to 'fill in the blank'. In the skip-gram model, the roles are reversed: the centre word is the input and the context words are the targets.</p> <p>This training can be viewed as self-supervised learning. The data is a large corpus of unlabelled text, from which many small windows of word sequences are sampled at random. The labels come from the text itself by masking the word whose value the network should predict. After training, the embedding matrix \\(\\mathbf{E}\\) is obtained from the network weights: it is the transpose of the second-layer weight matrix for the CBOW model, and the first-layer weight matrix for the skip-gram model. Here is a compact derivation for both CBOW and skip-gram.</p> <p>Embedding matrix in CBOW and Skip gram</p> <p>Setup</p> <p>Let</p> <ul> <li>\\(K\\) = vocabulary size,</li> <li>\\(D\\) = embedding dimension,</li> <li>\\(\\mathbf{x}(w) \\in \\mathbb{R}^K\\) = one-hot input vector for word \\(w\\),</li> <li>\\(\\mathbf{E} \\in \\mathbb{R}^{D \\times K}\\) = embedding matrix,</li> <li>\\(\\mathbf{v}(w) = \\mathbf{E}\\,\\mathbf{x}(w)\\) = embedding of word \\(w\\) (a column of \\(\\mathbf{E}\\)).</li> </ul>"},{"location":"Transformers-2-NLP/#cbow","title":"CBOW","text":"<p>We predict a target word \\(w_t\\) from its \\(M\\) context words.</p> <p>Architecture</p> <p>(i) First layer (input \\(\\to\\) hidden).</p> <p>Weight matrix \\(\\mathbf{W}^{(1)} \\in \\mathbb{R}^{D \\times K}\\). For one context word \\(w\\),</p> \\[ \\mathbf{h}(w) = \\mathbf{W}^{(1)} \\mathbf{x}(w). \\] <p>For a window of \\(M\\) context words \\(w_1,\\dots,w_M\\), CBOW uses the average</p> \\[ \\mathbf{h} = \\frac{1}{M} \\sum_{i=1}^M \\mathbf{W}^{(1)} \\mathbf{x}(w_i). \\] <p>(ii) Second layer (hidden \\(\\to\\) output).</p> <p>Weight matrix \\(\\mathbf{W}^{(2)} \\in \\mathbb{R}^{K \\times D}\\). Let \\(\\mathbf{w}^{(2)}_k{}^{\\!\\top}\\) be the \\(k\\)th row of \\(\\mathbf{W}^{(2)}\\).    The logit for predicting word \\(k\\) is</p> \\[ z_k = \\mathbf{w}^{(2)}_k{}^{\\!\\top} \\mathbf{h}, \\] <p>and the output distribution is</p> \\[ p(k \\mid \\text{context}) =   \\frac{\\exp(z_k)}{\\sum_{j=1}^K \\exp(z_j)}. \\] <p>Training adjusts \\(\\mathbf{W}^{(1)}\\) and \\(\\mathbf{W}^{(2)}\\) so that the true target word \\(t\\) has high probability.</p> <p>Where is the embedding matrix?</p> <p>In CBOW we have a hidden vector \\(\\mathbf{h}\\) that summarizes the context words. From this context we must decide which vocabulary word best fills the blank (the target word). For each possible target word \\(k\\) we therefore want a score that says \u201chow well does word \\(k\\) fit this context?  or a similarity measure\u201d. A simple way to get such a similarity measure is to compute a dot product between a vector for the context, \\(\\mathbf{h}\\), and a vector attached to word \\(k\\), call it \\(\\mathbf{v}(k)\\).</p> <p>So conceptually we want</p> \\[ z_k = \\mathbf{v}(k)^\\top \\mathbf{h}. \\] <p>Now look at what the network actually does. The last linear layer has weights \\(\\mathbf{W}^{(2)} \\in \\mathbb{R}^{K \\times D}\\) and computes</p> \\[ z_k = \\mathbf{w}^{(2)}_k{}^{\\!\\top} \\mathbf{h}, \\] <p>where \\(\\mathbf{w}^{(2)}_k{}^{\\!\\top}\\) is the \\(k\\)th row of \\(\\mathbf{W}^{(2)}\\).</p> <p>Compare the two expressions for \\(z_k\\). They will be identical for all \\(\\mathbf{h}\\) if we simply define</p> \\[ \\mathbf{v}(k) = \\mathbf{w}^{(2)}_k . \\] <p>Thus the word vector for word \\(k\\) is exactly the \\(k\\)th row of \\(\\mathbf{W}^{(2)}\\). If we stack these word vectors as columns we obtain the embedding matrix</p> \\[ \\mathbf{E} = \\begin{bmatrix} \\mathbf{v}(1) &amp; \\cdots &amp; \\mathbf{v}(K) \\end{bmatrix} = \\begin{bmatrix} \\mathbf{w}^{(2)}_1 &amp; \\cdots &amp; \\mathbf{w}^{(2)}_K \\end{bmatrix} = \\mathbf{W}^{(2)\\top}. \\] <p>So in CBOW the \u201clearned embeddings\u201d are the parameters of the output layer that connect the context representation \\(\\mathbf{h}\\) to each possible target word.</p>"},{"location":"Transformers-2-NLP/#skip-gram","title":"Skip-gram","text":"<p>Lets assume we have a corpus \\(w_1, w_2, \\dots, w_T\\). At each position \\(t\\) we treat \\(w_t\\) as the centre word. With window size \\(M\\) (e.g.\\(M=5\\)), the context of \\(w_t\\) is</p> \\[ w_{t+j} \\quad \\text{for} \\quad j \\in \\{-M,\\dots,-1,1,\\dots,M\\}, \\] <p>restricted to indices in \\(\\{1,\\dots,T\\}\\). For each valid \\(j\\) we form a training pair</p> \\[ (\\text{input } w_t,\\; \\text{target } w_{t+j}), \\] <p>using all such pairs or a random subset we perform the following operations.</p> <p>Architecture</p> <p>(i) First layer (input \\(\\to\\) hidden).</p> <p>Weight matrix \\(\\mathbf{W}^{(1)} \\in \\mathbb{R}^{D \\times K}\\).</p> <p>Input is the one-hot vector \\(\\mathbf{x}(w_t)\\) with shape \\(K \\times 1\\) for the centre word \\(w_t\\).    The hidden vector for the input (center) word is given by:</p> \\[ \\mathbf{h} = \\mathbf{W}^{(1)} \\mathbf{x}(w_t). \\] <p>Because \\(\\mathbf{x}(w_t)\\) has a single 1 and all zeros, \\(\\mathbf{h}\\) is the \\(t\\)-th column    of \\(\\mathbf{W}^{(1)}\\) and has the shape \\(D \\times 1\\). This give a \\(D\\) dimensional representation vector for the input word.</p> <p>(ii) Second layer (hidden \\(\\to\\) output).</p> <p>Weight matrix \\(\\mathbf{W}^{(2)} \\in \\mathbb{R}^{K \\times D}\\).</p> <p>For all context position \\(j\\) we use the same hidden vector    \\(\\mathbf{h} \\in \\mathbb{R}^D\\) and the same \\(\\mathbf{W}^{(2)}\\). Let    \\(\\mathbf{w}^{(2)}_k{}^{\\!\\top}\\) be the \\(k\\)th row of \\(\\mathbf{W}^{(2)}\\),    so \\(\\mathbf{w}^{(2)}_k \\in \\mathbb{R}^D\\) is a feature vector for    candidate word \\(k\\). The logit (a scalar) is</p> \\[ z_k = \\mathbf{w}^{(2)}_k{}^{\\!\\top} \\mathbf{h}, \\] <p>We compute their dot product, so if these two vectors point in a similar direction (high similarity in context),    \\(z_k\\) is large and word \\(k\\) becomes more likely, given the center or target word.</p> <p>Collecting all \\(z_k\\) into \\(\\mathbf{z} \\in \\mathbb{R}^K\\) gives a single    distribution</p> \\[ p(k \\mid w_t) = \\text{softmax}_k(\\mathbf{z}), \\] <p>which depends only on the centre word \\(w_t\\). Each context position \\(j\\)    uses this same distribution but has its own target label \\(w_{t+j}\\).</p> <p>Training adjusts \\(\\mathbf{W}^{(1)}\\) and \\(\\mathbf{W}^{(2)}\\) so that, for every pair \\((w_t, w_{t+j})\\) with \\(j \\in \\{-M,\\dots,-1,1,\\dots,M\\}\\), the probability \\(p(k \\mid w_t)\\) is high when \\(k\\) is the index of the true context word \\(w_{t+j}\\). Thus the model learns a distribution \\(p(\\cdot \\mid w_t)\\) that places high mass on all typical neighbours of \\(w_t\\).</p> <p>What skip-gram actually models</p> <p>Skip-gram does not model</p> \\[ p(w_{t+j} \\mid w_t, j). \\] <p>It models</p> \\[ p(c \\mid w_t), \\] <p>where \\(c\\) is any word appearing within the window around \\(w_t\\). For each centre position \\(t\\) and each valid offset \\(j \\neq 0\\) we create an independent pair</p> \\[ (\\text{input} = w_t,\\; \\text{target} = w_{t+j}). \\] <p>Why this still learns useful embeddings</p> <p>For a fixed centre word \\(w_t\\), the model sees many targets \\(w_{t+j}\\) sampled from words that tend to occur near \\(w_t\\). Gradients move the centre embedding \\(\\mathbf{v}(w_t)\\) closer (dot product) to the output vectors of these neighbours. If two words \\(w\\) and \\(w'\\) share similar sets of neighbours, they receive similar updates and end up with similar embeddings.</p> <p>Note that, the model does not answer \u201cwhat word is at offset \\(j=-2\\)?\u201d. It answers \u201cwhich words are likely to appear near this centre word?\u201d. Skip-gram is built to learn from co-occurrence patterns, not exact positions.</p> <p>Why the same distribution for all \\(j\\) is reasonable</p> <p>Using the same \\(\\mathbf{h}\\) and \\(\\mathbf{W}^{(2)}\\) for all \\(j\\) means</p> \\[ p(\\cdot \\mid w_t) \\text{ is the same for every context position.} \\] <p>This matches the modelling choice: the \\(M\\) context positions are treated as an unordered bag of neighbours. Distance information is discarded; the goal is to capture \u201cwhich words tend to co-occur\u201d, not \u201cwhere they appear\u201d.</p> <p>Where is the embedding matrix now?</p> <p>In skip-gram the network only represents the input word \\(w_t\\) by the hidden vector \\(\\mathbf{h}\\). Thus we define the embedding of \\(w_t\\) as</p> \\[ \\mathbf{v}(w_t) = \\mathbf{h}. \\] <p>The first layer is linear, with weights \\(\\mathbf{W}^{(1)} \\in \\mathbb{R}^{D \\times K}\\) and one-hot input \\(\\mathbf{x}(w_t)\\):</p> \\[ \\mathbf{h} = \\mathbf{W}^{(1)} \\mathbf{x}(w_t). \\] <p>Since \\(\\mathbf{x}(w_t)\\) has a single 1 at position \\(t\\),</p> \\[ \\mathbf{h} = \\mathbf{W}^{(1)}_{:,t}, \\] <p>so</p> \\[ \\mathbf{v}(w_t) = \\mathbf{W}^{(1)}_{:,t}, \\] <p>and all word embeddings are the columns of \\(\\mathbf{W}^{(1)}\\). Therefore</p> \\[ \\mathbf{E} = \\mathbf{W}^{(1)}. \\] <p>Summary</p> <ul> <li> <p>In CBOW, the embedding of a predicted word \\(k\\) is the vector used   to score that word at the output. Those vectors are the rows of   \\(\\mathbf{W}^{(2)}\\), so   \\(\\mathbf{E} = \\mathbf{W}^{(2)\\top}\\).</p> </li> <li> <p>In skip-gram, the embedding of an input word \\(k\\) is the hidden   vector produced by its one-hot input. Those vectors are the columns of   \\(\\mathbf{W}^{(1)}\\), so   \\(\\mathbf{E} = \\mathbf{W}^{(1)}\\).</p> </li> </ul> <p>Words that are semantically related are mapped to nearby positions in the embedding space. This happens because related words tend to occur with similar context words more often than unrelated words. For example, the words 'city' and 'capital' appear more often as context for target words such as 'Paris' or 'London' than for 'orange' or 'polynomial'. The network can then more easily predict the missing words if 'Paris' and 'London' are mapped to nearby embedding vectors.</p> <p>The learned embedding space often has richer semantic structure than simple closeness of related words, and it supports simple vector arithmetic. For example, the relation 'Paris is to France as Rome is to Italy' can be expressed in terms of embedding vectors. Writing \\(\\mathbf{v}(\\text{word})\\) for the embedding of word, we find</p> \\[ \\mathbf{v}(\\text{Paris}) - \\mathbf{v}(\\text{France}) + \\mathbf{v}(\\text{Italy}) \\simeq \\mathbf{v}(\\text{Rome}) \\] <p>Word embeddings were first developed as stand-alone tools for natural language processing. Today they are more often used as pre-processing steps for deep neural networks, and can be viewed as the first layer of such a network. The embedding matrix may be fixed, using some standard pre-trained embeddings, or treated as an adaptive layer that is learned during end-to-end training of the whole system. In the adaptive case, the embedding layer can be initialized with random weights or with a standard pre-trained embedding matrix.</p>"},{"location":"Transformers-2-NLP/#tokenization","title":"Tokenization","text":"<p>A fixed dictionary of whole words has problems. It cannot handle unseen or misspelled words, and it ignores punctuation and other character sequences such as computer code.</p> <p>A pure character-level approach fixes these issues by defining the dictionary as all characters: upper- and lower-case letters, digits, punctuation, and white-space symbols such as spaces and tabs. But this approach throws away explicit word structure. The neural network must then learn words from raw characters, and the sequence becomes much longer, increasing computation.</p> <p>We can combine the advantages of word- and character-level views by adding a pre-processing step called tokenization. This converts the original string of words and punctuation symbols into a string of tokens. Tokens are short character sequences. They may be complete common words, fragments of longer words, or even individual characters, which can be combined to represent rare words. Tokenization naturally handles punctuation, computer code, and other symbol sequences. It can also extend to other modalities such as images. Variants of the same word can share tokens: for example, 'cook', 'cooks', 'cooked', 'cooking', and 'cooker' can all include the token 'cook', so their representations are related.</p> <p>There are many tokenization methods. One important example is byte pair encoding (BPE), originally used for data compression and adapted to text by merging characters instead of bytes. The procedure is:</p> <ol> <li>Start with a token list that contains all individual characters.</li> <li>In a large text corpus, find the most frequent adjacent pair of tokens.</li> <li>Replace each occurrence of this pair with a new, single token.</li> <li>To avoid merging across word boundaries, do not create a new token from    a pair if the second token begins with a white-space character.</li> <li>Repeat the merge steps</li> </ol> <p>Initially, the number of tokens in the vocabulary equals the number of distinct characters, which is small. As merges are applied, the vocabulary size grows. If we continue long enough, the tokens approach whole words. In practice, we fix a maximum vocabulary size in advance as a compromise between character- level and word-level representations and stop the algorithm when this size is reached.</p> <p>In most deep learning applications to natural language, input text is first mapped to a tokenized sequence. However, for the rest of this chapter we will use word-level representations because they make the main ideas easier to present.</p>"},{"location":"Transformers-2-NLP/#bag-of-words","title":"Bag of words","text":"<p>The task is to calculate the likelihood (probability) of a specific sequence of words occurring. We now model the joint distribution \\(p(\\mathbf{x}_1,\\ldots,\\mathbf{x}_N)\\) of this ordered sequence of vectors (words/ token in our case). The simplest assumption is that all words are drawn independently from the same distribution (ignoring the order). Then</p> \\[ p(\\mathbf{x}_1,\\ldots,\\mathbf{x}_N) = \\prod_{n=1}^N p(\\mathbf{x}_n) \\] <p>The distribution \\(p(\\mathbf{x})\\) is the same for all positions or tokens and can be represented as a table of probabilities over the dictionary of words or tokens where each word or token has a fixed probability independent of its position. The maximum-likelihood estimate simply sets each table entry equal to the fraction of times that word occurs in the training set. This is called a bag of words model because it ignores word order completely.</p> <p>We can use the bag-of-words idea to build a simple text classifier. For instance, in sentiment analysis a review is classified as positive or negative. The naive Bayes classifier assumes that, within each class \\(C_k\\), the words are independent, but that each class has its own distribution. Thus</p> \\[ p(\\mathbf{x}_1,\\ldots,\\mathbf{x}_N \\mid C_k) = \\prod_{n=1}^N p(\\mathbf{x}_n \\mid C_k) \\] <p>Now apply Bayes\u2019 rule to get the posterior over the class:</p> \\[ p(C_k \\mid \\mathbf{x}_1,\\ldots,\\mathbf{x}_N) = \\frac{p(\\mathbf{x}_1,\\ldots,\\mathbf{x}_N \\mid C_k)\\,p(C_k)}        {p(\\mathbf{x}_1,\\ldots,\\mathbf{x}_N)}. \\] <p>Substitute the likelihood:</p> \\[ p(C_k \\mid \\mathbf{x}_1,\\ldots,\\mathbf{x}_N) = \\frac{\\Bigl[\\prod_{n=1}^N p(\\mathbf{x}_n \\mid C_k)\\Bigr]\\,p(C_k)}        {p(\\mathbf{x}_1,\\ldots,\\mathbf{x}_N)}. \\] <p>The denominator</p> \\[ p(\\mathbf{x}_1,\\ldots,\\mathbf{x}_N) \\] <p>does not depend on \\(k\\) (it is the same for all classes), so when we compare or normalize over \\(k\\) we can treat it as a constant. Therefore we write</p> \\[ p(C_k \\mid \\mathbf{x}_1,\\ldots,\\mathbf{x}_N) \\propto p(C_k)\\prod_{n=1}^N p(\\mathbf{x}_n \\mid C_k), \\] <p>Both the class-conditional distributions \\(p(\\mathbf{x}\\mid C_k)\\) and the priors \\(p(C_k)\\) can be estimated from training frequencies. For a new sequence, we multiply the corresponding table entries to obtain posterior scores. If a word appears in the test set but never appeared in the training set for a given class, then its estimated probability for that class is zero, and the whole product becomes zero. To avoid this, the probability tables are usually smoothed after training by adding a small amount of probability uniformly across all entries so that no entry is exactly zero.</p>"},{"location":"Transformers-2-NLP/#autoregressive-models","title":"Autoregressive models","text":"<p>A major limitation of the bag-of-words model is that it ignores word order. To include order we use an autoregressive factorization. Without loss of generality we can write the joint distribution over a sequence as</p> \\[ p(\\mathbf{x}_1,\\ldots,\\mathbf{x}_N) = \\prod_{n=1}^N p(\\mathbf{x}_n \\mid \\mathbf{x}_1,\\ldots,\\mathbf{x}_{n-1}) \\] <p>Each conditional \\(p(\\mathbf{x}_n \\mid \\mathbf{x}_1,\\ldots,\\mathbf{x}_{n-1})\\) could be stored as a table whose entries are estimated from frequency counts in the training corpus. But the size of these tables grows exponentially with the sequence length, so this direct approach is not feasible. Let us try to prove this in the following setup.</p> <p>Setup</p> <p>Assume a vocabulary of size \\(K\\) (number of distinct words or tokens), a sequence length \\(N\\) and each \\(x_i\\) being a discrete random variable taking one of these \\(K\\) values:</p> \\[ x_i \\in \\{1,2,\\ldots,K\\}. \\] <p>The full autoregressive factorization is</p> \\[ p(x_1,\\ldots,x_N) = \\prod_{n=1}^N p(x_n \\mid x_1,\\ldots,x_{n-1}). \\] <p>We want to store each conditional \\(p(x_n \\mid x_1,\\ldots,x_{n-1})\\) in a table. Here we are counting how many model parameters (independent probability values) are needed to specify this conditional distribution. Consider a specific position \\(n\\) and let us count how big that parameter table must be.</p> <p>1. Counting possible histories \\((x_1,\\ldots,x_{n-1})\\).</p> <p>A history for position \\(n\\) is any concrete sequence of values</p> \\[ h = (x_1,\\ldots,x_{n-1})   = (i_1,\\ldots,i_{n-1}), \\] <p>where each \\(i_j\\) is one of \\(\\{1,\\ldots,K\\}\\) token.</p> \\[ \\{\\text{number of possible histories of length } n-1\\} = \\underbrace{K \\times K \\times \\cdots \\times K}_{n-1 \\text{ factors}} = K^{n-1}. \\] <p>2. What is stored for one fixed history?</p> <p>Take one specific history</p> \\[ h = (x_1=i_1,\\ldots,x_{n-1}=i_{n-1}). \\] <p>For this history we need the conditional distribution of \\(x_n\\):</p> \\[ p(x_n \\mid h). \\] <p>The variable \\(x_n\\) can again be any of the \\(K\\) vocabulary items. So we must store \\(K\\) probabilities, one for each possible value of \\(x_n\\):</p> \\[ p(x_n = 1 \\mid h),\\; p(x_n = 2 \\mid h),\\;\\ldots,\\; p(x_n = K \\mid h). \\] <p>If we wrote this as a row in a table, that row would contain exactly these \\(K\\) numbers. These numbers are exactly the parameters that define the model\u2019s behaviour for this history.</p> <p>3. \\(K-1\\) free parameters per history.</p> <p>For each fixed history \\(h\\), the \\(K\\) probabilities must obey:</p> <p>(i) Non-negativity:</p> \\[ 0 \\le p(x_n = k \\mid h) \\le 1 \\qquad \\text{for all } k=1,\\ldots,K. \\] <p>(ii) Normalization:</p> \\[ \\sum_{k=1}^K p(x_n = k \\mid h) = 1. \\] <p>The normalization constraint removes one degree of freedom. Once we choose any \\(K-1\\) of the probabilities, the last one is forced to make the sum 1:</p> \\[ p(x_n = K \\mid h) = 1 - \\sum_{k=1}^{K-1} p(x_n = k \\mid h). \\] <p>So although there are \\(K\\) numbers in the row, only \\(K-1\\) of them can be chosen independently. We say the distribution for this history has \\(K-1\\) free parameters.</p> <p>4. Total number of parameters for the table at step \\(n\\).</p> <ul> <li>Number of distinct histories \\(h\\) of length \\(n-1\\): \\(K^{n-1}\\).</li> <li>Free parameters for each history\u2019s row: \\(K-1\\)</li> </ul> <p>Therefore, the total number of free parameters needed to specify the whole conditional table is</p> \\[ \\underbrace{K^{n-1}}_{\\text{rows (histories)}} \\times \\underbrace{(K-1)}_{\\text{free params per row}} = (K-1)K^{n-1}. \\] <p>This quantity grows proportionally to \\(K^{n-1}\\), which increases exponentially as \\(n\\) increases. That is why storing these conditionals as raw tables quickly becomes infeasible for realistic vocabulary sizes \\(K\\) and sequence lengths \\(n\\).</p> <p>5. Total size up to length \\(N\\).</p> <p>To model all positions \\(n=1,\\ldots,N\\), we need all these tables:</p> \\[ \\text{total parameters} = \\sum_{n=1}^N (K-1)K^{n-1} = (K-1)\\frac{K^{N}-1}{K-1} = K^{N}-1. \\] <p>So the total number of table entries is on the order of \\(K^{N}\\), which grows exponentially with the sequence length \\(N\\). Hence representing the autoregressive model directly with probability tables is infeasible for realistic \\(K\\) and \\(N\\).</p>"},{"location":"Transformers-2-NLP/#n-gram","title":"n-gram","text":"<p>We simplify the model by assuming that the conditional for step \\(n\\) depends only on the last \\(L\\) observations. For example, if \\(L=2\\) then</p> \\[ p(\\mathbf{x}_1,\\ldots,\\mathbf{x}_N) = p(\\mathbf{x}_1)p(\\mathbf{x}_2 \\mid \\mathbf{x}_1)   \\prod_{n=3}^N p(\\mathbf{x}_n \\mid \\mathbf{x}_{n-1},\\mathbf{x}_{n-2}) \\] <p>The conditional distributions \\(p(\\mathbf{x}_n \\mid \\mathbf{x}_{n-1},\\mathbf{x}_{n-2})\\) are shared across all positions and can be represented as tables whose entries are estimated from statistics of successive triplets of words in a corpus. The case with \\(L=1\\) is a bi-gram model, which depends on pairs of adjacent words. The case \\(L=2\\) is a tri-gram model, involving triplets. More generally these are called \\(n\\)-gram models.</p> <p>All the models in this section can be run generatively to create text. For example, given the first two words we can sample the third from \\(p(\\mathbf{x}_n \\mid \\mathbf{x}_{n-1},\\mathbf{x}_{n-2})\\), then use the second and third words to sample the fourth, and so on. The resulting text will usually be incoherent, because each word depends only on a short context. Good text models must capture long-range dependencies in language. Simply increasing \\(L\\) is not practical, because the size of the probability tables grows exponentially in \\(L\\), making models beyond tri-grams prohibitively expensive. Nevertheless, the autoregressive factorization will remain central when we move to modern language models based on deep neural networks, such as transformers.</p> <p>One way to extend the effective context for language, without the exponential number of parameters of \\(n\\)-gram tables, is to add latent (hidden) variables and use a hidden Markov model (HMM).</p>"},{"location":"Transformers-2-NLP/#hmm-for-a-word-sequence-n-gram","title":"HMM for a word sequence (n-gram)","text":"<p>Consider a sequence of \\(N\\) words (or tokens)</p> \\[ x_1, x_2, \\dots, x_N. \\] <p>For each position \\(n\\) in the sequence we introduce a hidden state \\(z_n \\in \\{ z_1, z_2, \\dots, z_N\\}\\), which can take one of \\(S\\) discrete values (for example, \\(S\\) different latent \u201ctopics\u2019\u2019 or parts of speech such as noun, verb, adjective), and an observed word \\(x_n\\) that takes one of \\(K\\) vocabulary values. The model defines:</p> <ul> <li>Initial state distribution \\(p(z_1)\\).   This is a categorical distribution over the \\(S\\) possible values of   \\(z_1\\), so it is a length-\\(S\\) vector:</li> </ul> \\[ p(z_1) = \\big(p(z_1=1),\\dots,p(z_1=S)\\big), \\quad \\sum_{s=1}^S p(z_1=s)=1. \\] <ul> <li>Transition distribution \\(p(z_n \\mid z_{n-1})\\).   For each previous state \\(i \\in \\{1,\\dots,S\\}\\) we need a full   distribution over the next state \\(j \\in \\{1,\\dots,S\\}\\). Thus we have   \\(S\\) rows (one per \\(i\\)) and \\(S\\) columns (one per \\(j\\)):</li> </ul> \\[ p(z_n \\mid z_{n-1}) = \\big[p(z_n=j \\mid z_{n-1}=i)\\big]_{i,j=1}^S, \\] <p>an \\(S \\times S\\) matrix whose each row sums to 1.</p> <ul> <li>Emission distribution \\(p(x_n \\mid z_n)\\).   For each hidden state \\(s\\) we need a distribution over all \\(K\\) words   \\(k \\in \\{1,\\dots,K\\}\\). So we have \\(S\\) rows and \\(K\\) columns:</li> </ul> \\[ p(x_n \\mid z_n) = \\big[p(x_n=k \\mid z_n=s)\\big]_{s=1,\\dots,S;\\;k=1,\\dots,K}, \\] <p>an \\(S \\times K\\) matrix whose each row sums to 1.</p> <p>Now, we want the joint distribution over all hidden states and observations:</p> \\[ p(x_{1:N}, z_{1:N}) = p(z_1, x_1, z_2, x_2, \\dots, z_N, x_N). \\] <p>This can be done in the following steps:</p> <p>1. Chain rule.</p> <p>Apply the chain rule in the time order:</p> \\[ \\begin{aligned} p(x_{1:N}, z_{1:N}) &amp;= p(z_1)\\,    p(x_1 \\mid z_1)\\,    p(z_2 \\mid z_1, x_1)\\,    p(x_2 \\mid z_1, x_1, z_2)\\,\\cdots \\\\ &amp;\\quad \\cdots\\,    p(z_N \\mid z_{1:N-1}, x_{1:N-1})\\,    p(x_N \\mid z_{1:N}, x_{1:N-1}). \\end{aligned} \\] <p>2. HMM assumptions.</p> <p>An HMM imposes two conditional independence assumptions:</p> <p>(i) Markov property for hidden states</p> \\[ p(z_n \\mid z_{1:n-1}, x_{1:n-1}) = p(z_n \\mid z_{n-1}) \\quad\\text{for } n \\ge 2. \\] <p>(ii) Emission depends only on current state</p> \\[ p(x_n \\mid z_{1:n}, x_{1:n-1}) = p(x_n \\mid z_n) \\quad\\text{for } n \\ge 1. \\] <p>3. Simplify each factor.</p> <p>Apply these to the chain rule factors:</p> <ul> <li>For \\(n=1\\):</li> </ul> \\[ p(z_1) \\quad\\text{(unchanged)}, \\qquad p(x_1 \\mid z_1) \\quad\\text{(already of the form } p(x_1 \\mid z_1)\\text{)}. \\] <ul> <li>For \\(n=2\\):</li> </ul> \\[ p(z_2 \\mid z_1, x_1) = p(z_2 \\mid z_1), \\] \\[ p(x_2 \\mid z_1, x_1, z_2) = p(x_2 \\mid z_2). \\] <ul> <li>In general, for \\(n = 2,\\dots,N\\):</li> </ul> \\[ p(z_n \\mid z_{1:n-1}, x_{1:n-1}) = p(z_n \\mid z_{n-1}), \\] \\[ p(x_n \\mid z_{1:n}, x_{1:n-1}) = p(x_n \\mid z_n). \\] <p>4. Collect all terms.</p> <p>Replacing every factor in the chain rule by its simplified HMM form gives</p> \\[ \\begin{aligned} p(x_{1:N}, z_{1:N}) &amp;= p(z_1)\\, p(x_1 \\mid z_1)\\,    \\prod_{n=2}^N p(z_n \\mid z_{n-1})\\, p(x_n \\mid z_n) \\\\ &amp;= p(z_1)\\,\\Bigg[\\prod_{n=2}^N p(z_n \\mid z_{n-1})\\Bigg]\\,            \\Bigg[\\prod_{n=1}^N p(x_n \\mid z_n)\\Bigg]. \\end{aligned} \\] \\[ p(x_{1:N}, z_{1:N}) = p(z_1)\\,\\prod_{n=2}^N p(z_n \\mid z_{n-1})\\,   \\prod_{n=1}^N p(x_n \\mid z_n). \\] <p>Operationally, the model generates a sequence as follows:</p> <p>(i) Sample the first hidden state</p> \\[ z_1 \\sim p(z_1). \\] <p>(ii) Emit the first word from this state</p> \\[ x_1 \\sim p(x_1 \\mid z_1). \\] <p>(iii) For each later position \\(n=2,\\dots,N\\):</p> \\[ \\begin{aligned}   z_n &amp;\\sim p(z_n \\mid z_{n-1}) &amp;&amp; \\text{(move to a new hidden state)}\\\\   x_n &amp;\\sim p(x_n \\mid z_n)     &amp;&amp; \\text{(emit the next word).} \\end{aligned} \\] <p>This step-by-step process corresponds exactly to the factorization</p> \\[ p(x_{1:N}, z_{1:N}) = p(z_1)\\,\\prod_{n=2}^N p(z_n \\mid z_{n-1})\\,   \\prod_{n=1}^N p(x_n \\mid z_n). \\] <p>Since \\(p(z_1)\\) has \\(O(S)\\) parameters, the \\(S \\times S\\) transition matrix \\(p(z_n \\mid z_{n-1})\\) has \\(O(S^2)\\) parameters, and the \\(S \\times K\\) emission matrix \\(p(x_n \\mid z_n)\\) has \\(O(SK)\\) parameters, the total number of learnable parameters scales as</p> \\[ O(S^2 + SK), \\] <p>which depends only on the number of states \\(S\\) and vocabulary size \\(K\\), but not on the sequence length \\(N\\). In this way an HMM can model long sequences without the \\(O(K^L)\\) parameter blow-up of an \\(L\\)-gram table.</p> <p>Long-range dependencies.</p> <p>We want to see how \\(x_n\\) can depend on all earlier words in an HMM.</p> \\[ p(x_n \\mid x_{1:n-1}) = \\sum_{z_n} p(x_n, z_n \\mid x_{1:n-1}) = \\sum_{z_n} p(x_n \\mid z_n, x_{1:n-1})\\,p(z_n \\mid x_{1:n-1}). \\] <p>1. By definition of conditional probability.</p> \\[ p(x_n \\mid x_{1:n-1}) = \\frac{p(x_n, x_{1:n-1})}{p(x_{1:n-1})}. \\] <p>2. Insert the hidden variable by marginalization.</p> <p>The joint probability of \\((x_n, x_{1:n-1})\\) can be written by summing over all possible values of the hidden state \\(z_n\\) (from law of total probability):</p> \\[ p(x_n, x_{1:n-1}) = \\sum_{z_n} p(x_n, z_n, x_{1:n-1}). \\] <p>Substitute this into the conditional:</p> \\[ p(x_n \\mid x_{1:n-1}) = \\frac{1}{p(x_{1:n-1})}   \\sum_{z_n} p(x_n, z_n, x_{1:n-1}). \\] <p>Now divide inside the sum:</p> \\[ p(x_n \\mid x_{1:n-1}) = \\sum_{z_n} \\frac{p(x_n, z_n, x_{1:n-1})}{p(x_{1:n-1})} = \\sum_{z_n} p(x_n, z_n \\mid x_{1:n-1}). \\] <p>This gives:</p> \\[ p(x_n \\mid x_{1:n-1}) = \\sum_{z_n} p(x_n, z_n \\mid x_{1:n-1}). \\] <p>3. Apply the product rule to the conditional joint.</p> <p>For any random variables \\(A,B,C\\),</p> \\[ p(A,B \\mid C) = p(A \\mid B,C)\\,p(B \\mid C). \\] <p>This is just the chain rule applied to the conditional distribution given \\(C\\):</p> \\[ p(A,B,C) = p(A \\mid B,C)\\,p(B \\mid C)\\,p(C), \\] <p>and dividing both sides by \\(p(C)\\) gives the conditional form.</p> <p>Now take</p> \\[ A = x_n,\\quad B = z_n,\\quad C = x_{1:n-1}. \\] <p>Then</p> \\[ p(x_n, z_n \\mid x_{1:n-1}) = p(x_n \\mid z_n, x_{1:n-1})\\,p(z_n \\mid x_{1:n-1}). \\] <p>Substitute this into the previous sum:</p> \\[ \\begin{aligned} p(x_n \\mid x_{1:n-1}) &amp;= \\sum_{z_n} p(x_n, z_n \\mid x_{1:n-1}) \\\\ &amp;= \\sum_{z_n} p(x_n \\mid z_n, x_{1:n-1})\\,p(z_n \\mid x_{1:n-1}). \\end{aligned} \\] <p>By the HMM emission assumption, the observation depends only on the current state:</p> \\[ p(x_n \\mid z_n, x_{1:n-1}) = p(x_n \\mid z_n). \\] <p>So</p> \\[ p(x_n \\mid x_{1:n-1}) = \\sum_{z_n} p(x_n \\mid z_n)\\,p(z_n \\mid x_{1:n-1}). \\] <p>Here \\(p(z_n \\mid x_{1:n-1})\\) is the belief over the current hidden state given all previous words. This belief is updated recursively from \\(p(z_{n-1} \\mid x_{1:n-2})\\) using the transition and emission distributions. In principle, every past word \\(x_1,\\dots,x_{n-1}\\) can influence \\(p(z_n \\mid x_{1:n-1})\\), and therefore \\(p(x_n \\mid x_{1:n-1})\\).</p> <p>How new observations overwrite older information.</p> <p>We want an explicit formula for our updated belief about the current hidden state after seeing the new observation at time \\(t\\); this belief is the filtering distribution</p> \\[ p(z_t \\mid x_{1:t}). \\] <p>1. Start from Bayes\u2019 rule. Treat \\(x_t\\) as the new observation and \\(x_{1:t-1}\\) as given context: We have:</p> \\[ p(A \\mid B,C) = \\frac{p(B \\mid A,C)\\,p(A \\mid C)}{p(B \\mid C)}. \\] <p>Where,</p> \\[ A = z_t,\\quad B = x_t,\\quad C = x_{1:t-1}. \\] <p>So</p> \\[ p(z_t \\mid x_{1:t-1}, x_t) = \\frac{p(x_t \\mid z_t, x_{1:t-1})\\,p(z_t \\mid x_{1:t-1})}        {p(x_t \\mid x_{1:t-1})}. \\] <p>Finally, note that</p> \\[ p(z_t \\mid x_{1:t}) = p(z_t \\mid x_{1:t-1}, x_t), \\] <p>2. Use the emission assumption. In an HMM, \\(x_t\\) depends only on \\(z_t\\):</p> \\[ p(x_t \\mid z_t, x_{1:t-1}) = p(x_t \\mid z_t). \\] <p>So</p> \\[ p(z_t \\mid x_{1:t}) = \\frac{p(x_t \\mid z_t)\\,p(z_t \\mid x_{1:t-1})}        {p(x_t \\mid x_{1:t-1})}. \\] <p>The denominator does not depend on \\(z_t\\), so it is a normalizing constant. Thus</p> \\[ p(z_t \\mid x_{1:t}) \\propto p(x_t \\mid z_t)\\,p(z_t \\mid x_{1:t-1}). \\] <p>3. Expand the predictive term.</p> <p>In the previous step we obtained</p> \\[ p(z_t \\mid x_{1:t}) \\propto p(x_t \\mid z_t)\\,p(z_t \\mid x_{1:t-1}), \\] <p>so to complete the update we need an explicit expression for the predictive distribution \\(p(z_t \\mid x_{1:t-1})\\) in terms of quantities at time \\(t-1\\). This is where the Markov structure of the HMM enters.</p> <p>(i) Start from the definition of the predictive term.</p> <p>By definition of conditional probability,</p> \\[ p(z_t \\mid x_{1:t-1}) = \\frac{p(z_t, x_{1:t-1})}{p(x_{1:t-1})}. \\] <p>(ii) Introduce \\(z_{t-1}\\) by marginalization.</p> <p>Use the law of total probability on the joint:</p> \\[ p(z_t, x_{1:t-1}) = \\sum_{z_{t-1}} p(z_t, z_{t-1}, x_{1:t-1}). \\] <p>Substitute into the conditional:</p> \\[ p(z_t \\mid x_{1:t-1}) = \\frac{1}{p(x_{1:t-1})}   \\sum_{z_{t-1}} p(z_t, z_{t-1}, x_{1:t-1}). \\] <p>Bring the constant denominator inside the sum:</p> \\[ p(z_t \\mid x_{1:t-1}) = \\sum_{z_{t-1}} \\frac{p(z_t, z_{t-1}, x_{1:t-1})}{p(x_{1:t-1})} = \\sum_{z_{t-1}} p(z_t, z_{t-1} \\mid x_{1:t-1}). \\] <p>This gives</p> \\[ p(z_t \\mid x_{1:t-1}) = \\sum_{z_{t-1}} p(z_t, z_{t-1} \\mid x_{1:t-1}). \\] <p>(iii) Apply the product rule inside the sum.</p> <p>For any variables \\(A,B,C\\),</p> \\[ p(A,B \\mid C) = p(A \\mid B,C)\\,p(B \\mid C). \\] <p>Let</p> \\[ A = z_t,\\quad B = z_{t-1},\\quad C = x_{1:t-1}. \\] <p>Then</p> \\[ p(z_t, z_{t-1} \\mid x_{1:t-1}) = p(z_t \\mid z_{t-1}, x_{1:t-1})\\,   p(z_{t-1} \\mid x_{1:t-1}). \\] <p>Substitute back:</p> \\[ \\begin{aligned} p(z_t \\mid x_{1:t-1}) &amp;= \\sum_{z_{t-1}} p(z_t, z_{t-1} \\mid x_{1:t-1}) \\\\ &amp;= \\sum_{z_{t-1}}    p(z_t \\mid z_{t-1}, x_{1:t-1})\\,    p(z_{t-1} \\mid x_{1:t-1}). \\end{aligned} \\] <p>This is the desired expression of the predictive term in terms of the previous belief \\(p(z_{t-1} \\mid x_{1:t-1})\\) and the state dynamics.</p> <p>4. Use the Markov assumption. In an HMM, the next state depends only on the previous state:</p> \\[ p(z_t \\mid z_{t-1}, x_{1:t-1}) = p(z_t \\mid z_{t-1}). \\] <p>So</p> \\[ p(z_t \\mid x_{1:t-1}) = \\sum_{z_{t-1}} p(z_t \\mid z_{t-1})\\,p(z_{t-1} \\mid x_{1:t-1}). \\] <p>5. Combine the pieces.</p> <p>Substitute the expression for \\(p(z_t \\mid x_{1:t-1})\\) back into the proportional form:</p> \\[ \\begin{aligned} p(z_t \\mid x_{1:t}) &amp;\\propto p(x_t \\mid z_t)\\,p(z_t \\mid x_{1:t-1}) \\\\ &amp;= p(x_t \\mid z_t)    \\sum_{z_{t-1}} p(z_t \\mid z_{t-1})\\,p(z_{t-1} \\mid x_{1:t-1}). \\end{aligned} \\] <p>This gives</p> \\[ p(z_t \\mid x_{1:t}) \\propto p(x_t \\mid z_t)         \\sum_{z_{t-1}} p(z_t \\mid z_{t-1})\\,p(z_{t-1} \\mid x_{1:t-1}), \\] <p>with the proportionality constant chosen so that \\(\\sum_{z_t} p(z_t \\mid x_{1:t}) = 1\\). This is the standard forward (filtering) update: the belief over \\(z_t\\) after seeing \\(x_t\\).</p> <p>It is often useful to view this update at time \\(t+1\\) as two steps:</p> <p>(i) Prediction (from \\(t\\) to \\(t+1\\)):</p> \\[ \\tilde{p}(z_{t+1} \\mid x_{1:t}) = \\sum_{z_t} p(z_{t+1} \\mid z_t)\\,p(z_t \\mid x_{1:t}), \\] <p>which uses only the transition probabilities and the previous belief.</p> <p>(ii) Correction (using \\(x_{t+1}\\)):</p> \\[ p(z_{t+1} \\mid x_{1:t+1}) \\propto p(x_{t+1} \\mid z_{t+1})\\,          \\tilde{p}(z_{t+1} \\mid x_{1:t}), \\] \\[ p(z_{t+1} \\mid x_{1:t+1}) \\propto p(x_{t+1} \\mid z_{t+1})\\,           \\sum_{z_t} p(z_{t+1} \\mid z_t)\\,p(z_t \\mid x_{1:t}), \\] <p>which reweights the predicted belief using the likelihood of the new    observation.</p> <p>Thus, all past observations \\(x_{1:t}\\) affect \\(p(z_{t+1} \\mid x_{1:t+1})\\) only through the current summary \\(p(z_t \\mid x_{1:t})\\). At each new time step this summary is first mixed by the transition probabilities and then reshaped by the new word. After many such updates, different early histories produce almost the same state distribution. In practice, this means that the influence of very old observations on future predictions becomes very small very quickly.</p>"},{"location":"Transformers-2-NLP/#recurrent-neural-networks","title":"Recurrent neural networks","text":"<p>\\(n\\)-gram models scale badly with sequence length because they store large, unstructured tables of conditional probabilities. We can get much better scaling by using parameterized models based on neural networks. But if we try to apply a standard feed-forward network directly to word sequences, two problems appear:</p> <ul> <li>The network expects a fixed number of inputs and outputs, but real   sequences have variable length in both training and test data.</li> <li>A word (or phrase) that appears in different positions should usually   represent the same concept, but a plain feed-forward network would   treat each position with separate parameters.</li> </ul> <p>Ideally, we want an architecture that:</p> <ol> <li>shares parameters across all positions in the sequence (an equivariance property), and</li> <li>can handle sequences of different lengths.</li> </ol> <p>To achieve this, we take inspiration from the hidden Markov model and introduce a hidden state \\(z_n\\) for each step \\(n\\) in the sequence. At each step the network takes as input</p> \\[ (x_n, z_{n-1}) \\] <p>where \\(x_n\\) is the current word and \\(z_{n-1}\\) is the previous hidden state, and it outputs</p> \\[ (y_n, z_n), \\] <p>where \\(y_n\\) is the network output at that step (for example, a predicted word) and \\(z_n\\) is the updated hidden state. Instead of building a different network for each time step, we use one neural network cell and apply it repeatedly along the sequence. Formally, if the cell has parameters \\(\\theta\\) (weights and biases), then at every step \\(n\\) we compute</p> \\[ (z_n, y_n) = f_\\theta(x_n, z_{n-1}), \\] <p>using the same \\(\\theta\\) for all \\(n\\). The resulting architecture is called a recurrent neural network (RNN). A common choice is to initialize the hidden state to a default value such as</p> \\[ z_0 = (0,0,\\ldots,0)^\\top. \\] <p>As a concrete example, consider translating sentences from English to Dutch. Input and output sentences have variable length, and the output length may differ from the input length. The model may also need to see the entire English sentence before producing any Dutch words.</p> <p>With an RNN we can:</p> <ol> <li>Feed the whole English sentence word by word.</li> <li>Then feed a special token \\(\\langle\\text{start}\\rangle\\) to signal the    beginning of the translation.</li> </ol> <p>During training, the network learns that \\(\\langle\\text{start}\\rangle\\) indicates the point at which it should begin generating the translated sentence. At each subsequent time step:</p> <ul> <li>the RNN outputs the next Dutch word,</li> <li>we feed that output word back as the next input.</li> </ul> <p>The network is also trained to emit a special token \\(\\langle\\text{stop}\\rangle\\) that marks the end of the translation. We now describe the encoder--decoder RNN more explicitly. Let the English input sentence be</p> \\[ (e_1, e_2, \\ldots, e_T), \\] <p>and the Dutch output sentence be</p> \\[ (d_1, d_2, \\ldots, d_M). \\] <p>Encoder (reads English, no outputs used).</p> <p>We start with a hidden state</p> \\[ z_0 = \\mathbf{0}. \\] <p>For each English word \\(e_t\\) we apply the recurrent update</p> \\[ z_t = f_{\\text{enc}}(z_{t-1}, e_t), \\qquad t = 1,\\ldots,T, \\] <p>where \\(f_{\\text{enc}}\\) is the RNN cell (e.g. a simple RNN, LSTM, or GRU). During this phase:</p> <ul> <li>inputs: \\((z_{t-1}, e_t)\\),</li> <li>outputs: intermediate \\(z_t\\); we ignore any word-level outputs.</li> </ul> <p>After the last English word we keep only the final hidden state</p> \\[ z^\\ast = z_T. \\] <p>This \\(z^\\ast\\) is a fixed-length vector that summarizes the whole English sentence. It is the encoder representation.</p> <p>Decoder (generates Dutch, uses \\(z^\\ast\\)).</p> <p>The decoder is another RNN (often with the same form of cell) that starts from the encoder state. We set</p> \\[ h_0 = z^\\ast, \\] <p>and feed a special start token \\(\\langle\\text{start}\\rangle\\) as the first input. At step \\(m = 1,2,\\ldots\\) we compute</p> \\[ h_m = f_{\\text{dec}}(h_{m-1}, u_m), \\] <p>where:</p> <ul> <li>for \\(m=1\\), \\(u_1 = \\langle\\text{start}\\rangle\\),</li> <li>for \\(m&gt;1\\), \\(u_m = d_{m-1}\\), the previous Dutch word.</li> </ul> <p>From \\(h_m\\) the decoder produces a distribution over the next Dutch word:</p> \\[ p(d_m \\mid h_m) = \\text{softmax}(W h_m + b). \\] <p>During training we use the true previous word \\(d_{m-1}\\) as input; at test time we instead feed back the word sampled (or chosen) from \\(p(d_m \\mid h_m)\\). The decoder continues until it outputs a special stop token \\(\\langle\\text{stop}\\rangle\\). Thus:</p> <ul> <li>inputs to decoder: \\(z^\\ast\\) (via \\(h_0\\)) and the sequence   \\((\\langle\\text{start}\\rangle, d_1, d_2,\\ldots)\\),</li> <li>outputs from decoder: the Dutch words   \\((d_1, d_2,\\ldots,d_M,\\langle\\text{stop}\\rangle)\\).</li> </ul> <p>Autoregressive structure.</p> <p>Conditioned on the English sentence \\(e_{1:T}\\) (summarized by \\(z^\\ast\\)), the decoder defines</p> \\[ p(d_1,\\ldots,d_M \\mid e_{1:T}) = \\prod_{m=1}^M p\\bigl(d_m \\mid d_{1:m-1}, e_{1:T}\\bigr), \\] <p>because each step \\(m\\) takes as input the previous hidden state \\(h_{m-1}\\) and previous output word \\(d_{m-1}\\). This is exactly an autoregressive factorization over the Dutch sequence, with the entire English sentence influencing each term through \\(z^\\ast\\) and the hidden states \\(h_m\\).</p>"},{"location":"Transformers-2-NLP/#backpropagation-through-time","title":"Backpropagation through time","text":"<p>We fix a simple language-model RNN that predicts the next token at each time step. We train RNNs with stochastic gradient descent, using gradients computed by backpropagation and automatic differentiation, just as for standard neural networks.</p> <p>Data, inputs, and outputs Consider Vocabulary size \\(K\\), a training sequence is \\(x_{1:N} = (x_1,\\dots,x_N)\\), with \\(x_n \\in \\{1,\\dots,K\\}\\). The target at step \\(n\\) is the next token \\(t_n = x_{n+1}\\) (or a special end token).</p> <p>Each token is mapped to a vector \\(\\mathbf{x}_n\\) either via a one-hot vector in \\(\\mathbb{R}^K\\) or an embedding \\(\\mathbf{x}_n = \\mathbf{E}\\,\\mathbf{e}(x_n) \\in \\mathbb{R}^D\\), where \\(\\mathbf{E} \\in \\mathbb{R}^{D\\times K}\\) is the embedding matrix.</p> <p>The RNN cell has hidden state \\(\\mathbf{h}_n \\in \\mathbb{R}^H\\), output logits \\(\\mathbf{o}_n \\in \\mathbb{R}^K\\) and output probabilities \\(\\mathbf{y}_n \\in \\mathbb{R}^K\\).</p> <p>Forward pass through time</p> <p>At each time step \\(n\\) the RNN computes</p> \\[ \\mathbf{h}_n = f(\\mathbf{W}_{xh}\\mathbf{x}_n + \\mathbf{W}_{hh}\\mathbf{h}_{n-1} + \\mathbf{b}_h), \\] \\[ \\mathbf{o}_n = \\mathbf{W}_{ho}\\mathbf{h}_n + \\mathbf{b}_o, \\] \\[ \\mathbf{y}_n = \\text{softmax}(\\mathbf{o}_n), \\] <p>with initial state \\(\\mathbf{h}_0\\) (often \\(\\mathbf{0}\\)). Unrolling this gives a chain</p> \\[ (\\mathbf{x}_1,\\mathbf{h}_0) \\to \\mathbf{h}_1 \\to \\mathbf{o}_1 \\to \\mathbf{y}_1, \\quad (\\mathbf{x}_2,\\mathbf{h}_1) \\to \\mathbf{h}_2 \\to \\mathbf{o}_2 \\to \\mathbf{y}_2, \\quad \\dots \\] <p>All steps share the same parameters</p> \\[ \\theta = \\{\\mathbf{E},\\mathbf{W}_{xh},\\mathbf{W}_{hh},\\mathbf{W}_{ho},\\mathbf{b}_h,\\mathbf{b}_o\\}. \\] <p>Loss definition</p> <p>At step \\(n\\), the target is a one-hot vector \\(\\mathbf{t}_n \\in \\mathbb{R}^K\\) with components \\(t_{n,k}\\) and \\(\\sum_k t_{n,k}=1\\). The cross-entropy loss at that step is</p> \\[ L_n = - \\sum_{k=1}^K t_{n,k}\\log y_{n,k}. \\] <p>The total loss for the sequence (over \\(N'\\) training steps) is</p> \\[ L = \\sum_{n=1}^{N'} L_n   = - \\sum_{n=1}^{N'} \\sum_{k=1}^K t_{n,k}\\log y_{n,k}. \\] <p>Because \\(\\mathbf{t}_n\\) is one-hot, there is a unique index \\(t_n\\) with \\(t_{n,t_n}=1\\) and zero for all other, so</p> \\[ L_n = -\\log y_{n,t_n}. \\] <p>Thus, the total loss can also be written as</p> \\[ L = - \\sum_{n=1}^{N'} \\log y_{n,t_n}. \\] <p>Backward pass</p> <p>To compute gradients we view the unrolled RNN over \\(N'\\) steps as one large feed-forward network and apply standard backpropagation. Gradients at each step flow</p> \\[ L_n \\;\\to\\; \\mathbf{y}_n \\;\\to\\; \\mathbf{o}_n \\;\\to\\; \\mathbf{h}_n \\;\\to\\; \\mathbf{h}_{n-1} \\;\\to\\; \\theta. \\] <ul> <li>The recurrent connection \\(\\mathbf{h}_{n-1} \\to \\mathbf{h}_n\\) causes the   gradient at time \\(n\\) to contribute to the gradient at all earlier times. Since the same parameters \\(\\theta\\) are reused at every time step, the total   gradient is the sum of contributions from all steps:</li> </ul> \\[ \\frac{\\partial L}{\\partial \\theta} = \\sum_{n=1}^{N'} \\frac{\\partial L}{\\partial \\theta}\\Big|_{\\text{via step }n}. \\] <p>Running this backward pass over the unrolled network is called backpropagation through time (BPTT). Conceptually it is straightforward, but in practice very long sequences cause training difficulties: gradients can either vanish or explode, just as in very deep feed-forward networks.</p>"},{"location":"Transformers-2-NLP/#long-range-dependencies","title":"Long range dependencies","text":"<p>Standard RNNs also struggle with long-range dependencies. Natural language often contains concepts introduced early in a passage that strongly influence words much later. In the encoder\u2013decoder architecture described earlier, the entire meaning of the English sentence must be stored in a single fixed-length hidden vector \\(z^\\ast\\). As sequences grow longer, compressing all relevant information into \\(z^\\ast\\) becomes harder. This is called a bottleneck problem: an arbitrarily long input sequence must be summarized into one hidden vector before the network can start producing the output translation.</p> <p>To address both vanishing/exploding gradients and limited long-range memory, we can change the recurrent cell to include additional pathways that let signals bypass many intermediate computations. This helps information persist over more time steps. The most prominent examples are long short-term memory (LSTM) networks and gated recurrent units (GRU). These architectures improve performance over standard RNNs, but they still have restricted ability to capture very long-range dependencies. Their more complex cells also make them slower to train. All recurrent models, including LSTMs and GRUs, share two structural limitations:</p> <ul> <li>The length of the signal path between distant time steps grows   linearly with the sequence length. To see this, look at the unrolled RNN:</li> </ul> \\[  (\\mathbf{x}_1,\\mathbf{h}_0) \\to \\mathbf{h}_1 \\to \\mathbf{h}_2 \\to \\cdots \\to \\mathbf{h}_N \\to \\mathbf{y}_N. \\] <p>Any influence from time step \\(i\\) on time step \\(j&gt;i\\) must pass through the   chain</p> \\[  \\mathbf{x}_i \\to \\mathbf{h}_i \\to \\mathbf{h}_{i+1} \\to \\cdots \\to \\mathbf{h}_j \\to \\mathbf{y}_j. \\] <p>This path contains \\((j-i)\\) recurrent transitions   \\(\\mathbf{h}_{t-1}\\to\\mathbf{h}_t\\) (plus a constant number of input/output   edges), so its length is proportional to \\(j-i\\). For two positions that are   \\(L\\) steps apart, the signal must traverse \\(O(L)\\) nonlinear transformations.</p> <ul> <li>Computation within a single sequence is inherently sequential, so   different time steps cannot be processed in parallel.</li> </ul> <p>As a result, RNNs cannot exploit modern highly parallel hardware (such as GPUs) efficiently. These limitations motivate replacing RNNs with transformer architectures.</p>"},{"location":"Transformers-2-NLP/#references","title":"References","text":"<ul> <li>Bishop, C. M., &amp; Bishop, H. (2023). Transformers. In Deep Learning: Foundations and Concepts (pp. 357-406). Cham: Springer International Publishing.</li> </ul>"},{"location":"Transformers-3-LLMs/","title":"Transformer Language Models","text":""},{"location":"Transformers-3-LLMs/#introduction","title":"Introduction","text":"<p>The transformer layer is a very flexible building block for neural networks, and it works especially well for natural language. When we scale transformers up, we get massive neural networks called large language models (LLMs), which have turned out to be remarkably capable.</p> <p>We can use transformers for many different language tasks, and it helps to think of them in three main categories, based on the kinds of inputs and outputs they handle:</p> <ul> <li> <p>Encoder models. Here the input is a sequence of words, and the output is a single value or label. For example, in sentiment analysis we feed in a sentence and output one variable that describes its sentiment, such as happy or sad. In this setup, the transformer acts as an encoder of the input sequence.</p> </li> <li> <p>Decoder models. In this case the input is a single vector, and the output is a sequence of words. A common example is image captioning, where an input image is first mapped to a vector, and then a transformer decoder takes that vector and generates a text caption word by word.</p> </li> <li> <p>Encoder\u2013decoder (sequence-to-sequence) models. Here both the input and the output are word sequences. A typical example is machine translation, where the model takes a sentence in one language as input and produces a sentence in another language as output. In this setup, we use transformers in both roles: an encoder for the input sequence and a decoder for the output sequence.</p> </li> </ul> <p>In the rest of this chapter, we will look at each of these three classes of language model in turn, using example architectures to show how they are built.</p>"},{"location":"Transformers-3-LLMs/#decoder-transformers","title":"Decoder transformers","text":"<p>We now look at decoder-only transformer models. These are used as generative models: given a prefix of a sequence, they generate the rest, token by token. A key example is the GPT family (generative pre-trained transformer). GPT uses the transformer architecture to build an autoregressive model, where each conditional distribution</p> \\[ p(x_n \\mid x_1,\\ldots,x_{n-1}) \\] <p>is represented by the same transformer neural network (meaning same parameters) learned from data. At step \\(n\\), the model conceptually takes the first \\(n-1\\) tokens as input and outputs the conditional distribution over the vocabulary of size \\(K\\) for token \\(n\\). Sampling from this distribution extends the sequence to length \\(n\\), and we can repeat this process to get token \\(n+1\\), \\(n+2\\), and so on, up to a maximum sequence length set by the transformer. This token-by-token view is how we generate text. Training follows a different paradigm defined later in this chapter.</p>"},{"location":"Transformers-3-LLMs/#network-architecture-and-self-supervised-training","title":"Network architecture and self-supervised training","text":"<p>A GPT-style model is a stack of transformer layers. The input is a sequence of token embeddings</p> \\[ x_1,\\ldots,x_N, \\] <p>with each \\(x_n \\in \\mathbb{R}^D\\). Stacking these row-wise gives \\(X \\in \\mathbb{R}^{N\\times D}\\). The transformer stack maps \\(X\\) to a sequence of hidden vectors</p> \\[ \\tilde{x}_1,\\ldots,\\tilde{x}_N, \\] <p>collected in \\(\\tilde{X} \\in \\mathbb{R}^{N\\times D}\\). Each \\(\\tilde{x}_n\\) is the hidden representation used to predict \\(x_n\\) given the prefix \\(x_{1:n-1}\\). At every position we want a probability distribution over a vocabulary of \\(K\\) tokens, but the transformer outputs \\(D\\)-dimensional vectors. We therefore apply the same linear layer at all positions, with weight matrix \\(W^{(p)} \\in \\mathbb{R}^{D\\times K}\\), followed by a softmax:</p> \\[ Y = Softmax\\!\\bigl(\\tilde{X} W^{(p)}\\bigr) \\in \\mathbb{R}^{N\\times K}, \\] <p>so that the \\(n\\)th row \\(y_n^\\top\\) is a full probability distribution over the \\(K\\) vocabulary items and is the model\u2019s prediction for token \\(x_n\\) given \\(x_{1:n-1}\\).</p> <p>Self supervised training.</p> <p>We train this model on a large corpus of raw text using a self-supervised objective, where the input text itself provides the output targets. To turn text into something the model can handle, we first map each token to an integer in a fixed vocabulary. Let \\(t_n \\in \\{1,\\ldots,K\\}\\) be the index of \\(x_n\\) in the vocabulary. Then for a single sequence, the loss is the sum of cross-entropies over all positions and can be written as:</p> \\[ \\mathcal{L}   = - \\sum_{n=1}^{N} \\log y_{n,t_n}. \\] <p>This means:</p> <ul> <li>At each position \\(n\\), the model outputs a vector</li> </ul> \\[ y_n \\in \\mathbb{R}^K, \\] <p>which is the \\(n\\)th row of \\(Y\\). We can write it as</p> \\[ y_n = (y_{n,1}, y_{n,2}, \\ldots, y_{n,K}), \\] <p>where \\(y_{n,k}\\) is the predicted probability that the \\(n\\)th token is the   \\(k\\)th vocabulary item.</p> <ul> <li>The true token at position \\(n\\) is \\(x_n\\). We represent it by its   vocabulary index \\(t_n \\in \\{1,\\ldots,K\\}\\) (for example, if \u201criver\u201d is the   57th word in the vocabulary, then \\(t_n = 57\\) for that position). Then</li> </ul> \\[ y_{n,t_n} = \\text{``model\u2019s predicted probability of the true token at step } n\\text{''}. \\] <ul> <li>The cross-entropy loss at that position \\(n\\) is then given by:</li> </ul> \\[ -\\log y_{n,t_n}, \\] <p>which is large if the model puts low probability on the correct word, and   small if it puts high probability on it, and we want to minimize this loss.</p> <p>Summing over all positions \\(n=1,\\ldots,N\\) gives the total loss \\(\\mathcal{L}\\) for that one sequence where every token in the sequence contributes one term: \\(-\\log y_{n,t_n}\\).</p> <p>Over the whole dataset we have many sequences (sentences, or longer chunks). We usually treat them as i.i.d. samples and sum the same loss over all of them. If we index the many sequences by \\(s=1,\\ldots,S\\), with length \\(N^{(s)}\\), then the total loss is</p> \\[ \\mathcal{L}_{\\text{total}}   = \\sum_{s=1}^{S} \\sum_{n=1}^{N^{(s)}} - \\log y^{(s)}_{n,t^{(s)}_n}, \\] <p>where \\(y^{(s)}_{n,\\cdot}\\) is the prediction vector for position \\(n\\) in sequence \\(s\\), and \\(t^{(s)}_n\\) is the true vocabulary index at that position. Intuitively, each token plays two roles:</p> <ul> <li>it is a target we want the model to predict correctly, and</li> <li>it is part of the prefix that helps predict later tokens.</li> </ul> <p>For example, for the sentence</p> <p>I swam across the river to get to the other bank.</p> <p>we embed all tokens into a matrix \\(X\\) and feed it through the transformer to get predictions \\(y_1,\\ldots,y_N\\). At the position of the word \u201cthe\u201d, the input effectively corresponds to the prefix \u201cI swam across\u201d, and the model output \\(y_n\\) should put high probability on the vocabulary index of \u201cthe\u201d. At the next position, the input prefix is \u201cI swam across the\u201d, and the output \\(y_{n+1}\\) should place high probability on \u201criver\u201d, and so on for all later positions.</p> <p>However, if during training we let the network use the entire sentence at every position, then when predicting the \\(n\\)th token it can also see token \\(n\\) itself (and even later tokens). In that case it can simply learn to copy the next word from the input instead of genuinely modelling \\(p(x_n \\mid x_{1:n-1})\\). This behaviour would give a very low training loss, but it would fail at generation time, where future tokens are not available. In the next section we will see how the architecture is constrained so that each prediction can only depend on earlier tokens in the sequence.</p>"},{"location":"Transformers-3-LLMs/#preventing-cheating-shifting-and-masking","title":"Preventing cheating: shifting and masking","text":"<p>We prevent the above issue in two ways:</p> <ol> <li>Shifted inputs. We shift the input sequence one step to the    right. Input token \\(x_n\\) now corresponds to output \\(y_{n+1}\\), with target    \\(x_{n+1}\\). We prepend a special start-of-sequence token, \\(\\langle\\text{start}\\rangle\\), at the first input position. Even with this shift, a single training sequence \\((x_1,\\dots,x_T)\\) still    yields many input\u2013target pairs: for each \\(n \\ge 1\\) we treat the prefix    \\((x_1,\\dots,x_{n-1})\\) (or, after shifting, \\((\\langle\\text{start}\\rangle,x_1,\\dots,x_{n-1})\\)) as the input and \\(x_n\\) as the corresponding target.</li> </ol> <p>Before shifting. Conceptually, we were thinking of many separate    next-token training pairs, e.g.</p> \\[ (\\text{I} \\to \\text{swam}),\\quad (\\text{I swam} \\to \\text{across}),\\quad \\ldots \\] <p>Each pair would be run through the model as its own little training example.</p> <p>What we change. Instead of \\(N\\) separate runs for one sentence, we    pack everything into a single sequence:</p> <ul> <li>Start from the raw text tokens:</li> </ul> \\[ x_1, x_2, \\ldots, x_N. \\] <ul> <li>Build the input row by shifting right and inserting the start token:</li> </ul> \\[ \\underbrace{\\langle\\text{start}\\rangle, x_1, x_2, \\ldots, x_{N-1}}_{\\text{inputs}} \\] <ul> <li>Build the target row by shifting left:</li> </ul> \\[ \\underbrace{x_1, x_2, \\ldots, x_N}_{\\text{targets}}. \\] <p>So compared to the original text, we have:</p> <ul> <li>removed \\(x_N\\) from the input side,</li> <li>added \\(\\langle\\text{start}\\rangle\\) at the front,</li> <li>kept the targets as the original sequence.</li> </ul> <p>With this layout, column \\(n\\) of the model sees a prefix ending at \\(x_{n-1}\\)    and is trained to predict \\(x_n\\). All \\(N\\) next-token prediction tasks are now    done in one forward pass, and the masking step (described next) makes sure    each position only uses past tokens.</p> <ol> <li>Masked (causal) attention, padding, and efficient generation.</li> </ol> <p>Before masking. In plain self-attention, every token can attend to    every other token in the sequence. If we used this directly for    next-token prediction, token \\(n\\) could see token \\(n+1\\) and simply copy it,    which is useless at generation time when \\(x_{n+1}\\) is not known.</p> <p>What we change (causal mask). We force each token to look only at    itself and earlier tokens:</p> <ul> <li> <p>In the attention matrix \\(\\text{Attention}(Q,K,V)\\) we zero out all entries      where a token would attend to any later position.</p> </li> <li> <p>In practice we set the corresponding logits to \\(-\\infty\\), so the      attention softmax gives probability almost \\(0\\) there and renormalizes over the allowed      positions.</p> </li> </ul> <p>The result is a lower-triangular attention matrix: row \\(n\\) only uses columns    \\(1,\\dots,n\\).</p> <p>Handling different lengths. Real sentences have different lengths, but GPUs work    best if we process many sequences together as one batch tensor. Without care,    shorter sequences would leave random or empty slots that other tokens might    attend to.</p> <p>What we change (padding mask).</p> <ul> <li> <p>We pad shorter sequences with a special token      \\(\\langle\\text{pad}\\rangle\\) so all sequences share the same length.</p> </li> <li> <p>We add a second mask that blocks attention to any position containing      \\(\\langle\\text{pad}\\rangle\\). This mask is specific to each sequence in the      batch.</p> </li> </ul> <p>Caching. During generation, we repeatedly:</p> <ol> <li>feed the current prefix into the model,</li> <li>use the softmax output to get a distribution over the next token,</li> <li>sample or choose a token, append it, and repeat.</li> </ol> <p>Naively, this means re-running the whole transformer on the entire prefix for    every new token.</p> <p>What we change. Because of the causal mask,    the representation of token \\(i\\) depends only on tokens \\(1,\\dots,i\\) and never    on future tokens. So when we extend the sequence:</p> <ul> <li> <p>At step \\(t\\) we run the full model once for \\(x_1,\\dots,x_t\\) and cache      per-layer key and value tensors \\(K^{(\\ell)}_1,\\dots,K^{(\\ell)}_t\\) and      \\(V^{(\\ell)}_1,\\dots,V^{(\\ell)}_t\\). Note that we do not cache queries because each query is only used once for its own token at that time step and is never reused later, so caching it would waste memory without reducing computation.</p> </li> <li> <p>At step \\(t+1\\) we keep these cached states fixed (earlier tokens are      not allowed to change). We only compute the new token\u2019s hidden states and      its \\(Q^{(\\ell)}_{t+1},K^{(\\ell)}_{t+1},V^{(\\ell)}_{t+1}\\), then run      attention for position \\(t+1\\) using the cached keys/values plus the new ones.</p> </li> </ul> <p>Conceptually we still \u201crun the model\u201d at each step, but most computation is    reused, making long-sequence generation practical.</p>"},{"location":"Transformers-3-LLMs/#how-shifting-and-masking-work-together","title":"How shifting and masking work together","text":"<p>Shifted inputs and causal masking are two separate ideas, but in practice they are always used together in decoder-only language models. The typical order each forward pass is:</p> \\[ \\text{raw tokens} \\;\\Rightarrow\\; \\text{shifted inputs + targets} \\;\\Rightarrow\\; \\text{build mask from positions} \\;\\Rightarrow\\; \\text{run transformer}. \\] <p>Below is the same sentence at each stage.</p> <p>Step 0: The naive case.</p> <p>Raw token sequence:</p> \\[ (x_1, x_2, \\dots, x_N). \\] <p>If we fed this directly into full self-attention, each position \\(n\\) could attend to \\(x_{n+1}, \\dots, x_N\\) and just copy the next token. This would lead to data leakage.</p> <p>Step 1: Apply shifted inputs.</p> <p>First we turn the raw sequence into an input row and a target row:</p> \\[ \\begin{aligned} \\text{inputs} &amp;: (\\langle\\text{start}\\rangle, x_1, x_2, \\dots, x_{N-1}), \\\\ \\text{targets} &amp;: (x_1, x_2, \\dots, x_N). \\end{aligned} \\] <p>Now a single forward pass gives us \\(N\\) next-token predictions in parallel: at position \\(n\\) the model outputs a distribution meant to match target \\(x_n\\). At this stage, if attention were still fully unmasked, position \\(n\\) could still peek at later inputs and cheat.</p> <p>Step 2: Add causal masking.</p> <p>We keep the shifted inputs and targets exactly as above. What we change now is only the attention pattern. We build a mask matrix \\(M \\in \\{0,-\\infty\\}^{N \\times N}\\):</p> \\[ M_{ij} = \\begin{cases} 0      &amp; \\text{if } j \\le i, \\\\ -\\infty &amp; \\text{if } j &gt; i. \\end{cases} \\] <p>This mask is added to the attention logits before the softmax. The final layout looks like this:</p> \\[ \\begin{array}{c|cccc} \\text{position} &amp; 1 &amp; 2 &amp; \\cdots &amp; N \\\\ \\hline \\text{input token}  &amp; \\langle\\text{start}\\rangle &amp; x_1 &amp; \\cdots &amp; x_{N-1} \\\\ \\text{target token} &amp; x_1                        &amp; x_2 &amp; \\cdots &amp; x_N     \\\\ \\text{may attend to input position}&amp; 1                          &amp; 1,2 &amp; \\cdots &amp; 1,\\dots,N \\end{array} \\] <p>So in the final model:</p> <ul> <li>shifting decides which token we try to predict at each position,</li> <li>masking decides which past tokens each position is allowed to use.</li> </ul> <p>Both are applied every time we run the transformer model.</p>"},{"location":"Transformers-3-LLMs/#sampling-strategies","title":"Sampling strategies","text":"<p>As we saw a decoder transformer outputs, at each step, a probability distribution over the next token. To extend a sequence we must turn this distribution into a concrete choice. For this, several strategies are used.</p>"},{"location":"Transformers-3-LLMs/#greedy-search","title":"Greedy search","text":"<p>The simplest method, greedy search, always chooses the token with the highest probability. This makes generation deterministic: the same input prefix always produces the same continuation.</p> <p>Note that choosing the most probable token at each step is not the same as choosing the most probable overall sequence. The probability of a full sequence \\(y_1,\\ldots,y_N\\) is</p> \\[ p(y_1,\\ldots,y_N)=\\prod_{n=1}^N p(y_n \\mid y_1,\\ldots,y_{n-1}) \\] <p>If there are \\(N\\) steps and a vocabulary of size \\(K\\), the number of possible sequences is \\(\\mathcal{O}(K^N)\\), which grows exponentially with \\(N\\), so exhaustively finding the single most probable sequence is infeasible. Greedy search, by contrast, has cost \\(\\mathcal{O}(KN)\\): at each of the \\(N\\) steps it scores all \\(K\\) tokens once and picks the best, so the total work scales linearly with \\(N\\).</p>"},{"location":"Transformers-3-LLMs/#beam-search","title":"Beam search","text":"<p>To get higher-probability sequences than greedy search, we can use beam search. Instead of keeping only one hypothesis, we maintain \\(B\\) partial sequences at step \\(n\\) where \\(B\\) is the beam width. We feed all \\(B\\) sequences through the model and, for each, consider the \\(B\\) most probable next tokens. Since each of the \\(B\\) partial sequences can be extended in \\(B\\) ways, this yields \\(B \\cdot B = B^2\\) candidate sequences, from which we keep the \\(B\\) sequences with the highest total sequence probability. The algorithm therefore tracks \\(B\\) alternatives and their probabilities at all times, and finally returns the most probable sequence among them. For example, with \\(B=2\\) and current beam { <code>I</code>, <code>You</code> }, if the top two continuations for each are { <code>am</code>,<code>like</code> } and { <code>are</code>,<code>like</code> }, the \\(B^2=4\\) candidates are { <code>I am</code>,<code>I like</code>,<code>You are</code>,<code>You like</code> }, from which we keep the best \\(B=2\\).</p> <p>Because the probability of a sequence is a product of stepwise probabilities, and each probability is at most one, long sequences tend to have lower raw probability than short ones. Beam search is therefore usually combined with a length normalization so that different sequence lengths can be compared fairly. Its computational cost is \\(\\mathcal{O}(BKN)\\), still linear in \\(N\\) but \\(B\\) times more expensive than greedy search. For very large language models this extra factor can make beam search unattractive.</p>"},{"location":"Transformers-3-LLMs/#diversity-and-randomness","title":"Diversity and randomness","text":"<p>Greedy and beam search both focus on high-probability sequences, but this often reduces diversity and can even cause loops in which the same subsequence is repeated. Human-written text is often more surprising (lower probability under the model) than automatically generated text.</p> <p>An alternative is to sample the next token directly from the softmax distribution at each step: instead of always taking the single most probable token (the argmax), we treat the softmax output as a categorical distribution over the \\(K\\) tokens and randomly draw one token according to these probabilities. This is same as picking one of the \\(K\\) outputs from the final softmax at each step, but doing so stochastically according to their probabilities rather than deterministically choosing the largest. This can give diverse outputs, but with a large vocabulary the distribution typically has a long tail of very low-probability tokens, and sampling from the full distribution can easily pick poor choices.</p>"},{"location":"Transformers-3-LLMs/#top-k-and-nucleus-sampling","title":"Top-\\(K\\) and nucleus sampling","text":"<p>To balance between determinism and randomness, we can restrict sampling to the most likely tokens. In top-\\(K\\) sampling we keep only the \\(K\\) tokens with highest probability, renormalize their probabilities, and sample from this reduced set. This removes the very low-probability tail, which cuts down on wild or nonsensical tokens but still allows multiple plausible choices.</p> <p>A popular variant is top-\\(p\\) or nucleus sampling. Here we choose the smallest set of tokens whose cumulative probability reaches a threshold \\(p\\), then renormalize and sample only from that subset. Unlike top-\\(K\\), the size of this set adapts to the model\u2019s confidence: when the model is sure, the nucleus is small and more focused; when it is uncertain, the nucleus becomes larger and more diverse.</p>"},{"location":"Transformers-3-LLMs/#temperature","title":"Temperature","text":"<p>A softer way to control randomness is to introduce a temperature parameter \\(T\\) into the softmax:</p> \\[ y_i = \\frac{\\exp(a_i/T)}{\\sum_j \\exp(a_j/T)} \\] <p>We then sample the next token from this modified distribution.</p> <ul> <li>\\(T=0\\) concentrates all probability on the most likely token   (greedy selection).</li> <li>\\(T=1\\) recovers the original softmax distribution.</li> <li>\\(T \\to \\infty\\) gives a uniform distribution over all tokens.</li> <li>For \\(0 &lt; T &lt; 1\\), probability mass is pushed towards higher-probability   tokens.</li> </ul> <p>Training vs. generation (exposure bias).</p> <p>During training, the model sees human-generated sequences as input. During generation, however, the input prefix is itself model-generated. Over time, this mismatch can cause the model to drift away from the distribution of sequences present in the training data, which is an important challenge in sequence generation.</p>"},{"location":"Transformers-3-LLMs/#encoder-transformers","title":"Encoder transformers","text":"<p>Encoder-based transformer language models take a whole sequence as input and turn it into one or more fixed-size vectors. These vectors can then be used to predict a discrete category (a class label), such as positive vs.\\ negative sentiment, or spam vs. not spam. In other words, they encode the entire sentence into one or more summary representations, but they do not generate text by themselves. This contrasts with decoder models, which are trained to predict the next token and can therefore generate sequences, and with encoder\u2013decoder models, which first encode an input sequence and then use a decoder to generate a separate output sequence (as in machine translation). A key example is BERT (bidirectional encoder representations from transformers). The idea is:</p> <ul> <li>pre-train a transformer encoder on a huge text corpus,</li> <li>then apply transfer learning by fine-tuning it on many downstream   tasks, each with a much smaller task-specific data set.</li> </ul>"},{"location":"Transformers-3-LLMs/#pre-training-with-masked-tokens","title":"Pre-training with masked tokens","text":"<p>Our goal in the pre-training stage is to teach the encoder a rich, general understanding of language using only raw text (no human labels). To do this we use a self-supervised prediction task that encourages the model to use both the preceding and following words around a token, so it learns bidirectional representations. This turns plain text into its own source of supervision: by hiding some words and asking the model to guess them, we create huge numbers of training examples for free while directly teaching it to understand how words fit together in context.</p> <p>BERT achieves this with a masked language modelling objective. Every input sequence begins with a special token \\(\\langle\\text{class}\\rangle\\) whose output is ignored during pre-training but will be used later. The model is then trained on sequences of tokens where a random subset (e.g. \\(15\\%\\) of tokens) is replaced by a special \\(\\langle\\text{mask}\\rangle\\) token. The task is then to predict the original tokens at the corresponding output positions. For example take the input sequence:</p> \\[ \\text{I } \\langle\\text{mask}\\rangle \\text{ across the river to get to the } \\langle\\text{mask}\\rangle \\text{ bank.} \\] <p>The network should output \u201cswam\u201d at position \\(2\\) and \u201cother\u201d at position \\(10\\) while all other outputs are ignored for computing the loss. As a result of bidirection:</p> <ul> <li>we do not shift inputs to the right (no autoregressive structure),</li> <li>we do not need causal masks to hide future tokens.</li> </ul> <p>Compared with decoder models, this is less efficient for training, because only a subset of tokens provide supervised targets, and the encoder alone cannot generate sequences. If we always replaced the chosen tokens by \\(\\langle\\text{mask}\\rangle\\) during pre-training, the model would mainly learn to handle inputs that contain many \\(\\langle\\text{mask}\\rangle\\) symbols. At fine-tuning and test time, however, it is given normal sentences with no \\(\\langle\\text{mask}\\rangle\\) tokens, so the input distribution looks very different from what it saw during pre-training. This mismatch can make the learned representations less useful, because the model has had much less practice dealing with real words in those positions. To reduce this gap, we can adjust the \\(15\\%\\) selected tokens as follows:</p> <ul> <li>\\(80\\%\\) are replaced by \\(\\langle\\text{mask}\\rangle\\),</li> <li>\\(10\\%\\) are replaced by a random vocabulary token,</li> <li>\\(10\\%\\) are left unchanged (but the model is still trained to predict   them).</li> </ul>"},{"location":"Transformers-3-LLMs/#fine-tuning-for-downstream-tasks","title":"Fine-tuning for downstream tasks","text":"<p>Once the encoder is pre-trained, we attach a task-specific output layer and fine-tune the whole model.</p> <ul> <li>Sequence-level classification (e.g. sentiment).   The input can be a whole sentence or multiple paragraphs, tokenized and fed through   the encoder with the \\(\\langle\\text{class}\\rangle\\) token at the first   position. After the final encoder layer we get one output vector for each   input token \\(h_0, h_1, \\cdots , h_n\\). The first one, \\(h_{0} \\in \\mathbb{R}^D\\) is treated as a summary of the entire sequence.</li> </ul> <p>To turn this summary into a label, we attach a small task-specific   classifier on top. The simplest choice is a linear layer with parameter   matrix \\(W \\in \\mathbb{R}^{K \\times D}\\) and bias \\(b \\in \\mathbb{R}^K\\),   giving logits</p> \\[ z = W h_{\\text{class}} + b . \\] <p>For \\(K\\)-way classification we apply a softmax to \\(z\\) to get class   probabilities. For binary classification (\\(K=2\\)) like positive or negative sentiment, a common variant is   to use a single output score \\(s = w^\\top h_{\\text{class}} + b\\) followed   by a logistic sigmoid. This linear head is just a minimal example, in practice we can replace   it with a small MLP or any other differentiable module that maps   \\(h_{0}\\) to the desired label space.</p> <ul> <li>Token-level classification (e.g. tagging each word as person,   place, colour, etc.).   Here the input is again a full sequence as above with the \\(\\langle\\text{class}\\rangle\\) token at the beginning. After passing this   sequence through the encoder, we obtain one hidden vector for each input   position as well:</li> </ul> \\[ h_0, h_1, \\ldots, h_N \\in \\mathbb{R}^D, \\] <p>where \\(h_0\\) corresponds to \\(\\langle\\text{class}\\rangle\\) and   \\(h_1,\\ldots,h_N\\) correspond to the actual tokens in the sentence.</p> <p>For token-level labelling we ignore \\(h_0\\) and attach the same   linear classifier to each of the remaining hidden states. Concretely, we   use a weight matrix \\(W \\in \\mathbb{R}^{K \\times D}\\) and bias   \\(b \\in \\mathbb{R}^K\\) (shared across positions). For each token position   \\(i = 1,\\ldots,N\\) we compute</p> \\[ z_i = W h_i + b, \\] <p>and apply a softmax to \\(z_i\\) to obtain a probability distribution over   \\(K\\) possible labels for that token (e.g. <code>PERSON</code>, <code>LOC</code>,   <code>COLOR</code>, etc.).</p> <p>During training, each token in the input sequence has a ground-truth   label, and we sum the cross-entropy loss over all token positions   (optionally skipping special tokens such as padding). At test time, we   simply pick the most likely label for each position, giving a predicted   tag sequence aligned with the original input tokens.</p> <p>During fine-tuning, all parameters, including the new output layer, are updated using stochastic gradient descent to maximize the log probability of the correct labels. Finally, instead of a simple classifier head, the encoder\u2019s representations can also be fed into a more advanced generative model, for example in text-to-image synthesis systems.</p>"},{"location":"Transformers-3-LLMs/#sequence-to-sequence-transformers","title":"Sequence-to-sequence transformers","text":"<p>The third family of transformer models combines an encoder with a decoder, as in the original transformer paper of Vaswani et al. (2017). A typical example is machine translation, say from English to Dutch. Let:</p> <ul> <li>\\(x_1,\\dots,x_M\\) = tokens of the English sentence </li> <li>\\(y_1,\\dots,y_N\\) = tokens of the Dutch sentence </li> </ul>"},{"location":"Transformers-3-LLMs/#decoder-only-transformer-gpt-style","title":"Decoder-only transformer (GPT-style)","text":"<p>Here we model a single sequence only, lets say english:</p> <ul> <li>Input tokens: \\(x_1,\\dots,x_M\\).</li> <li>Each \\(x_t\\) is embedded to a vector \\(e_t\\) with \\(e_t \\in \\mathbb{R}^D\\), using an embedding matrix \\(E \\in \\mathbb{R}^{K \\times D}\\).</li> <li>Masked self-attention processes \\((e_1,\\dots,e_{t-1})\\) to produce a   hidden state \\(h_t\\) for position \\(t\\) (with \\(h_t \\in \\mathbb{R}^D\\)).</li> <li>A linear+softmax layer turns \\(h_t\\) into a distribution over the English   vocabulary:</li> </ul> \\[ p(x_t \\mid x_1,\\dots,x_{t-1}). \\] <p>Concretely, a weight matrix \\(W^{\\text{out}} \\in \\mathbb{R}^{D \\times K}\\)   and bias \\(b^{\\text{out}} \\in \\mathbb{R}^K\\) map \\(h_t\\) to logits in   \\(\\mathbb{R}^K\\), which are then passed through a softmax to get a length-\\(K\\)   probability vector. So, for this we only have one sequence and one language. Each token is predicted from the   previous tokens in that same sequence.</p>"},{"location":"Transformers-3-LLMs/#encoderdecoder-transformer-seq2seq-for-translation","title":"Encoder\u2013decoder transformer (seq2seq for translation)","text":"<p>Now we truly have two sequences:</p> \\[ x_1,\\dots,x_M \\ (\\text{English source}), \\qquad y_1,\\dots,y_N \\ (\\text{Dutch target}). \\] <p>Encoder (English side).</p> <p>(i) Each English token \\(x_m\\) is embedded to \\(e^{\\text{src}}_m\\)   (with \\(e^{\\text{src}}_m \\in \\mathbb{R}^D\\), using a source embedding matrix   \\(E^{\\text{src}} \\in \\mathbb{R}^{K_{\\text{src}} \\times D}\\)).</p> <p>(ii) Bidirectional self-attention over all \\(e^{\\text{src}}_1,\\dots,e^{\\text{src}}_M\\)   produces encoder states \\(z_1,\\dots,z_M\\)   (each \\(z_m \\in \\mathbb{R}^D\\)).   Each \\(z_m\\) summarizes information about the whole English   sentence, but is still tied to position \\(m\\).</p> <p>Decoder (Dutch side).</p> <p>(i) We have a target (Dutch) sequence with tokens \\(y_1,\\dots,y_N\\). During training   we feed the decoder the shifted input sequence   \\((\\langle\\text{start}\\rangle, y_1,\\dots,y_{N-1})\\) and train it to predict   \\((y_1,\\dots,y_N)\\).</p> <p>(ii) Each token in this decoder input is embedded   to \\(e^{\\text{tgt}}_n\\) (with \\(e^{\\text{tgt}}_n \\in \\mathbb{R}^D\\), via a target embedding matrix \\(E^{\\text{tgt}} \\in \\mathbb{R}^{K_{\\text{tgt}} \\times D}\\)).</p> <p>(iii) Masked self-attention over these target embeddings produces intermediate   states \\(\\hat{h}_n\\), where each \\(\\hat{h}_n\\) can only attend to earlier positions in   the decoder input, i.e. to \\(y_1,\\dots,y_{n-1}\\) (no peeking at future Dutch   tokens). Each \\(\\hat{h}_n \\in \\mathbb{R}^D\\).</p> <p>(iv) Now we use Cross-attention where the key and query comes from different datasets or sequences (here English and Dutch). In this for each position \\(n\\) we:</p> <ul> <li>use \\(\\hat{h}_n\\) as a query \\(q_n\\),</li> <li>use all encoder states \\(z_1,\\dots,z_M\\) as keys and values.</li> </ul> <p>Concretely, we apply learned projection matrices   \\(W^Q, W^K, W^V \\in \\mathbb{R}^{D \\times D}\\) to obtain</p> \\[   q_n = \\hat{h}_n W^Q,\\quad   k_m = z_m W^K,\\quad   v_m = z_m W^V, \\] <p>where \\(q_n, k_m, v_m \\in \\mathbb{R}^D\\). The attention scores are first computed from the query \\(q_n\\) and each   key \\(k_m\\):</p> \\[   s_{n,m} = q_n^\\top k_m, \\] <p>so \\(s_{n,m}\\) is a scalar, and the score matrix \\(S = [s_{n,m}]\\) has shape   \\(\\mathbb{R}^{N \\times M}\\). We then turn these scores into attention weights by applying a softmax over   \\(m\\):</p> \\[   \\alpha_{n,m}   = \\frac{\\exp(s_{n,m})}{\\sum_{j=1}^M \\exp(s_{n,j})}   \\;\\;\\propto\\;\\; \\exp(q_n^\\top k_m). \\] <p>Each \\(\\alpha_{n,m}\\) is a scalar, and for fixed \\(n\\) the vector   \\((\\alpha_{n,1},\\dots,\\alpha_{n,M})\\) lies in \\(\\mathbb{R}^M\\) and sums to \\(1\\). Finally, the cross-attention output at position \\(n\\) is a weighted average of   the value vectors \\(v_m\\):</p> \\[   c_n = \\sum_{m=1}^M \\alpha_{n,m} v_m, \\] <p>so \\(c_n \\in \\mathbb{R}^D\\), and the representation at position \\(n\\) can directly \u201clook at\u201d any   English position \\(m\\) via its weight \\(\\alpha_{n,m}\\).</p> <p>(v) \\(c_n\\) is then combined with \\(\\hat{h}_n\\) using the usual transformer   block structure: first a residual (skip) connection, then layer   normalization, and then a feed-forward network. Concretely, we can write</p> \\[   u_n = \\mathrm{LayerNorm}(\\hat{h}_n + c_n), \\qquad   h_n = \\mathrm{FFN}(u_n), \\] <p>where \\(\\mathrm{FFN}\\) is a small position-wise MLP that maps   \\(\\mathbb{R}^D \\to \\mathbb{R}^D\\). Both \\(u_n\\) and \\(h_n\\) are in   \\(\\mathbb{R}^D\\). The final state   \\(h_n\\) then goes through a linear+softmax layer to produce</p> \\[   p(y_n \\mid y_1,\\dots,y_{n-1}, x_1,\\dots,x_M). \\] <p>Here a projection \\(W^{\\text{tgt}} \\in \\mathbb{R}^{D \\times K_{\\text{tgt}}}\\) and bias   \\(b^{\\text{tgt}} \\in \\mathbb{R}^{K_{\\text{tgt}}}\\) map \\(h_n\\) to logits in   \\(\\mathbb{R}^{K_{\\text{tgt}}}\\), followed by a softmax over the \\(K_{\\text{tgt}}\\) target tokens.</p> <p>(vi) The encoder and decoder are trained together, end-to-end. A   training pair \\((x_{1:M}, y_{1:N})\\) consists of a source sequence \\(x_{1:M}\\) (e.g. an English sentence) and   its corresponding target sequence \\(y_{1:N}\\) (e.g. the Dutch translation).   For each training pair, we:</p> <ul> <li>run the encoder on the entire source sequence \\(x_{1:M}\\),</li> <li>feed the shifted target sequence     \\((\\langle\\text{start}\\rangle, y_1,\\dots,y_{N-1})\\) into the decoder,</li> <li>obtain a predicted distribution for every position in the target     sequence and compute a cross-entropy loss at each step \\(n\\) for \\(y_n\\).</li> </ul> <p>In this way, every part of the target sequence contributes to the loss, and   gradients update all parameters (encoder, decoder, and   cross-attention) jointly. </p> <p>Key points:</p> <ul> <li>We still never allow \\(y_n\\) to see future \\(y_{n+1},y_{n+2},\\dots\\),   so there is no data leakage.</li> <li>What changes compared to the decoder-only model is that each   target token \\(y_n\\) can now attend to all encoder states   \\(z_1,\\dots,z_M\\), i.e. to the entire English sentence \\(x_1,\\dots,x_M\\),   via cross-attention.</li> </ul> <p>Intuitively, this is like a user sending their query to a different streaming service: the service compares the query with its own library of key vectors and returns the best-matching movie as the value vector. When we wire the encoder and decoder together in this way we obtain the classic sequence-to-sequence transformer architecture. The model is trained in a supervised way using sentence pairs: for each English input sentence \\(x_1,\\dots,x_M\\) we provide the corresponding Dutch output sentence \\(y_1,\\dots,y_N\\), and the network learns to map the full source sentence to its correct translated target sentence.</p>"},{"location":"Transformers-3-LLMs/#large-language-models","title":"Large Language models","text":"<p>One of the biggest shifts in modern machine learning has been the rise of very large transformer-based neural networks for language, called large language models (LLMs). Here 'large' refers to the number of learnable weights and biases, which at the time of writing can reach around one trillion \\((10^{12})\\). These models are expensive to train, but their extraordinary capabilities make them worth the effort.</p> <p>Several trends made LLMs possible:</p> <ul> <li>huge text data sets,</li> <li>massively parallel hardware, especially GPUs and related accelerators,   organized in large clusters with fast interconnects and lots of memory,</li> <li>the transformer architecture, which uses this hardware very efficiently.</li> </ul> <p>In practice, simply scaling up data and parameter count often improves performance more than clever architectures or hand-built domain knowledge. For example, the big performance jumps in the GPT series have mainly come from increased scale. This has led to a new kind of \u201cMoore\u2019s law\u201d for ML: since about 2012, the compute needed to train a state-of-the-art model has grown exponentially, with a doubling time of roughly about \\(3.4\\) months.</p> <p>From supervised to self-supervised training</p> <p>Early language models were trained with supervised learning. For instance, to build a translation system one would use many pairs of aligned sentences in two languages. The problem is that such labelled data must be curated by humans, so it is scarce. This forces strong inductive biases (feature engineering, rigid architectures) just to reach decent performance.</p> <p>LLMs instead use self-supervised learning on very large unlabelled data sets of text (and often other token sequences such as source code). As we saw with decoder transformers, we can treat each token in a sequence as a labelled target, with the previous tokens as input, and learn a conditional probability distribution over the next token. This \u201cself-labelling\u201d turns raw text into a massive training set and makes it practical to exploit deep networks with huge numbers of parameters.</p> <p>Pre-training, fine-tuning, and foundation models.</p> <p>This self-supervised approach led to a new training paradigm:</p> <ol> <li>Pre-train a large model on unlabelled data.</li> <li>Fine-tune it with supervised learning on a much smaller    labelled data set for a particular task.</li> </ol> <p>This is a form of transfer learning, and the same pre-trained model can be reused across many downstream applications. A broadly capable model that can be fine-tuned for many tasks is called a foundation model. Fine-tuning can be done in various ways:</p> <ul> <li>add new layers on top and train them on labelled data,</li> <li>or replace the last few layers with new parameters and train only those.</li> </ul> <p>During fine-tuning, the main network weights can be frozen or allowed small adjustments. In either case, the compute cost of fine-tuning is usually tiny compared with pre-training.</p> <p>Low-rank adaptation (LoRA).</p> <p>A very efficient fine-tuning method is low-rank adaptation. It is motivated by the observation that over-parameterized models often have low intrinsic dimensionality for fine-tuning: useful parameter changes lie on a much lower-dimensional manifold than the full space.</p> <p>LoRA keeps the original model weights fixed and adds small trainable low-rank matrices to each transformer layer, usually only in the attention blocks (MLP layers stay fixed). Consider a weight matrix \\(\\mathbf{W}_0 \\in \\mathbb{R}^{D\\times D}\\), representing, say, the combined query/key/value matrix for all attention heads. LoRA introduces a parallel set of weights \\(\\mathbf{A} \\in \\mathbb{R}^{D\\times R}\\) and \\(\\mathbf{B} \\in \\mathbb{R}^{R\\times D}\\) freezing the weights \\(W_0\\), and the layer output becomes</p> \\[ X\\mathbf{W}_0 + X\\mathbf{A}\\mathbf{B}. \\] <p>The added matrix \\(\\mathbf{A}\\mathbf{B}\\) has \\(2RD\\) parameters, compared with the \\(D^2\\) parameters of \\(\\mathbf{W}_0\\). When \\(R \\ll D\\), the number of parameters that must be trained is much smaller than in the original transformer, often by up to a factor of \\(10{,}000\\) in practice. After fine-tuning, the adapted weights are simply merged into the original matrix:</p> \\[ \\widehat{\\mathbf{W}} = \\mathbf{W}_0 + \\mathbf{A}\\mathbf{B} \\] <p>so at inference time there is no extra computational cost and the model size is the same as before. As LLMs grow more powerful, we can increasingly skip fine-tuning altogether and instead solve many tasks directly through text-based interaction.</p> <p>Zero-shot and few-shot capabilities.</p> <p>Zero-shot learning means the model is asked to do a task that it has never seen explicitly in training, and we give it only an instruction or pattern in the prompt (no examples). Few-shot learning is similar, but we also give a small number of input\u2013output examples in the prompt to guide the model.</p> <p>As a simple zero-shot example, suppose we give the following text to a generative language model:</p> <p>English: the cat sat on the mat. French:</p> <p>Here the French part is intentionally left blank, this is the task we want the model to solve. If this whole line is used as the input sequence, an autoregressive language model will continue generating tokens until it produces a special \\(\\langle\\text{stop}\\rangle\\) token. The generated continuation will typically be a French translation of the English sentence. Crucially, the model was never directly trained as a translation system, it acquired this ability indirectly by being pre-trained on a very large corpus of text that includes many different languages.</p> <p>Interaction, RLHF, and ChatGPT.</p> <p>Users can interact with such models via natural language dialogue, making them highly accessible. To improve user experience and output quality, LLMs are often aligned using human feedback. A popular approach is reinforcement learning through human feedback (RLHF), where humans rate model outputs and these ratings are used to further train the model. These techniques have enabled easy-to-use conversational systems such as OpenAI\u2019s ChatGPT.</p> <p>Prompts and prompt engineering.</p> <p>The sequence of input tokens provided by the user is called a prompt. It might be:</p> <ul> <li>the opening of a story for the model to complete,</li> <li>a question the model should answer,</li> <li>a request such as \u201cwrite Python code that \u2026\u201d or \u201ccompose a   rhyme about \u2026\u201d.</li> </ul> <p>By changing the prompt, the same underlying network can perform many tasks: code generation from a plain-text description, poetry on demand, and much more. Model performance therefore depends strongly on how we phrase the prompt. This has led to a new field, prompt engineering, which focuses on designing prompts that yield high-quality outputs.</p> <p>We can also modify behaviour by automatically editing the user\u2019s prompt before it reaches the model. A common method is to prepend an additional token sequence called a prefix prompt. For instance, the prefix might contain instructions in standard English telling the model to avoid offensive language. The main prompt then follows this prefix.</p> <p>This mechanism lets us solve new tasks simply by including a few examples or instructions in the prompt, without changing model parameters, an ability known as few-shot learning.</p> <p>Current state and outlook.</p> <p>State-of-the-art models such as GPT-5 already show striking capabilities that some authors describe as early signs of artificial general intelligence. They are driving a major new wave of technological innovation, and their abilities continue to advance at an impressive pace.</p>"},{"location":"Transformers-3-LLMs/#references","title":"References","text":"<ul> <li>Bishop, C. M., &amp; Bishop, H. (2023). Transformers. In Deep Learning: Foundations and Concepts (pp. 357-406). Cham: Springer International Publishing.</li> <li>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... and Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.</li> </ul>"},{"location":"Transformers-4-MMT/","title":"Multimodal Transformers","text":""},{"location":"Transformers-4-MMT/#introduction","title":"Introduction","text":"<p>Transformers were first introduced as an alternative to recurrent networks for handling sequential language data. Today, they are used across almost all areas of deep learning.</p> <p>A key reason is that transformers are very general\u2014they make only weak assumptions about the structure of the input. This contrasts with convolutional networks, which strongly assume that important patterns are local and behave similarly when shifted across the input (equivariance and locality).</p> <ul> <li>Locality means that each neuron in a convolutional layer only looks at a small neighborhood (a local patch) of the input, not the whole image or signal at once. We are building in the belief that useful features like edges, corners, or textures can be detected from nearby pixels.</li> <li>Equivariance means that if we shift the input (for example, move an object a few pixels to the left), the feature maps produced by the convolution also shift in the same way. The pattern is recognized no matter where it appears, and the response simply moves along with it. This is a very strong built-in assumption about how patterns behave across space, and it is one of the reasons convolutions work so well on images but are less general than transformers.</li> </ul> <p>Because of this generality, transformers have become state-of-the-art on many data types, including text, images, video, point clouds, and audio. Within each of these domains, they are used for both discriminative tasks (such as classification) and generative tasks (such as synthesis). Interestingly, the basic transformer layer architecture has stayed almost the same over time and across applications. Most of the innovation needed to move from pure language to other domains has focused instead on how we represent and encode the inputs and outputs so that the transformer can work with them.</p> <p>Having a single architecture that can process many kinds of data also makes multimodal applications much easier. Here, \u201cmultimodal\u201d means we combine two or more types of data in the inputs, the outputs, or both. For example, we can generate an image from a text prompt, or we can build a robot that fuses information from cameras, radar, and microphones.</p> <p>The key takeaway is simple: if we can tokenize the inputs and later decode the output tokens back into a useful form, there is a good chance that a transformer can be applied.</p>"},{"location":"Transformers-4-MMT/#vision-transformers","title":"Vision transformers","text":"<p>Transformers also work very well for vision and now reach state-of-the-art results on many image tasks. When we use a standard transformer encoder on images, we call it a vision transformer (ViT). To use a transformer, we must first turn an image into a sequence of tokens. The simplest idea is to treat each pixel as one token after a linear projection, but this is usually impossible in practice because in this case the memory cost of a transformer grows roughly with the square of the number of tokens/pixels. Instead, ViTs almost always use patch tokens. Assume an image</p> \\[ \\mathbf{x} \\in \\mathbb{R}^{H \\times W \\times C}, \\] <p>where \\(H\\) and \\(W\\) are height and width in pixels and \\(C\\) is the number of channels (typically \\(C=3\\) for RGB). In a ViT we make a different design choice: one token = one image patch. So pixels are no longer tokens. To achieve this we cut the image into non-overlapping \\(P \\times P\\) patches (e.g. \\(P = 16\\)). Each patch contains \\(P \\times P \\times C\\) pixel values, which we reshape into a vector of length \\(P^{2}C\\). This raw patch vector is then mapped, via a linear layer, to a \\(D\\)-dimensional embedding, that \\(D\\)-dimensional vector is one token. Doing this for all patches gives</p> \\[ \\mathbf{x}_p \\in \\mathbb{R}^{N \\times (P^{2} C)}, \\] <p>before the linear projection, where</p> \\[ N = \\frac{HWC}{P^{2}C} = \\frac{HW}{P^{2}} \\] <p>is the number of patches, and therefore the number of tokens. Each patch (and thus each token) contains \\(P^{2}\\) pixels in space and \\(P^{2}C\\) pixel values in total (because there are \\(C\\) channels). After projection, each token is a \\(D\\)-dimensional vector fed into the transformer.</p> <p>Another way to create tokens is to first pass the image through a small convolutional neural network (CNN). The CNN down-samples the spatial resolution, and we then treat each spatial location of the final feature map as one token. For example, a typical ResNet18 encoder reduces height and width each by a factor of \\(8\\), so we obtain \\(64\\) times fewer tokens than raw pixels.</p> <p>Positional embeddings. We also need to encode where each patch comes from. One option is to build explicit 2D positional embeddings that represent the \\((x,y)\\) location of each patch. In practice, this rarely helps compared to simply learning a positional embedding vector per token index, so learned positional embeddings are more common. Unlike NLP transformers, vision transformers usually assume a fixed number of tokens (for example, always \\(14 \\times 14\\) patches), so these learned embeddings do not need to generalize to different input lengths or image sizes.</p> <p>Architecturally, a ViT is very different from a CNN. CNNs have strong built-in inductive biases: weight sharing, locality, and approximate translation equivariance. In a ViT, the only real inductive bias is the decision to slice the image into patches, everything else about image geometry must be learned from data. As a result, ViTs typically need more training data than comparable CNNs. The upside is that, because they do not hard-code many assumptions about the input structure, transformers can often reach higher accuracy once enough data and compute are available. This nicely illustrates the trade-off between strong inductive biases and the amount of training data.</p>"},{"location":"Transformers-4-MMT/#generative-image-transformers","title":"Generative image transformers","text":"<p>In language, transformers shine when used as autoregressive generators: they predict the next token given all previous ones and can synthesize long texts. A natural question is whether we can do the same for images. Language is intrinsically sequential, so an autoregressive ordering is obvious. Images, in contrast, have no natural pixel order. Mathematically, however, any joint distribution over variables \\(\\mathbf{x}_1,\\dots,\\mathbf{x}_N\\) can be written as a product of conditionals once we pick some ordering:</p> \\[ p(\\mathbf{x}_1,\\dots,\\mathbf{x}_N) = \\prod_{n=1}^N p(\\mathbf{x}_n \\mid \\mathbf{x}_1,\\dots,\\mathbf{x}_{n-1}) \\] <p>This factorization is completely general and does not restrict the form of the conditionals \\(p(\\mathbf{x}_n \\mid \\mathbf{x}_1,\\dots,\\mathbf{x}_{n-1})\\).</p> <p>For images, lets assume we can choose \\(\\mathbf{x}_n\\) to be the RGB vector of the \\(n\\)-th pixel. We then need an ordering of pixels. A common choice is a raster scan (left-to-right, top-to-bottom). Generating an image autoregressively means sampling each pixel in this raster scan order using the above equation. Autoregressive image models existed well before transformers. PixelCNN and PixelRNN, for example, used specially masked convolutions so that each pixel only depends on earlier pixels in the raster order.</p> <p>Real-valued (continuous) image representations work very well for discriminative tasks such as classification: a CNN can map real-valued pixels to real-valued features and then to class scores with no problem. For generation, however, if we model the conditionals with continuous distributions such as Gaussians and train by maximum likelihood, the model is encouraged to predict the average of all plausible pixel values, which often leads to smooth, blurry images. Discrete representations avoid this by treating each pixel or patch as choosing from a finite set of codes and modelling a categorical distribution (over these codes) with a softmax. In this case a conditional \\(p(\\mathbf{x}_n \\mid \\mathbf{x}_1,\\dots,\\mathbf{x}_{n-1})\\) can assign high probability to both \u201cblack\u201d and \u201cwhite\u201d for a pixel, rather than collapsing to \u201cgrey\u201d, so sharp, multimodal structure is captured much more naturally and samples are typically higher quality.</p> <p>However, working directly with discrete pixels is still difficult. A single colour pixel has 8 bits for each of the three RGB channels, so each channel can take \\(2^8 = 256\\) values. The total number of possible colours per pixel is \\(256^3 = 2^{24} \\approx 16\\text{M}\\), so using a separate softmax over all options for every pixel is computationally impractical. A popular fix is vector quantization, which we can view as learned compression.</p>"},{"location":"Transformers-4-MMT/#vector-quantization","title":"Vector quantization","text":"<p>Assume our dataset can be written as a matrix \\(\\mathbf{X} \\in \\mathbb{R}^{N \\times D}\\), where each row is a data vector \\(\\mathbf{x}_1,\\dots,\\mathbf{x}_N \\in \\mathbb{R}^D\\) (e.g. pixels). We also have a set of \\(K\\) codebook vectors \\(\\mathcal{C} = \\{\\mathbf{c}_1,\\dots,\\mathbf{c}_K\\} \\subset \\mathbb{R}^D\\), with \\(K \\ll D\\). We approximate each data vector by its nearest codebook vector, usually in Euclidean distance:</p> \\[ \\mathbf{x}_n \\;\\rightarrow\\; \\arg\\min_{\\mathbf{c}_k \\in \\mathcal{C}} \\|\\mathbf{x}_n - \\mathbf{c}_k\\|^2 \\] <p>Because there are only \\(K\\) codebook vectors, we can represent each \\(\\mathbf{x}_n\\) by a \\(K\\)-dimensional one-hot code, so the whole dataset becomes a matrix of codes \\(\\mathbf{Z} \\in \\{0,1\\}^{N \\times K}\\) (or, equivalently, an index vector in \\(\\{1,\\dots,K\\}^N\\)). By choosing \\(K\\), we control a trade-off: larger \\(K\\) gives a more faithful representation, smaller \\(K\\) gives stronger compression. We can now map all pixels into this lower-dimensional codebook space, train an autoregressive transformer to generate sequences of code indices, and finally map these indices back to image pixels by replacing index \\(k\\) with its codebook vector \\(\\mathbf{c}_k\\). This reconstruction is only approximate: we generally cannot recover the exact original \\(\\mathbf{x}_n\\), only its nearest codebook vector \\(\\mathbf{c}_k\\). In practice, we pick \\(K\\) (and learn the codebook) so that this quantization error is small enough that the generated images still look sharp and realistic.</p> <p>ImageGPT was one of the first autoregressive transformers for images. It clusters the colour space with \\(K\\)-means and treats each pixel as belonging to one of the resulting \\(K\\) RGB codebook vectors. The one-hot codes act as discrete tokens, just like words in language models, and the transformer is trained with a next-token prediction loss. This objective gives strong image representations that can be fine-tuned for downstream tasks, mirroring language modelling.</p> <p>As in vision transformers, it is more efficient to use patches as tokens. Fewer tokens make high-resolution images feasible. We still want discrete tokens to capture multimodal conditionals, but now the dimensionality explodes: the number of possible patches grows exponentially with the number of pixels in a patch. If we take \\(16\\times 16\\) patches with just two pixel values (black/white), there are</p> \\[ 16 \\times 16 = 256 \\quad\\Rightarrow\\quad \\text{number of patches} = 2^{256} \\approx 1.16 \\times 10^{77}, \\] <p>since each of the \\(256\\) pixels can independently be black or white.</p> <p>So we again turn to vector quantization, now applied to patches. We learn a codebook of patch vectors from data, using methods like \\(K\\)-means, fully convolutional networks, or even vision transformers. A difficulty is that the quantization step (the nearest-codebook lookup) is not differentiable. In practice, we use the straight-through estimator: during backpropagation we simply copy gradients through the non-differentiable step as if it were the identity function. Finally, the same idea extends naturally from images to videos. We treat a video as one long sequence of vector-quantized tokens (over space and time) and train an autoregressive transformer over this sequence to generate videos frame by frame.</p>"},{"location":"Transformers-4-MMT/#audio-data","title":"Audio data","text":"<p>Transformers can also process audio. Raw sound is usually stored as a waveform: a sequence of air-pressure samples over time. Instead of using this directly, we usually convert it to a mel spectrogram, a matrix whose columns are time steps and rows are frequency bands on the mel scale, designed so equal steps roughly match equal perceived pitch changes.</p> <p>A core task is audio classification, where short clips are assigned labels such as car, animal, or laughter. A common benchmark is the AudioSet dataset. Before transformers, the best systems treated mel spectrograms as images and used CNNs. CNNs capture local patterns well but struggle with long-range temporal dependencies, which often matter for audio.</p> <p>Now, transformers are increasingly used instead. A transformer encoder with the same architecture as in language or vision can classify audio. We view the mel spectrogram as an image, split it into patches (optionally overlapping), and flatten each patch into a 1D vector. Each patch becomes a token, we add positional encodings, prepend a special <code>&lt;class&gt;</code> token, and feed all tokens to the transformer encoder. The final output at the <code>&lt;class&gt;</code> position goes through a linear layer and a softmax, and the whole model is trained end-to-end with a cross-entropy loss.</p>"},{"location":"Transformers-4-MMT/#text-to-speech","title":"Text-to-speech","text":"<p>Classification is only one success story for transformers in audio. Another is text-to-speech (TTS): generating spoken audio that follows a given text, often in the voice of a specific speaker. In a traditional TTS system, we record many examples of a single speaker and train a supervised regression model to map text to a low-level representation of speech, such as a mel spectrogram. At inference time, we feed in new text, get a predicted spectrogram, and convert it deterministically back to a waveform.</p> <p>This setup has several drawbacks. If we predict very low-level units (for example, phonemes), the model must handle long-range context to make sentences sound natural. If we instead predict longer segments, the input space becomes huge and requires impractically large datasets. The approach also does not share knowledge across speakers, so each new voice needs a lot of data. Finally, TTS is inherently a generative problem: many different speech signals are valid for the same text and speaker, while regression tends to average them into bland, less expressive outputs.</p> <p>A more modern view treats speech like language and frames TTS as conditional language modelling. We still use transformers, but now the model predicts the next audio token given previous audio tokens and the input text. The main design questions become: (1) how to tokenize speech so we can decode predictions back to audio, and (2) how to condition the model on the desired speaker\u2019s voice.</p> <p>First, speech is converted into a sequence of speech tokens using vector quantization. We learn a codebook (dictionary) of audio embeddings, split a waveform into short frames, and replace each frame by the index of its nearest codebook vector. During training, the transformer input is a single sequence built by concatenating (i) the text tokens for the sentence we want to speak and (ii) a short run of speech tokens taken from a separate sample of the same speaker. The model is trained to output the speech tokens that correspond to the full spoken version of the input text. Intuitively, the text tokens tell the model what to say, while the conditioning speech tokens tell it in which voice to say it.</p> <p>At test time, we provide new text plus a brief speech sample from a new speaker. The model generates speech tokens conditioned on both the text and this speaker snippet. Finally, we map the tokens back through the same codebook to produce a waveform. This lets the system read out arbitrary text in the voice of a speaker it has only heard for a few seconds.</p>"},{"location":"Transformers-4-MMT/#vision-and-language-transformers","title":"Vision and language transformers","text":"<p>So far we have seen how to build discrete tokens for text, audio, and images. A natural next step is to mix modalities: let the input tokens come from one modality and the output tokens from another, or even use several modalities on both sides. In practice, the most studied case is text + vision, but the same ideas extend to other combinations.</p> <p>The first ingredient is a large multimodal dataset. For text\u2013image work, the LAION-400M dataset has played a role similar to ImageNet for image classification, enabling rapid progress in both text-to-image generation and image captioning. Text-to-image generation is very close to unconditional image generation, except that we now condition on a text prompt. With transformers, conditioning on text is straightforward. We first encode the prompt into text tokens and keep them in the context, at every step when the model predicts the next image token, its attention layers can look back at both the previously generated image tokens and these fixed text tokens, so the visual details follow the words in the prompt.</p> <p>We can also view text-to-image as a standard sequence-to-sequence problem, like machine translation, but with discrete image tokens as the target sequence instead of words. This motivates using a full encoder\u2013decoder transformer: the encoder reads the text tokens \\(X\\), and the decoder outputs the image tokens \\(Y\\). Models such as Parti follow this pattern and scale the transformer to tens of billions of parameters, with performance improving as the model size increases.</p> <p>Another research line starts from large pre-trained language models and adapts them so they can also accept visual inputs. These systems usually use custom modules that map images to continuous feature vectors, which are then injected into the language model. Because the visual representation is tied to a specific encoder and feature format, it is awkward to plug in new kinds of data, such as audio or video, without redesigning this interface. This also makes it harder to apply the same model to generate images, since the model never sees discrete image tokens that could be decoded back into pixels in a unified way. Ideally, we would like a single model that can consume and produce both text and image tokens (and possibly more). The simplest recipe is to treat everything as one long token sequence and define a joint vocabulary that is just the union of the text token dictionary and the image token codebook. The key point is that all modalities now share this one vocabulary, so a single transformer can read and generate mixed streams of tokens (text, image, audio, \u2026) without any modality-specific heads or separate architectures.</p> <p>Models such as CM3 and CM3Leon follow this language-modelling view. They are trained on HTML pages from the web that contain both text and images, using a variant of next-token prediction over the mixed token stream. With enough training data and a scalable architecture, these models become very powerful and flexible: they can do text-to-image generation, image captioning, image editing, text completion, and essentially any task a regular language model can handle, all within a single multimodal transformer.</p>"},{"location":"Transformers-4-MMT/#references","title":"References","text":"<ul> <li>Bishop, C. M., &amp; Bishop, H. (2023). Transformers. In Deep Learning: Foundations and Concepts (pp. 357-406). Cham: Springer International Publishing.</li> </ul>"},{"location":"linearRegression/","title":"Linear Regression \u2014 Intro","text":""},{"location":"linearRegression/#what-problem-are-we-solving","title":"What problem are we solving?","text":"<p>Suppose we have examples \\((\\mathbf{x}_i, y_i)\\) where \\(i = 1,...,n\\), so we have n total examples and we want to predict an output \\(y\\) from the input feature \\(\\mathbf{x}\\). Let us say that we also have a model \\(f(\\mathbf{.})\\) that makes predictions \\(\\hat y\\) as some function of the input x, giving \\(\\hat{y} = f(x)\\). We judge our model by how close \\(\\hat y\\) is to \\(y\\).</p> <p>How wrong we are: The most common way to judge our model is to check it's mean squared error (MSE), given by:</p> \\[ L=\\frac{1}{n}\\sum_{i=1}^n \\bigl(y_i-\\hat y_i\\bigr)^2 \\] <p>where, \\(\\hat{y_i} = f(x_i)\\)</p>"},{"location":"linearRegression/#a-tiny-grounding-example","title":"A tiny grounding example","text":"<p>Suppose \\(x\\) = study hours and \\(y\\) = test score for three students:</p> \\(i\\) \\(x_i\\) (hours) \\(y_i\\) (score) 1 1 2 2 2 3 3 3 5 <p>We will use this toy set to make the formulas concrete.</p>"},{"location":"linearRegression/#baseline-always-predict-the-average","title":"Baseline: Always predict the average","text":"<p>If we must make a constant prediction for everyone \\((\\hat{y_i} =f(x_i) =w_0)\\), the best constant \\((w_0^*)\\) is the average of the training targets, this can be proved as follows. We had our models MSE as:</p> \\[ L=\\frac{1}{n}\\sum_{i=1}^n \\bigl(y_i-w_0\\bigr)^2. \\] <p>We try to minimize this MSE to find the best prediction \\((w_0^*)\\)</p> \\[ \\frac{\\partial L}{\\partial w_0}=0. \\] <p>This gives:</p> \\[ w_0^*=\\bar y=\\frac{1}{n}\\sum_{i=1}^n y_i. \\] <p>This ``do-nothing'' baseline matters because any smart model should beat it, meaning the predictions should be better than this baseline model.</p>"},{"location":"linearRegression/#simple-one-feature-linear-regression","title":"Simple (one-feature) linear regression","text":"<p>Now let the prediction depend on \\(x\\) instead of just being constant, so \\(\\hat{y_i} =f(x_i) \\neq w_0\\) anymore, but:</p> \\[ \\hat y_i = w_0 + w_1 x_i \\] <p>We can then define the error for each sample i as: \\(e_i = y_i-\\hat y_i\\). Then we can write \\(y_i = \\hat y_i + e_i\\). This means the error term captures everything from the reality that the model cannot.</p> <p>Optimal Parameters \\(w_0^*\\) and \\( w_1^*\\): We can write the MSE in this case as:</p> \\[ L=\\frac{1}{n}\\sum_{i=1}^n \\bigl(y_i-w_0-w_1x_i\\bigr)^2. \\] <p>Minimizing MSE gives the ordinary least squares (OLS) solution</p> \\[ w_1^*=\\frac{\\sum (x_i-\\bar x)(y_i-\\bar y)}{\\sum (x_i-\\bar x)^2} =\\frac{\\sigma_{xy}}{\\sigma_x^2} = r_{xy}\\frac{\\sigma_y}{\\sigma_x}, \\qquad w_0^*=\\bar y-w_1^*\\bar x, \\] <p>where,</p> \\[ \\bar x=\\frac{1}{n}\\sum x_i,\\quad \\sigma_x^2=\\frac{1}{n}\\sum (x_i-\\bar x)^2,\\quad \\sigma_{xy}=\\frac{1}{n}\\sum (x_i-\\bar x)(y_i-\\bar y),\\quad r_{xy}=\\frac{\\sigma_{xy}}{\\sigma_x\\sigma_y}. \\]"},{"location":"linearRegression/#metrics-for-goodness-of-fit","title":"Metrics for goodness of fit","text":"<p>We know the residuals \\(e_i=y_i-\\hat y_i\\) with \\(\\hat y_i=w_0+w_1x_i\\), and their (biased) variance is given by:</p> \\[ \\sigma_e^2=\\frac{1}{n}\\sum_{i=1}^n e_i^2. \\] <p>We call it biased because it divides by \\(n\\). The unbiased version (simple regression with intercept) is:</p> \\[ s_e^2=\\frac{1}{n-2}\\sum_{i=1}^n e_i^2. \\] <p>Start from the least squares objective:</p> \\[ L(w_0,w_1)=\\sum_{i=1}^n\\bigl(y_i-w_0-w_1x_i\\bigr)^2. \\] <p>Set the partial derivatives to zero:</p> \\[ \\frac{\\partial S}{\\partial w_0}=-2\\sum (y_i-w_0-w_1x_i)=0,\\qquad \\frac{\\partial S}{\\partial w_1}=-2\\sum x_i(y_i-w_0-w_1x_i)=0. \\] <p>These two equations give the normal equations</p> \\[ \\sum e_i=0 \\ \\Rightarrow\\  w_0=\\bar y - w_1\\bar x, \\qquad \\sum x_ie_i=0 \\ \\Rightarrow\\  w_1=\\frac{\\sum (x_i-\\bar x)(y_i-\\bar y)}{\\sum (x_i-\\bar x)^2}. \\] <p>Define the basic sums</p> \\[ S_{xx}=\\sum (x_i-\\bar x)^2,\\quad S_{yy}=\\sum (y_i-\\bar y)^2,\\quad S_{xy}=\\sum (x_i-\\bar x)(y_i-\\bar y), \\] <p>so \\(w_1=S_{xy}/S_{xx}\\) and \\(w_0=\\bar y-w_1\\bar x\\). Substituting \\(w_0\\) we get:</p> \\[ e_i = y_i - w_0 - w_1x_i = (y_i-\\bar y) - w_1(x_i-\\bar x). \\] <p>Therefore the (biased) residual variance times \\(n\\) (i.e., SSE) is</p> \\[ \\sum e_i^2 =\\sum\\bigl[(y_i-\\bar y)-w_1(x_i-\\bar x)\\bigr]^2 = S_{yy} - 2w_1 S_{xy} + w_1^2 S_{xx}. \\] <p>Substitute \\(w_1=S_{xy}/S_{xx}\\) and simplify:</p> \\[ \\sum e_i^2 = S_{yy} - 2\\frac{S_{xy}^2}{S_{xx}} + \\frac{S_{xy}^2}{S_{xx}} = S_{yy} - \\frac{S_{xy}^2}{S_{xx}}. \\] <p>Divide by \\(n\\) to match the ``population'' convention used for \\(\\sigma_e^2\\):</p> \\[ \\sigma_e^2 = \\frac{1}{n}\\sum e_i^2 = \\frac{S_{yy}}{n} - \\frac{S_{xy}^2}{n\\,S_{xx}} = \\sigma_y^2 - \\frac{\\sigma_{xy}^2}{\\sigma_x^2}. \\] <p>Finally, define \\(R^2=1-\\dfrac{\\text{SSE}}{\\text{SST}}=1-\\dfrac{\\sum e_i^2}{\\sum (y_i-\\bar y)^2}\\) to get</p> \\[ R^2 = 1-\\frac{S_{yy}-\\frac{S_{xy}^2}{S_{xx}}}{S_{yy}} = \\frac{S_{xy}^2}{S_{xx}\\,S_{yy}} = \\frac{\\sigma_{xy}^2}{\\sigma_x^2\\,\\sigma_y^2} = r_{xy}^2. \\]"},{"location":"linearRegression/#how-do-we-know-if-the-fit-is-good","title":"How do we know if the fit is good?","text":"<ul> <li>Residual variance \\(\\boldsymbol{\\sigma_e^2}\\) (or RMSE \\(=\\sqrt{\\sigma_e^2}\\)) should be small relative to the scale of \\(y\\).   Rule of thumb: \\(\\mathrm{RMSE} \\ll \\text{SD}(y)=\\sigma_y\\) is good.</li> <li>\\(\\boldsymbol{R^2}\\) close to \\(1\\) means the model explains most variance.</li> <li>Beating the baseline: ensure \\(\\sigma_e^2 &lt; \\sigma_y^2\\) (equivalently \\(R^2&gt;0\\)). If not, the mean-only model is better.</li> <li>Out-of-sample: check test/validation \\(R^2\\) or RMSE. A good fit generalizes (train and test metrics are similar).</li> <li>Residual diagnostics: residuals should look like noise (no trend vs.\\ \\(\\hat y\\) or \\(x\\), roughly constant spread, few large outliers).</li> <li>Adjusted \\(R^2\\) (for multiple \\(x\\)): prefer higher adjusted \\(R^2\\); it penalizes unnecessary features.</li> </ul>"},{"location":"linearRegression/#multiple-linear-regression-many-features","title":"Multiple linear regression (many features)","text":"<p>When we have multiple input features \\((d)\\) per example, stack them in a matrix:</p> \\[ \\mathbf{X}\\in\\mathbb{R}^{n\\times d} \\quad \\mathbf{y}\\in\\mathbb{R}^{n},\\quad \\mathbf{A}=\\bigl[\\mathbf{1}\\ \\ \\mathbf{X}\\bigr]\\in\\mathbb{R}^{n\\times(d+1)}. \\] <p>For each sample we have:</p> \\[ \\hat{y_i} = w_0 + w_1x_{i,1} + \\cdots + w_dx_{i,d} \\] <p>Then the model can be written as :</p> \\[ \\hat{\\mathbf{y}}=\\mathbf{A}\\mathbf{w}, \\] <p>We want \\( \\mathbf{w}^* \\) that minimizes squared error between \\(\\hat{\\mathbf{y}}=\\mathbf{A}\\mathbf{w}\\) and \\(\\mathbf{y}\\).</p> \\[ L(\\mathbf{w})  = \\|\\hat{\\mathbf{y}}-\\mathbf{y}\\|_2^2 = \\|\\mathbf{A}\\mathbf{w}-\\mathbf{y}\\|_2^2 = (\\mathbf{A}\\mathbf{w}-\\mathbf{y})^\\top(\\mathbf{A}\\mathbf{w}-\\mathbf{y}). \\] \\[ L(\\mathbf{w}) = \\mathbf{w}^\\top\\mathbf{A}^\\top\\mathbf{A}\\mathbf{w} - 2\\,\\mathbf{y}^\\top\\mathbf{A}\\mathbf{w} + \\mathbf{y}^\\top\\mathbf{y}. \\] <p>Using matrix calculus identities:</p> <ul> <li>\\(\\nabla_{\\mathbf{w}}\\,\\mathbf{w}^\\top\\mathbf{B}\\mathbf{w} = (\\mathbf{B}+\\mathbf{B}^\\top)\\mathbf{w}\\), so with \\(\\mathbf{B}=\\mathbf{A}^\\top\\mathbf{A}\\) (symmetric) we get \\(2\\mathbf{A}^\\top\\mathbf{A}\\mathbf{w}\\).</li> <li>\\(\\nabla_{\\mathbf{w}}\\,\\mathbf{c}^\\top\\mathbf{w} = \\mathbf{c}\\).</li> </ul> <p>We get:</p> \\[ \\nabla_{\\mathbf{w}} L(\\mathbf{w}) = 2\\mathbf{A}^\\top\\mathbf{A}\\mathbf{w} - 2\\mathbf{A}^\\top\\mathbf{y}. \\] <p>Set the gradient to zero and solve for \\(\\mathbf{w}\\):</p> \\[ 2\\mathbf{A}^\\top\\mathbf{A}\\mathbf{w} - 2\\mathbf{A}^\\top\\mathbf{y} = \\mathbf{0} \\quad\\Longrightarrow\\quad \\mathbf{A}^\\top\\mathbf{A}\\mathbf{w} = \\mathbf{A}^\\top\\mathbf{y}. \\] <p>If \\(\\mathbf{A}^\\top\\mathbf{A}\\) is invertible:</p> \\[ \\mathbf{w}^* = (\\mathbf{A}^\\top\\mathbf{A})^{-1}\\mathbf{A}^\\top\\mathbf{y}. \\]"},{"location":"linearRegression/#linear-basis-function-regression-same-idea-with-richer-inputs","title":"Linear basis function regression (same idea with richer inputs)","text":"<p>The assumption that the output is a linear function of the input features is very restrictive. Instead, we can consider them to be linear combinations of fixed non-linear functions. Therefore, instead of feeding raw features, we can feed any fixed transformations: polynomials, splines, one-hot bins, etc.</p> \\[ \\hat y_i=\\sum_{j=0}^{p} w_j\\,\\phi_j(\\mathbf{x}_i) = \\mathbf{w}^\\top\\boldsymbol{\\phi}(\\mathbf{x}_i),\\qquad \\phi_0(\\cdot)=1. \\] <p>In matrix form,</p> \\[ \\boldsymbol{\\Phi}= \\begin{bmatrix} \\phi_0(\\mathbf{x}_1)&amp;\\cdots&amp;\\phi_p(\\mathbf{x}_1)\\\\ \\vdots&amp;\\ddots&amp;\\vdots\\\\ \\phi_0(\\mathbf{x}_n)&amp;\\cdots&amp;\\phi_p(\\mathbf{x}_n) \\end{bmatrix}, \\] <p>Again, minimizing gives:</p> \\[ \\mathbf{w}^*=(\\boldsymbol{\\Phi}^\\top\\boldsymbol{\\Phi})^{-1}\\boldsymbol{\\Phi}^\\top\\mathbf{y} \\] <p>Same solver, just a different design matrix.</p>"},{"location":"linearRegression/#why-the-normal-equations-appear-gradient-view","title":"Why the normal equations appear (gradient view)","text":"<p>Write the loss as a clean quadratic:</p> \\[ L(\\mathbf{w})=\\tfrac12\\|\\mathbf{y}-\\mathbf{B}\\mathbf{w}\\|_2^2,\\quad \\mathbf{B}\\in\\{\\mathbf{A},\\boldsymbol{\\Phi}\\}. \\] <p>Setting the gradient to zero:</p> \\[ \\nabla L(\\mathbf{w})=-\\mathbf{B}^\\top(\\mathbf{y}-\\mathbf{B}\\mathbf{w})=0 \\ \\Rightarrow\\ \\mathbf{B}^\\top\\mathbf{B}\\,\\mathbf{w}=\\mathbf{B}^\\top\\mathbf{y}. \\] <p>That linear system is the normal equations above.</p>"},{"location":"linearRegression/#computing-ols-solution","title":"Computing OLS solution","text":"<p>How long does it take to compute</p> \\[ w^* = (\\Phi^T \\Phi)^{-1} \\Phi^T y \\] <p>where \\(\\Phi\\) is an \\(n \\times d\\) matrix? Runtime of a \u201cnaive\u201d solution using \u201cstandard\u201d matrix multiplication is given by:</p> \\[ \\begin{aligned} &amp; O(d^2 n) \\text{ to multiply } \\Phi^T \\Phi \\\\ &amp; O(d n) \\text{ to multiply } \\Phi^T y \\\\ &amp; O(d^3) \\text{ to compute the inverse of } \\Phi^T \\Phi     \\text{ (Note: in practice, we can do it a bit faster.)} \\end{aligned} \\] <p>Since \\(n\\) is generally much larger than \\(d\\), the first term dominates and the runtime is \\(O(d^2 n)\\).</p>"},{"location":"linearRegression/#quick-ols-recipe-practical-path","title":"Quick OLS recipe (practical path)","text":"<ol> <li>Gather data \\((\\mathbf{x}_i,y_i)_{i=1}^n\\) including an intercept column of ones.</li> <li>Choose your features/bases \\(\\boldsymbol{\\phi}(\\cdot)\\) (raw, polynomial, binned, etc.).</li> <li>Fit by OLS: \\(\\mathbf{w}^*=(\\mathbf{B}^\\top\\mathbf{B})^{-1}\\mathbf{B}^\\top\\mathbf{y}\\) or a numeric solver (QR/SVD are more stable).</li> <li>Predict: \\(\\hat y=\\langle\\boldsymbol{\\phi}(\\mathbf{x}),\\mathbf{w}^*\\rangle\\).</li> <li>Evaluate on held-out data (MSE, \\(R^2\\)).</li> </ol>"}]}