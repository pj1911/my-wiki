## Introduction to Model-Free Control

Model-free control means learning to maximise the value function of an unknown MDP. The main idea is simple: we do not build or use an explicit transition/reward model. Instead, we learn directly from experience samples, i.e. observed tuples \((S_t, A_t, R_{t+1}, S_{t+1})\).

In practice, model-free methods show up in two common regimes. First, the MDP model is unknown but we can sample experience by interacting with the environment, so learning is naturally driven by collected trajectories. Second, the MDP model is known in principle (we could write down \(P(s'|s,a)\) and \(R(s,a)\)), but the state-action space is so large that exact dynamic programming updates are not feasible. In that case, we still fall back to sampled experiences.

**On-Policy vs Off-Policy Learning.**

To talk about how data is collected, it helps to distinguish two policies. The target policy \(\pi\) is the policy we want to evaluate or improve, while the behaviour policy \(\mu\) is the policy that actually generates the data.

- In on-policy learning, we "learn on the job": we learn about \(\pi\) using experience produced by \(\pi\) itself. Equivalently, the data distribution matches the policy being learned:

$$
  (S_t, A_t) \sim \pi(\cdot \mid S_t).
$$

- In off-policy learning, we "look over someone's shoulder": we learn about \(\pi\) using experience generated by a different policy \(\mu\). Here the trajectories come from \(\mu\), but the objective is still to optimise or evaluate \(\pi\):

$$
  (S_t, A_t) \sim \mu(\cdot \mid S_t), \quad \text{while optimising/evaluating } \pi.
$$

## From DP Policy Iteration to MC Control

In dynamic programming (DP), policy iteration alternates between two ideas: first, policy evaluation, where we compute \(v_\pi\) (for example via iterative policy evaluation), and then policy improvement, where we construct a better policy \(\pi' \ge \pi\) (often by making \(\pi'\) greedy with respect to the current value estimate). A natural model-free question is whether we can keep the same loop, but replace DP evaluation with Monte Carlo (MC) evaluation:

At a high level, the answer is yes: this fits the idea of Generalised Policy Iteration (GPI), where evaluation and improvement are interleaved even when evaluation is only approximate. With MC, we can estimate the value of \(\pi\) directly from sampled experience, using returns:

$$
v_\pi(s) = \mathbb{E}_\pi\!\left[G_t \mid S_t=s\right],
\qquad
G_t = \sum_{k=0}^{T-t-1}\gamma^k R_{t+k+1}.
$$

So the evaluation step can be done without knowing the model. The catch is the improvement step. In DP, "greedy improvement over \(V\)" is typically implemented by a one-step lookahead:

$$
\pi'(s) \in \arg\max_{a\in\mathcal{A}} \sum_{s'} P(s'\mid s,a)\Big(R(s,a,s') + \gamma V(s')\Big).
$$

Even if MC gives a good estimate \(V(s)\approx v_\pi(s)\), this greedy update still needs \(P(s'\mid s,a)\) (and the expected rewards). So MC evaluation + DP-style greedy improvement is not fully model-free: the evaluation is sample-based, but the improvement step still assumes access to the dynamics.

### Model-Free Fix: Improve Using the Action-Value Function \(Q(s,a)\)

A clean way to stay model-free is to improve using the action-value function rather than \(V(s)\). We estimate

$$
q_\pi(s,a) = \mathbb{E}_\pi\!\left[G_t \mid S_t=s, A_t=a\right],
$$

and then improve greedily without any one-step lookahead:

$$
\pi'(s) \in \arg\max_{a\in\mathcal{A}} q_\pi(s,a).
$$

This step is model-free because it only compares learned estimates of \(Q(s,a)\) across actions, instead of predicting next states via \(P(\cdot\mid s,a)\). This is the basic motivation for MC control via action-values: estimate \(Q\) from experience, and act (approximately) greedily with respect to \(Q\).

**Additional Practical Issues with MC in a Control Loop.**
Even if we ignore the fact that \(V\)-greedy improvement needs a model, MC still sufferes from other problems. Standard MC relies on complete returns, so it fits episodic tasks best, in continuing tasks we usually need truncation or a different setup. Returns \(G_t\) can also have high variance, especially when horizons are long, which means value estimates may require many episodes to stabilise. Control adds an exploration headache too: if we become fully greedy too early, we may stop trying other actions and never learn whether they are better. Finally, the process is non-stationary: as the policy changes, the data distribution changes with it, so evaluation is always approximate and intertwined with improvement.

## Exploration vs Greedy Improvement: \(\varepsilon\)-Greedy MC Control

If we always act greedily with respect to the current estimate \(Q(s,a)\) (or \(V(s)\)), we risk stopping exploration. Early estimates are noisy, so the action that looks best after a few trials may not actually be best. Once a greedy policy commits, rarely chosen actions stay poorly estimated, and learning can get "stuck" in a suboptimal policy. A tiny example makes this concrete. Imagine a single-state choice with two actions (two doors): left and right. We try left once and see reward \(0\), then try right once and see reward \(+1\). At this moment,

$$
\hat V(\texttt{left}) = 0,\qquad \hat V(\texttt{right}) = 1,
$$

so a purely greedy agent will keep choosing right and keep updating its estimate from more samples, e.g.

$$
\hat V(\texttt{right}) \leftarrow 2,\ \ldots
$$

But the important question is: are we sure we picked the best door? If left has uncertainty or rare high payoffs, never trying it again means we can never correct a wrong early guess.

A simple fix for this is doing a \(\varepsilon\)-greedy exploration. It keeps the greedy preference, but forces continued exploration by giving every action non-zero probability. To be more concrete, let \(m = |\mathcal{A}|\) be the number of actions, then we can define a greedy action as

$$
a^*(s) \in \arg\max_{a\in\mathcal{A}} Q(s,a).
$$

The \(\varepsilon\)-greedy rule is: choose \(a^*(s)\) with probability \(1-\varepsilon\), and with probability \(\varepsilon\) choose a random action uniformly from \(\mathcal{A}\). Equivalently for each action,

$$
\pi(a\mid s) =
\begin{cases}
1-\varepsilon + \dfrac{\varepsilon}{m}, & \text{if } a=a^*(s),\\
\dfrac{\varepsilon}{m}, & \text{otherwise.}
\end{cases}
$$

We add the \(\varepsilon/m\) term in the first case because even in the exploration step (which occurs with probability \(\varepsilon\)), the policy chooses uniformly from all \(m\) actions and can still select the greedy action with probability \(1/m\), therefore

$$
\pi(a^*(s)\mid s)=(1-\varepsilon)+\varepsilon\cdot\frac{1}{m}=1-\varepsilon+\frac{\varepsilon}{m}.
$$

This helps in three straightforward ways: it prevents premature convergence by continuing to sample non-greedy actions, it improves the accuracy of \(Q(s,a)\) by collecting more balanced data, and it guarantees non-zero exploration. Informally, if \(\varepsilon\) does not decay too fast, every action keeps getting tried often enough for learning to correct itself (formal convergence needs extra conditions).

With this in mind, MC control with action-values becomes a practical on-policy loop: we estimate \(Q \approx q_\pi\) from sampled returns, then we improve the policy toward greedy with respect to \(Q\), while behaving according to an \(\varepsilon\)-greedy version to maintain exploration. This is the standard recipe for \(\varepsilon\)-greedy MC control.

### \(\varepsilon\)-Greedy Policy Improvement (Proof)

Fix an \(\varepsilon\)-greedy policy \(\pi\) and suppose we can evaluate its action-values \(q_\pi(s,a)\). 
We construct an improved policy \(\pi'\) by acting \(\varepsilon\)-greedily with respect to \(q_\pi\): in each state \(s\) we prefer actions with maximal \(q_\pi(s,a)\), but keep uniform exploration over all actions.

Let

$$
a^*(s)\in \arg\max_{a\in\mathcal{A}} q_\pi(s,a),\qquad m = |\mathcal{A}|,
$$

and define

$$
\pi'(a\mid s)=
\begin{cases}
1-\varepsilon + \dfrac{\varepsilon}{m}, & a=a^*(s),\\
\dfrac{\varepsilon}{m}, & \text{otherwise}.
\end{cases}
$$

Thus \(\pi'\) concentrates probability on greedy actions while assigning positive probability to every action. We now show that this improvement step cannot decrease performance.

**[Theorem] \(\varepsilon\)-greedy policy improvement.** For any \(\varepsilon\)-greedy policy \(\pi\), the \(\varepsilon\)-greedy policy \(\pi'\) with respect to \(q_\pi\)
is an improvement, i.e.

$$
v_{\pi'}(s)\ge v_\pi(s)\quad \text{for all } s\in\mathcal{S}.
$$

**Proof.**
Fix a state \(s\). For any policy \(\pi'\) we can write:

$$
q_\pi(s,\pi'(s))=\sum_{a\in\mathcal{A}}\pi'(a\mid s)\,q_\pi(s,a).
$$

**Step 1: Expand \(q_\pi(s,\pi'(s))\) using the definition of \(\pi'\).**
Let \(a^*(s)\in\arg\max_{a\in\mathcal{A}}q_\pi(s,a)\) and \(m=|\mathcal{A}|\). Then

$$
\begin{aligned}
q_\pi(s,\pi'(s))
&=\Bigl(1-\varepsilon+\frac{\varepsilon}{m}\Bigr)q_\pi\bigl(s,a^*(s)\bigr)
  \;+\;\sum_{a\neq a^*(s)}\frac{\varepsilon}{m}\,q_\pi(s,a)\\
&=(1-\varepsilon)\,q_\pi\bigl(s,a^*(s)\bigr)
  \;+\;\frac{\varepsilon}{m}\sum_{a\in\mathcal{A}}q_\pi(s,a)\\
&=(1-\varepsilon)\max_{a\in\mathcal{A}}q_\pi(s,a)\;+\;\frac{\varepsilon}{m}\sum_{a\in\mathcal{A}}q_\pi(s,a).
\end{aligned}
$$

This is exactly "mostly greedy" plus a uniform exploration average.

**Step 2: Compare the greedy maximum to the \(q_\pi\)-expectation under \(\pi\).**
A maximum dominates every convex combination: for any distribution \(w(\cdot)\) on \(\mathcal{A}\),

$$
\max_{a\in\mathcal{A}}q_\pi(s,a)\;\ge\;\sum_{a\in\mathcal{A}}w(a)\,q_\pi(s,a).
$$

Define

$$
w(a)\;=\frac{\pi(a\mid s)-\varepsilon/m}{1-\varepsilon}.
$$

This is a valid distribution:

- Nonnegativity: since \(\pi\) is \(\varepsilon\)-greedy, \(\pi(a\mid s)\ge \varepsilon/m\) for all \(a\), hence \(w(a)\ge 0\).
- Normalization:

$$
\sum_{a\in\mathcal{A}}w(a)
=\frac{\sum_a\pi(a\mid s)-\sum_a\varepsilon/m}{1-\varepsilon}
=\frac{1-\varepsilon}{1-\varepsilon}=1.
$$

Plugging this \(w\) into the inequality and multiplying by \((1-\varepsilon)\) gives

$$
(1-\varepsilon)\max_{a\in\mathcal{A}}q_\pi(s,a)
\;\ge\;\sum_{a\in\mathcal{A}}\bigl(\pi(a\mid s)-\varepsilon/m\bigr)\,q_\pi(s,a).
$$

**Step 3: Combine with Step 1 to obtain the key statewise inequality.**
Using the expansion from Step 1,

$$
\begin{aligned}
q_\pi(s,\pi'(s))
&=\frac{\varepsilon}{m}\sum_{a\in\mathcal{A}}q_\pi(s,a)\;+\;(1-\varepsilon)\max_{a\in\mathcal{A}}q_\pi(s,a)\\
&\ge \frac{\varepsilon}{m}\sum_{a\in\mathcal{A}}q_\pi(s,a)
 + \sum_{a\in\mathcal{A}}\bigl(\pi(a\mid s)-\varepsilon/m\bigr)\,q_\pi(s,a)\\
&=\sum_{a\in\mathcal{A}}\pi(a\mid s)\,q_\pi(s,a)\\
&=q_\pi(s,\pi(s))\\
&=v_\pi(s),
\end{aligned}
$$

where the last equality is the standard identity \(v_\pi(s)=\sum_a \pi(a\mid s)\,q_\pi(s,a)\).

Thus, for every state \(s\),

$$
q_\pi\bigl(s,\pi'(s)\bigr)\;\ge\;v_\pi(s).
$$

**Step 4: Conclude \(v_{\pi'}\ge v_\pi\) (policy-improvement step).**
Recall that for any \(s\),

$$
(T_{\pi'}v_\pi)(s)\;=\;\sum_{a\in\mathcal{A}}\pi'(a\mid s)\Bigl[r(s,a)+\gamma\sum_{s'}P(s'\mid s,a)\,v_\pi(s')\Bigr]
=\sum_{a\in\mathcal{A}}\pi'(a\mid s)\,q_\pi(s,a)
=q_\pi(s,\pi'(s)).
$$

So the inequality above is exactly

$$
(T_{\pi'}v_\pi)(s)\;\ge\;v_\pi(s)\qquad \forall s.
$$

Applying \(T_{\pi'}\) repeatedly and using monotonicity yields

$$
v_\pi \;\le\; T_{\pi'}v_\pi \;\le\; T_{\pi'}^2v_\pi \;\le\;\cdots
$$

and since \(T_{\pi'}\) is a \(\gamma\)-contraction, \(T_{\pi'}^k v_\pi \to v_{\pi'}\) as \(k\to\infty\). Therefore,

$$
v_{\pi'}(s)\;\ge\;v_\pi(s)\qquad \forall s.
$$

**Practical Note.** The policy-improvement result above assumes access to the true action-values \(q_\pi\). In MC control we only have an estimate \(Q\approx q_\pi\), so the improvement step is approximate and performance need not increase monotonically after each episode. Moreover, while \(\varepsilon\)-greedy ensures continued exploration in any visited state,

$$
\pi(a\mid s)\ge \varepsilon/m,
$$

it does not provide a useful bound on how quickly the process visits all states or if it tries each action often enough to accurately estimate \(q_\pi\). In large or poorly-connected MDPs, some regions may be reached very rarely, making naive exploration slow and inefficient. In practice, exploration is often made more directed (e.g. optimistic initialisation, UCB-style bonuses, Boltzmann/softmax exploration), and many algorithms switch to TD methods (e.g. SARSA, Q-learning) that learn online without waiting for episode termination.

## Greedy in the Limit with Infinite Exploration (GLIE)

A fixed \(\varepsilon>0\) in \(\varepsilon\)-greedy control is great for exploration, but it has an obvious downside: the policy stays random forever, so it never becomes fully greedy. What we usually want is a schedule that explores a lot early (when \(Q\) is unreliable) and then gradually exploits more as the estimates improve. GLIE is a standard way to formalise this idea.

**Definition.** A sequence of policies \(\{\pi_k\}_{k\ge 1}\) is Greedy in the Limit with Infinite Exploration (GLIE) if it satisfies two properties. First, it performs infinite exploration: every state-action pair is visited infinitely often,

$$
\lim_{k\to\infty} N_k(s,a) = \infty
\quad \text{for all } (s,a)\in \mathcal{S}\times\mathcal{A},
$$

where \(N_k(s,a)\) counts how many times \((s,a)\) has appeared up to episode \(k\). Second, it becomes greedy in the limit: as \(k\) grows, the policy converges to choosing greedy actions with respect to the learned action-values,

$$
\lim_{k\to\infty}\pi_k(a\mid s) = \mathbf{1}\!\left(a \in \arg\max_{a'\in\mathcal{A}} Q_k(s,a')\right).
$$

A typical example is decaying \(\varepsilon\)-greedy, where \(\varepsilon_k\to 0\) (for instance \(\varepsilon_k=1/k\)), provided the induced behaviour still visits states often enough. This directly addresses the issue that a fixed \(\varepsilon\) can be too random forever.

### GLIE Monte-Carlo Control (episode-by-episode)

Control is not an i.i.d. estimation problem: the data are generated by the current policy, and as the policy changes, the state-action distribution of the samples changes as well. GLIE MC control therefore implements generalised policy iteration: it interleaves (approximate) policy evaluation with policy improvement, updating both from the same evolving stream of experience. Concretely, at episode \(k\) we roll out a trajectory under the current policy \(\pi_k\),

$$
\{S_1, A_1, R_2, \ldots, S_T\} \sim \pi_k,
$$

use the resulting returns to update \(Q\) by incremental Monte-Carlo averaging for each visited pair \((S_t,A_t)\),

$$
N(S_t,A_t) \leftarrow N(S_t,A_t) + 1,\qquad
Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \frac{1}{N(S_t,A_t)}\Bigl(G_t - Q(S_t,A_t)\Bigr),
$$

and then improve the policy by making it \(\varepsilon_k\)-greedy with respect to the updated \(Q\) while simultaneously reducing exploration according to a GLIE schedule (e.g. \(\varepsilon_k=1/k\)):

$$
\varepsilon_k \leftarrow \frac{1}{k}, \qquad \pi_{k+1} \leftarrow \varepsilon_k\text{-greedy}(Q).
$$

The decay \(\varepsilon_k\to 0\) makes the policy greedy in the limit, while keeping \(\varepsilon_k>0\) for all finite \(k\) ensures continued exploration. Under standard assumptions for episodic finite MDPs and a GLIE policy sequence, tabular GLIE Monte-Carlo control converges:

$$
Q_k(s,a) \to q_*(s,a) \quad \text{as } k\to\infty.
$$

**Proof sketch.**
Fix a state-action pair \((s,a)\). Under a GLIE policy sequence, every pair is visited infinitely often while the policy becomes greedy in the limit: \(\varepsilon_k\to 0\) and \(N_k(s,a)\to\infty\). For first-visit (or every-visit) Monte-Carlo evaluation, the update

$$
Q_{n+1}(s,a) \;=\; Q_n(s,a) + \frac{1}{n}\bigl(G^{(n)}(s,a)-Q_n(s,a)\bigr)
$$

is the sample-average estimator of \(\mathbb{E}_\pi[G\mid S=s,A=a]=q_\pi(s,a)\), hence by the law of large numbers,

$$
Q_n(s,a)\to q_{\pi}(s,a)\qquad \text{for fixed }\pi.
$$

In control, \(\pi_k\) changes with \(k\), but GLIE interleaves these evaluation updates with \(\varepsilon_k\)-greedy improvement. The improvement step is greedy in the limit, so the sequence of policies approaches a greedy policy w.r.t. the limiting action-values. Combining (i) infinite exploration (so each \((s,a)\) is learned) with (ii) greedy-in-the-limit improvement (so suboptimal actions are eventually rejected) yields convergence of the action-value estimates to optimality:

$$
Q_k(s,a)\to q_*(s,a)\quad \text{as }k\to\infty.
$$

A full proof formalises this using stochastic approximation arguments and the GLIE conditions (see standard RL texts).

**Additional comments.** In tabular GLIE theory, if the GLIE conditions hold (infinite exploration and greedy in the limit), then the choice of initial values does not affect the limit: the estimates still converge to \(q_*\). What initialisation does affect is the transient behaviour, how quickly \(Q_k\) becomes accurate and how quickly the induced policy becomes good.

In practice, initialisation can noticeably change learning speed. With optimistic initialisation (starting with large \(Q\) values), actions look promising until they are tried, which can encourage exploration and sometimes allows a smaller \(\varepsilon\) in simple tasks. With pessimistic (or zero) initialisation, many actions may look similarly unappealing, and if exploration is weak the agent may fail to collect enough diverse experience early on. In larger problems, and especially with function approximation, this can interact with limited state-action coverage and further slow learning. Finally, at optimality the limiting action-value function satisfies the Bellman optimality equation:

$$
q_*(s,a) = \mathbb{E}\!\left[ R_{t+1} + \gamma \max_{a'\in\mathcal{A}} q_*(S_{t+1}, a') \mid S_t=s, A_t=a \right].
$$

GLIE MC control targets \(q_*\) via sampled returns and \(\varepsilon_k\)-greedy improvement; the GLIE conditions are what ensure we keep visiting enough \((s,a)\) pairs to reliably identify the maximising actions.

## From MC Control to TD Control: SARSA

Monte-Carlo (MC) control updates \(Q\) using full returns, so it must wait until an episode ends and its targets can be noisy for long horizons. Temporal-Difference (TD) control changes the target: it bootstraps from the current value estimate instead of using the complete return. This has a few practical benefits. First, TD targets typically have lower variance than \(G_t\) because they depend on one reward plus an estimate, not an entire future sum. Second, TD can update online, step-by-step inside the episode. Third, TD can learn from truncated sequences and continuing tasks since it does not require a terminal state to define an update. The natural control recipe is the same as before: keep \(\varepsilon\)-greedy improvement, but replace MC evaluation with TD updates of \(Q(S,A)\) at every time-step.

### TD evaluation on action-values: the SARSA update

SARSA is an on-policy TD control method. Its name comes from the quintuple

$$
(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}).
$$

After taking \(A_t\) in \(S_t\) and observing \(R_{t+1}\) and \(S_{t+1}\), we also sample the next action \(A_{t+1}\) from the current behaviour policy (typically \(\varepsilon\)-greedy). With \(S_t=s\), \(A_t=a\), \(R_{t+1}=r\), \(S_{t+1}=s'\), and \(A_{t+1}=a'\), the SARSA update is

$$
Q(s,a)\;\leftarrow\; Q(s,a) + \alpha\Bigl(\underbrace{r + \gamma Q(s',a')}_{\text{TD target}} - Q(s,a)\Bigr).
$$

Equivalently, define the TD-error

$$
\delta_t \;=\; R_{t+1} + \gamma Q(S_{t+1},A_{t+1}) - Q(S_t,A_t),
$$

and update

$$
Q(S_t,A_t)\leftarrow Q(S_t,A_t) + \alpha\,\delta_t.
$$

### Why this TD target makes sense

Fix a policy \(\pi\). Its action-value function \(q_\pi\) satisfies the Bellman expectation equation

$$
q_\pi(s,a)
= \mathbb{E}_\pi\!\left[\,R_{t+1} + \gamma q_\pi(S_{t+1},A_{t+1}) \;\middle|\; S_t=s,\;A_t=a\right],
$$

where \(A_{t+1}\sim \pi(\cdot\mid S_{t+1})\). If \(q_\pi\) were known, then along a trajectory generated by \(\pi\) the random variable

$$
Y_{t} \;=\; R_{t+1} + \gamma q_\pi(S_{t+1},A_{t+1})
$$

has conditional expectation

$$
\mathbb{E}_\pi\!\left[\,Y_t \mid S_t=s,\;A_t=a\right] \;=\; q_\pi(s,a),
$$

so \(Y_t\) is a one-step, unbiased sample target for \(q_\pi(s,a)\). A natural way to estimate \(q_\pi\) is therefore to repeatedly move our estimate toward this target using stochastic approximation. SARSA follows exactly this logic, but replaces the unknown \(q_\pi\) with the current estimate \(Q\):

$$
\widehat{Y}_t \;=\; R_{t+1} + \gamma Q(S_{t+1},A_{t+1}).
$$

This is the TD target. With stepsize \(\alpha\), the SARSA update becomes

$$
Q(S_t,A_t)\leftarrow Q(S_t,A_t) + \alpha\bigl(\widehat{Y}_t - Q(S_t,A_t)\bigr),
$$

where the TD error

$$
\delta_t \;=\; R_{t+1} + \gamma Q(S_{t+1},A_{t+1}) - Q(S_t,A_t)
$$

measures the current violation of Bellman consistency. When \(Q\) matches \(q_\pi\), the expected TD error is zero, so on average the update stops pushing the estimate. In this sense, SARSA updates \(Q\) to make it increasingly consistent with the Bellman expectation equation for the policy being followed.

**Practical Notes.** Compared to MC, the TD target typically has lower variance, but it introduces bias because it bootstraps (it uses \(Q\) inside the target), in practice this tradeoff is often favourable, especially for long episodes or when we want to learn online. In SARSA control, we continually derive behaviour from the current action-values, most often by acting \(\varepsilon\)-greedily with respect to \(Q\),

$$
\pi(\cdot\mid s) \leftarrow \varepsilon\text{-greedy}(Q(\,s,\cdot\,)),
$$

because \(Q\) is updated at every time-step, the implied policy effectively changes at every time-step as well. This can be understood either through a policy iteration / GPI lens as TD updates act as approximate evaluation toward \(q_\pi\) while \(\varepsilon\)-greedy action selection supplies the improvement step, with the two interleaved continuously, or through an online control lens where behaviour is always generated by the current \(\varepsilon\)-greedy policy implied by \(Q\), so revisiting a state can lead to different actions simply because \(Q\) has changed since last time. The fact that behaviour changes while learning does matter: the data distribution is non-stationary because the policy evolves, however, SARSA is on-policy and therefore updates \(Q\) toward the value of the same policy that generates the data,

$$
\text{data from } \pi \quad \Longrightarrow \quad \text{update toward } q_\pi,
$$

so as \(\pi\) improves and the target \(q_\pi\) moves, SARSA keeps tracking it via continual small updates. Moreover, because TD methods bootstrap, they can immediately use the freshest estimates inside the target, which often speeds up learning relative to MC (which must wait for episode termination). In long-horizon tasks, repeated bootstrapped updates propagate information backward through the value function sooner. Finally, TD ideas extend naturally to off-policy control (e.g. Q-learning), where the target can use a greedy action even while behaviour remains exploratory, which is a major reason TD control sits at the core of many practical RL algorithms.

### SARSA algorithm

> Initialize \(Q(s,a)\) arbitrarily for all \(s\in\mathcal{S}, a\in\mathcal{A}(s)\) and set \(Q(\text{terminal},\cdot)=0\).
>
> For each episode:
>
>    1. Initialize \(S\).
>    2. Choose \(A\) from \(S\) using a policy derived from \(Q\) (e.g. \(\varepsilon\)-greedy).
>    3. Repeat (for each step of the episode):
>       1. Take action \(A\), observe \(R\) and next state \(S'\).
>       2. Choose \(A'\) from \(S'\) using a policy derived from \(Q\) (e.g. \(\varepsilon\)-greedy).
>       3. Update: \(Q(S,A) \leftarrow Q(S,A) + \alpha\bigl[R + \gamma Q(S',A') - Q(S,A)\bigr]\).
>       4. \(S \leftarrow S'\), \(A \leftarrow A'\) until \(S\) is terminal.



### Convergence of SARSA (tabular setting)

In the idealised tabular case (finite state-action spaces), SARSA is guaranteed to converge to the optimal action-value function provided (i) we explore forever but become greedy in the limit, and (ii) the step-sizes shrink in a controlled way. Concretely, SARSA converges in the sense that

$$
Q(s,a)\to q_*(s,a),
$$

under the following conditions:

- a GLIE sequence of behaviour policies \(\{\pi_t(\cdot\mid s)\}\) (infinite exploration, greedy in the limit),
- a Robbins-Monro step-size sequence \(\{\alpha_t\}\) satisfying

$$
\sum_{t=1}^{\infty}\alpha_t = \infty,
\qquad
\sum_{t=1}^{\infty}\alpha_t^2 < \infty.
$$

The Robbins-Monro conditions encode a practical balance between not giving up too early and eventual stability. The requirement \(\sum_t \alpha_t=\infty\) means there is enough total "learning rate" to keep making progress (the algorithm does not effectively freeze), while \(\sum_t \alpha_t^2<\infty\) ensures the steps become small enough that sampling noise averages out and the iterates settle. A standard schedule is \(\alpha_t=1/t\) (more generally \(\alpha_t=1/t^\beta\) with \(\tfrac12<\beta\le 1\)).

In real implementations it is common to use a small constant step-size (or a piecewise schedule) to learn faster and to track non-stationarity. This can violate Robbins-Monro, so the strict convergence guarantee no longer applies, but empirically it often works well, the theorem is mainly a clean guarantee for the stationary, tabular regime. These same ingredients also explain a common learning curve pattern in episodic tasks: learning can look extremely slow at first, then suddenly speed up. Early on, \(Q\) is uninformative, so an \(\varepsilon\)-greedy policy behaves close to random and may wander for a long time before reaching termination. As SARSA updates \(Q\) online, useful actions start receiving higher values, so the same \(\varepsilon\)-greedy rule increasingly prefers them and trajectories shorten dramatically. For example, in a sparse-reward navigation task where reward appears only at a goal state, the first episode might take a lot of steps to stumble into the goal but afterwards, TD bootstrapping propagates positive value back through earlier state-action choices, and later episodes can drop to a significantly lower number of steps even though exploration still occurs with probability \(\varepsilon\).

Overall GLIE ensures every \((s,a)\) keeps being tried while behaviour becomes greedy in the limit, and Robbins-Monro step-sizes ensure updates keep moving but eventually stabilise, together they yield the tabular convergence of SARSA.

## \(n\)-Step SARSA and SARSA(\(\lambda\)): Forward View vs Backward View

A useful way to understand the MC-TD connection is to compare how quickly we bootstrap. Plain SARSA bootstraps after one step: it replaces the unknown future return by the current estimate \(Q\) immediately, which typically reduces variance but introduces bias. Monte-Carlo (MC) on the other hand, waits until the end of the episode to form the full return, which is unbiased for \(q_\pi\) (given a fixed policy) but can have high variance and delayed learning. The natural middle ground is to bootstrap after \(n\) steps. This yields a family of methods indexed by an integer \(n\in\{1,2,\dots\}\), with \(n=1\) recovering SARSA and \(n=\infty\) corresponding to MC. Ideally, however, we would like a mechanism that blends these horizons rather than forcing us to pick a single "correct" \(n\).

### \(n\)-step returns for action-values

Consider a trajectory (episodic case) and fix a time index \(t\). The core object is a target for \(Q(S_t,A_t)\) that uses the next \(n\) rewards explicitly and then bootstraps from \(Q\) at time \(t+n\). Concretely, the first few \(n\)-step action-value targets are

$$
\begin{aligned}
n=1 \;(\text{SARSA}):\quad & q_t^{(1)} = R_{t+1} + \gamma Q(S_{t+1},A_{t+1}),\\
n=2:\quad & q_t^{(2)} = R_{t+1} + \gamma R_{t+2} + \gamma^2 Q(S_{t+2},A_{t+2}),\\
n=3:\quad & q_t^{(3)} = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 Q(S_{t+3},A_{t+3}),\\
&\vdots\\
n=\infty \;(\text{MC}):\quad & q_t^{(\infty)} = R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{T-t-1}R_T.
\end{aligned}
$$

More compactly, for a finite \(n\) we define the \(n\)-step Q-return

$$
q_t^{(n)}
\;=\;
\sum_{i=1}^{n}\gamma^{i-1}R_{t+i}
\;+\;
\gamma^{n}Q(S_{t+n},A_{t+n}).
$$

When the episode terminates before \(t+n\), we simply stop at \(T\) and there is no bootstrap term; equivalently, the bootstrap term is \(0\) after termination. Given this target, \(n\)-step SARSA applies the usual incremental update:

$$
Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\alpha\Bigl(q_t^{(n)}-Q(S_t,A_t)\Bigr).
$$

Intuitively, \(n\) controls the bias-variance tradeoff: increasing \(n\) replaces more of the bootstrapped tail by actual sampled rewards (less bias, more variance and delay), while decreasing \(n\) bootstraps sooner (more bias, typically lower variance and faster propagation).

### Forward-view SARSA(\(\lambda\)): averaging over all horizons

Picking \(n\) can be tricky: the best horizon depends on the problem, and performance can be sensitive to it. SARSA(\(\lambda\)) addresses this by forming a single target that is a weighted average of all \(n\)-step targets. We define the \(\lambda\)-return (forward view) by

$$
q_t^\lambda =
(1-\lambda)\sum_{n=1}^{\infty}\lambda^{n-1}\,q_t^{(n)},
\qquad \lambda\in[0,1].
$$

The coefficients \((1-\lambda)\lambda^{n-1}\) form a geometric distribution over \(n\) because

$$
\sum_{n=1}^{\infty}(1-\lambda)\lambda^{n-1}=1,
$$

so \(q_t^\lambda\) is literally a convex combination of the \(n\)-step returns. The parameter \(\lambda\) controls how much weight is placed on longer horizons:

- \(\lambda=0\) puts all weight on \(n=1\), so \(q_t^\lambda=q_t^{(1)}\) and we recover one-step SARSA.
- As \(\lambda\to 1\), the weights shift toward large \(n\), making the target increasingly MC-like.

Using \(q_t^\lambda\) as the target gives the forward-view update rule:

$$
Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\alpha\Bigl(q_t^\lambda-Q(S_t,A_t)\Bigr).
$$

This view is conceptually clean, it says "update toward a particular weighted return", but it is not the most computationally convenient online implementation because it require access to many future rewards to form the sum. In particular, To compute \(q_t^{(n)}\) we need information up to time \(t+n\), and to evaluate the infinite weighted sum we need the whole future trajectory (up to termination). The backward view (eligibility traces) provides an efficient online mechanism that is equivalent to this forward view under standard conditions, which we discuss next.

### Backward-view SARSA(\(\lambda\)): eligibility traces for online learning

The forward view defines an appealing multi-step target, but it is inconvenient online because it depends on future rewards. The backward view achieves the same effect incrementally by maintaining eligibility traces: a table \(E(s,a)\) that tracks how much "credit" each previously visited pair should receive from the current TD error. This has been discussed in great detail in the previous chapter on Model free prediction.

**Eligibility traces (accumulating).**
Initialize traces to zero:

$$
E_0(s,a)=0 \qquad \text{for all }(s,a).
$$

At each time-step \(t\), traces decay and the currently visited pair is reinforced:

$$
E_t(s,a)\;=\;\gamma\lambda\,E_{t-1}(s,a)\;+\;\mathbf{1}(S_t=s,\;A_t=a).
$$

Thus all past traces shrink by the factor \(\gamma\lambda\), while \((S_t,A_t)\) receives an additional \(+1\). Recent (and repeatedly visited) pairs therefore have larger traces than older pairs.

**TD error (same as one-step SARSA).**

$$
\delta_t \;=\; R_{t+1} + \gamma Q(S_{t+1},A_{t+1}) - Q(S_t,A_t).
$$

**Backward-view update (credit assignment).**
Instead of assigning \(\delta_t\) only to the current pair, SARSA(\(\lambda\)) distributes it across all pairs proportionally to their eligibility:

$$
Q(s,a)\;\leftarrow\; Q(s,a) + \alpha\,\delta_t\,E_t(s,a)
\qquad \text{for all }(s,a).
$$

Intuitively, the TD error computed now is pushed backward through the trace, so recently visited pairs are updated strongly, and older pairs receive little or no update as their traces decay.

**How traces remove the need to "look forward".**
The forward view updates \(Q(S_t,A_t)\) using a weighted average of future \(n\)-step returns. The backward view flips the computation: it stays fully online, forms a one-step TD error at time \(t\), and uses \(E_t\) to send that information backward to the relevant recent history. The parameter \(\lambda\) controls how far this backward credit assignment reaches: \(\lambda=0\) makes traces vanish immediately (recovering one-step SARSA), while larger \(\lambda\) keeps traces alive longer, producing more multi-step, MC-like behaviour.

**SARSA vs. SARSA(\(\lambda\)) in practice.**

SARSA propagates learning one step at a time, so delayed rewards can take many updates to affect earlier decisions. SARSA(\(\lambda\)) uses eligibility traces to push each TD error backward over multiple recent steps (with decaying strength), which often speeds up learning on long-horizon or delayed-reward tasks. SARSA(\(\lambda\)) tends to help most with delayed/sparse rewards and episodic tasks where whole action sequences should share credit, while plain SARSA is often preferable for maximum simplicity, very large spaces where traces are expensive, or very noisy environments where longer-horizon credit assignment inflates variance.

## Off-Policy Learning and Importance Sampling

### On-policy vs. off-policy: what changed, and why it matters

So far we have mostly worked for on-policy methods, where the same policy both generates the data and defines the quantity we evaluate or improve: if we follow \(\pi\), the trajectory distribution is induced by \(\pi\), and estimates like \(v_\pi\) or \(q_\pi\) are built from samples generated under that same \(\pi\). In off-policy learning these roles split: the behaviour policy \(\mu(a\mid s)\) is what we actually run to collect experience, while the target policy \(\pi(a\mid s)\) is what we want to evaluate (or improve toward). Concretely, we observe episodes under \(\mu\),

$$
(S_1,A_1,R_2,\ldots,S_T)\sim \mu,
$$

but our goal remains to estimate quantities for \(\pi\), such as \(v_\pi(s)\) or \(q_\pi(s,a)\). The motivation is largely data efficiency: interaction is expensive, so off-policy learning lets us learn from demonstrations or another agent (data from \(\mu\), questions about \(\pi\)), reuse trajectories collected under earlier policies instead of discarding them, behave exploratorily for coverage while learning a greedy (or near-greedy) target policy, and even evaluate multiple target policies from a single dataset. But there is a key technical issue with this approach called distribution mismatch discussed below.

**Distribution mismatch.** In off-policy learning we collect experience under a behaviour policy \(\mu\), but we want to evaluate a different target policy \(\pi\). The problem is that value functions are expectations under the target policy's trajectory distribution. For instance,

$$
v_\pi(s)=\mathbb{E}_\pi\!\left[G_t \mid S_t=s\right],
$$

and the subscript \(\pi\) matters: it means the entire future trajectory (actions, next states, rewards, and hence the return \(G_t\)) is generated by following \(\pi\). If instead we generate episodes under \(\mu\), then even from the same starting state \(s\) we generally obtain a different distribution over trajectories, so the distribution of returns changes as well. Therefore, if we simply average returns observed under \(\mu\) while pretending they came from \(\pi\), we converge to the wrong quantity:

$$
\mathbb{E}_\mu[G_t\mid S_t=s]\neq \mathbb{E}_\pi[G_t\mid S_t=s]\quad\text{in general.}
$$

Intuitively, trajectories that are frequent under \(\mu\) but rare under \(\pi\) will be over-represented in the data, and trajectories that \(\pi\) would often generate but \(\mu\) rarely produces will be under-represented. This is a bias issue, not merely a high-variance or small-sample issue: with infinitely many samples from \(\mu\) we would still estimate the expectation under \(\mu\), not under \(\pi\). A correction mechanism is needed to reweight samples so that, in aggregate, they reflect how likely each trajectory would be under \(\pi\) rather than under \(\mu\). This task is achieved by importance sampling.

### Importance sampling: correcting distribution mismatch

Off-policy evaluation fails if we treat data from \(\mu\) as if it came from \(\pi\), because \(\mu\) and \(\pi\) induce different distributions over outcomes. Importance sampling is the standard correction: it rewrites an expectation under one distribution using samples from another by reweighting each sample by a likelihood ratio. For a random variable \(X\) and function \(f\),

$$
\mathbb{E}_{X\sim P}[f(X)]
= \sum_x P(x)f(x)
= \sum_x Q(x)\frac{P(x)}{Q(x)}f(x)
= \mathbb{E}_{X\sim Q}\!\left[\frac{P(X)}{Q(X)}f(X)\right].
$$

In off-policy RL, \(P\) is the trajectory distribution induced by the target policy \(\pi\), and \(Q\) is the trajectory distribution induced by the behaviour policy \(\mu\): the ratio \(\frac{P(\tau)}{Q(\tau)}\) tells us how much a sampled trajectory \(\tau\) should count if our goal is to estimate an expectation under \(\pi\) while sampling under \(\mu\).

$$
\mathbb{E}_{\tau\sim \pi}\bigl[f(\tau)\bigr]
= \mathbb{E}_{\tau\sim \mu}\!\left[\frac{P_\pi(\tau)}{P_\mu(\tau)}\,f(\tau)\right],
$$

**Coverage (absolute continuity).**
This correction is only defined if \(\mu\) assigns positive probability wherever \(\pi\) might act. If for some \((s,a)\) we have

$$
\mu(a\mid s)=0 \ \text{and}\ \pi(a\mid s)>0,
$$

then \(\pi(a\mid s)/\mu(a\mid s)\) is undefined, and trajectories that \(\pi\) can generate are simply unobservable under \(\mu\). Informally: the behaviour policy must be willing to try any action that the target policy might take.

### Off-policy Monte-Carlo via importance sampling

In Monte-Carlo evaluation the return \(G_t\) depends on the entire future trajectory, so if data are generated by a behaviour policy \(\mu\) but we want values for a target policy \(\pi\), we must correct the distribution mismatch by reweighting the whole action sequence after time \(t\). For an episodic trajectory terminating at \(T\), define the (per-decision) importance ratio

$$
\rho_{t:T-1}
=
\prod_{k=t}^{T-1}\frac{\pi(A_k\mid S_k)}{\mu(A_k\mid S_k)}.
$$

This ratio is the likelihood ratio for producing the same sequence of actions under \(\pi\) instead of \(\mu\) (conditioned on the visited states), and it tells us how much the observed trajectory segment should "count" when our goal is an expectation under \(\pi\). The corresponding importance-sampled return is

$$
G_t^{\pi/\mu}=\rho_{t:T-1}\,G_t,
$$

so trajectories that are more likely under \(\pi\) than under \(\mu\) are upweighted and those that are less likely are downweighted, with this reweighting, the estimator has the correct expectation for the target policy. A standard incremental update then takes the usual MC form but uses the corrected return,

$$
V(S_t)\leftarrow V(S_t) + \alpha\Bigl(G_t^{\pi/\mu} - V(S_t)\Bigr).
$$

The main practical limitation with this approach is variance: \(\rho_{t:T-1}\) is a product of random ratios, so over long horizons even modest mismatch between \(\pi\) and \(\mu\) can cause the weights to explode (rare trajectories get enormous weight) or collapse toward zero (most trajectories contribute almost nothing), yielding high-variance estimates, this is why pure off-policy MC with importance sampling is often unstable in long tasks and motivates variance-reduction variants (e.g. weighted importance sampling) as well as off-policy TD methods.

### Off-policy TD: single-step correction

Off-policy learning is valuable because it lets us reuse data (old trajectories, demonstrations, exploratory behaviour) while estimating or improving a different target policy \(\pi\). The core difficulty is still distribution mismatch, so we still need importance sampling, but TD methods make this correction much lighter because their targets are local due to bootstrapping. In TD(0), the target for \(V\) uses only the immediate reward and the next state's value estimate,

$$
\text{TD target:}\quad R_{t+1}+\gamma V(S_{t+1}),
$$

so to correct for sampling under a behaviour policy \(\mu\) we only need to reweight the one action actually taken at time \(t\). Concretely, for off-policy TD(0) evaluation of \(\pi\) using transitions generated by \(\mu\), we apply a single-step importance ratio:

$$
V(S_t)\leftarrow V(S_t)+
\alpha\left(
\frac{\pi(A_t\mid S_t)}{\mu(A_t\mid S_t)}
\Bigl(R_{t+1}+\gamma V(S_{t+1})-V(S_t)\Bigr)
\right),
$$

i.e. we multiply the TD error by \(\pi(A_t\mid S_t)/\mu(A_t\mid S_t)\) to account for the fact that \(A_t\) was chosen according to \(\mu\) rather than \(\pi\). The practical advantage over off-policy Monte-Carlo is variance: MC must correct the probability of an entire future action sequence and therefore uses a product of many ratios, which can easily explode or collapse on long horizons, whereas off-policy TD uses only a single ratio per update. As a result, TD is typically far more stable and data-efficient off-policy, even when \(\pi\) and \(\mu\) are not extremely close. The remaining caveat is that very large per-step ratios \(\pi(A_t\mid S_t)/\mu(A_t\mid S_t)\) can still introduce instability, so in practice one often chooses \(\mu\) to provide good coverage and keeps \(\pi\) reasonably aligned with it.

## Q-Learning: Off-Policy Control with a Greedy Target
Unlike the off-policy methods above (which correct evaluation of a fixed target policy \(\pi\) using importance sampling), Q-learning builds the target directly from a greedy backup \(\max_{a'}Q(S_{t+1},a')\), so it can learn about an optimal greedy policy even while behaving according to a separate exploratory policy \(\mu\).

Q-learning is a canonical off-policy TD control algorithm. We interact with the environment using a behaviour policy \(\mu\) (typically exploratory, e.g. \(\varepsilon\)-greedy) in order to visit many state-action pairs, but we learn action-values for a different target policy: the greedy policy implied by the current action-value estimates. Equivalently, Q-learning aims directly at the optimal action-value function \(q_\star\) while allowing behaviour to remain exploratory.

### Greedy target and why the update does not use importance sampling

At time \(t\) we choose the executed action from the behaviour policy,

$$
A_t \sim \mu(\cdot\mid S_t),
$$

and we observe a transition \((S_t,A_t,R_{t+1},S_{t+1})\). The off-policy aspect is that the update does not try to evaluate the value of the next behaviour action \(A_{t+1}\). Instead, it asks a counterfactual question about the next state: if I were greedy at \(S_{t+1}\), what value would I obtain? This is implemented by taking a maximum over actions at the next state,

$$
\max_{a'} Q(S_{t+1},a')
\;=\;
Q\!\Bigl(S_{t+1}, \arg\max_{a'} Q(S_{t+1},a')\Bigr),
$$

which corresponds to the greedy target policy

$$
\pi(\cdot\mid s)\in \arg\max_{a} Q(s,a).
$$

Because the target is computed by an explicit \(\max\) rather than by sampling \(A_{t+1}\) from \(\pi\), Q-learning does not need to reweight trajectories to make them look like they were generated by \(\pi\). The update uses behaviour samples only to obtain a transition and reward, then performs a greedy (optimality-style) backup at the next state.

### Update rule (optimality-style TD target)

The Q-learning update is

$$
Q(S_t,A_t)\leftarrow Q(S_t,A_t)
+\alpha\Bigl(\underbrace{R_{t+1}+\gamma\max_{a'}Q(S_{t+1},a')}_{\text{TD target}}
- Q(S_t,A_t)\Bigr).
$$

The associated TD error is

$$
\delta_t^{\text{QL}} \;=; R_{t+1}+\gamma\max_{a'}Q(S_{t+1},a')-Q(S_t,A_t).
$$

So each observed transition nudges \(Q(S_t,A_t)\) toward "reward plus discounted value of the best next action." Previously, in off-policy TD(0) evaluation, we still bootstrapped but we were trying to match a fixed target policy \(\pi\), so we corrected the TD error using the one-step importance ratio \(\pi(A_t\mid S_t)/\mu(A_t\mid S_t)\). Q-learning instead targets optimality by replacing the next-step value under a policy with a greedy \(\max\) backup, and therefore does not use an explicit importance-sampling correction.

### Connection to the Bellman optimality equation

Q-learning is designed to solve the Bellman optimality equation. The optimal action-value function \(q_\star\) is characterised by

$$
q_\star(s,a) =
\mathbb{E}\!\left[ R_{t+1}+\gamma\max_{a'}q_\star(S_{t+1},a')
\;\middle|\; S_t=s,\;A_t=a \right].
$$

It is convenient to define the right-hand side as the optimal Bellman operator

$$
(T_\star Q)(s,a)=\mathbb{E}\!\left[ R_{t+1}+\gamma\max_{a'}Q(S_{t+1},a') \mid S_t=s,A_t=a\right].
$$

Then \(q_\star\) is exactly the fixed point of this operator:

$$
q_\star = T_\star q_\star.
$$

The Q-learning update

$$
Q(S_t,A_t)\leftarrow Q(S_t,A_t)
+\alpha\Bigl(R_{t+1}+\gamma\max_{a'}Q(S_{t+1},a')-Q(S_t,A_t)\Bigr)
$$

is an incremental (sample-based) attempt to enforce this fixed-point condition: it replaces the expectation in \(T_\star\) by the single observed transition and moves \(Q(S_t,A_t)\) toward the corresponding target. Under the usual tabular assumptions (every \((s,a)\) is updated infinitely often and \(\{\alpha_t\}\) satisfies Robbins-Monro), these stochastic updates converge to the unique fixed point, so

$$
Q_t(s,a)\to q_\star(s,a).
$$

As a result, acting greedily with respect to \(Q_t\) becomes optimal in the limit:

$$
\pi_t(s)\in\arg\max_a Q_t(s,a).
$$

### Algorithm sketch

1. Initialize \(Q(s,a)\) arbitrarily (often \(0\)) for all state-action pairs.
2. For each episode:
   1. Start in an initial state \(S\).
   2. Repeat until terminal:
      1. Choose \(A \sim \mu(\cdot\mid S)\) (e.g., \(\varepsilon\)-greedy w.r.t. \(Q\)).
      2. Execute \(A\), observe reward \(R\) and next state \(S'\).
      3. Update: \(Q(S,A) \leftarrow Q(S,A) + \alpha\bigl(R + \gamma \max_{a'} Q(S',a') - Q(S,A)\bigr)\).
      4. Set \(S \leftarrow S'\).


## Relationship Between DP and TD Learning

DP and TD are built on the same Bellman equations, what differs is how they implement the Bellman backup. DP performs a full backup: it explicitly takes an expectation over next states and rewards using the environment dynamics (so it needs a model). TD performs a sample backup: it replaces that expectation by a single observed transition \((s,a,r,s')\), which is why it is model-free. The correspondence is easiest to see by lining up the Bellman equation, the DP update, and the TD update.

### State-value prediction (\(v_\pi\))

The Bellman expectation equation for a fixed policy \(\pi\) is

$$
v_\pi(s)=\mathbb{E}\!\left[R_{t+1}+\gamma v_\pi(S_{t+1})\mid S_t=s\right].
$$

A DP-style evaluation step updates by taking the full conditional expectation:

$$
V(s)\leftarrow \mathbb{E}\!\left[R+\gamma V(S')\mid S=s\right].
$$

TD(0) keeps the same target structure but uses one sample transition \((S_t=s,R_{t+1}=R,S_{t+1}=s')\):

$$
V(s)\xleftarrow{\alpha} R+\gamma V(s'),
\qquad
\text{where }x\xleftarrow{\alpha}y \equiv x \leftarrow x+\alpha(y-x).
$$

### Action-value prediction / on-policy control (\(q_\pi\) and SARSA)

For action-values under a fixed policy,

$$
q_\pi(s,a)=\mathbb{E}\!\left[R_{t+1}+\gamma q_\pi(S_{t+1},A_{t+1})\mid S_t=s,A_t=a\right],
\qquad A_{t+1}\sim \pi(\cdot\mid S_{t+1}).
$$

DP would again take a full backup (typically inside policy iteration: evaluate \(q_\pi\), then improve \(\pi\)). The TD analogue replaces the expectation with a single sample next state and next action \((s',a')\), yielding the SARSA update:

$$
Q(s,a)\xleftarrow{\alpha} R+\gamma Q(s',a').
$$

This is on-policy in the sense that the same policy that generates behaviour also determines the next action \(a'\) appearing inside the target.

### Optimal control (\(q_\star\) and Q-learning)

For optimal control, the relevant Bellman equation is the optimality equation

$$
q_\star(s,a)=\mathbb{E}\!\left[R_{t+1}+\gamma\max_{a'}q_\star(S_{t+1},a')\mid S_t=s,A_t=a\right].
$$

DP implements the corresponding full optimality backup (value iteration / Q-iteration):

$$
Q(s,a)\leftarrow \mathbb{E}\!\left[R+\gamma\max_{a'}Q(S',a')\mid S=s,A=a\right].
$$

The TD analogue is Q-learning, which replaces the expectation by a single observed transition:

$$
Q(s,a)\xleftarrow{\alpha} R+\gamma\max_{a'}Q(s',a').
$$

Here the target is greedy (via the max backup), while the behaviour policy used to collect data can remain exploratory.

## References

- https://github.com/zyxue/youtube_RL_course_by_David_Silver
