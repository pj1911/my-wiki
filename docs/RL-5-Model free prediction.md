## Model-Free prediction (policy evaluation)

**Recap.** In the last chapter, we planned with dynamic programming to solve a known MDP. In this chapter, we move to model-free prediction, where we estimate a value function in an unknown MDP. In the next chapter, we study model-free control, where we optimize value and aim to find an optimal policy in an unknown MDP.

### Setting: unknown MDP, fixed policy
In model-free prediction, we do not know the MDP model (transitions and rewards are unknown), but we can sample experience by interacting with the environment. Here, we focus on policy evaluation: given a fixed policy \( \pi \), we estimate its value function. To estimate \( v_\pi \) in an unknown MDP, we mainly use:

- Monte Carlo (MC) prediction
- Temporal Difference (TD) prediction

## Monte Carlo prediction

Monte Carlo (MC) prediction estimates the value function of a fixed policy \( \pi \) using sampled episodes of experience. We do not need the MDP model (transitions or rewards). Instead, we generate episodes by interacting with the environment, following policy \( \pi \) and learn from what actually happened. We compute returns \( G_t \) from visited states using rewards until termination. Therefore, this method fits episodic tasks, where each episode terminates at some time \( T \). If an episode never ends, the return is not naturally available in this form.

### Return and value under a fixed policy
Consider one episode generated by following policy \( \pi \):

$$
S_1, A_1, R_2, S_2, A_2, R_3, \dots, S_T.
$$

For a time step \( t \), we define the (discounted) return from that time as

$$
G_t = \sum_{k=0}^{T-t-1}\gamma^k R_{t+k+1}
      = R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{T-1-t}R_T,
$$

and the state-value function as the expected return when we start from state \( s \) and follow \( \pi \):

$$
v_\pi(s) = \mathbb{E}_\pi\!\left[G_t \mid S_t=s\right].
$$

Each time we visit a state \( s \) while following \( \pi \), we can compute a return sample starting from that visit. Let \( G^{(1)}(s), G^{(2)}(s), \dots, G^{(N(s))}(s) \) be the returns observed from \( N(s) \) number of visits to \( s \).  
Then MC estimates the value by the sample mean:

$$
V(s) = \frac{1}{N(s)}\sum_{i=1}^{N(s)} G^{(i)}(s)
\approx v_\pi(s).
$$

As \( N(s) \) grows, this average becomes a better estimate of the true expectation. In short, MC policy evaluation replaces the expected return in \( v_\pi(s) \) with an empirical mean computed from experience.

## First-Visit Monte Carlo policy evaluation

In first-visit MC, we update each state using the return from its first occurrence in an episode, and then average these returns across episodes to approximate \( v_\pi(s) \). For each state \( s \), we maintain:

$$
N(s)\ \#\{\text{episodes in which } s \text{ is visited at least once}\},\qquad S(s)\ \text{(sum of returns)},\qquad V(s)=\frac{S(s)}{N(s)}.
$$

### Procedure
For each episode generated by following \( \pi \) until termination,

$$
S_1, A_1, R_2, S_2, \dots, S_T,
$$

we do the following for every state \( s \) that appears in the episode:

1. Find the first time step \( t^\star \) in the episode such that \( S_{t^\star}=s \).
2. Compute the return from that first visit:

$$
G_{t^\star} = \sum_{k=0}^{T-t^\star-1}\gamma^k R_{t^\star+k+1}.
$$

3. Update totals and counts after each episode:

$$
N(s)\leftarrow N(s)+1,\qquad S(s)\leftarrow S(s)+G_{t^\star},\qquad V(s)\leftarrow \frac{S(s)}{N(s)}.
$$

**Practical notes.**
Episodes start from an initial-state distribution, which is typically set by the environment reset rule (sometimes a fixed start state, sometimes random). During an episode, a state \( s \) may appear multiple times, and first-visit MC means we update \( s \) using only the first time it appears in that episode (we do not need to return to \( s \) again within the same episode). Across episodes, we count how often \( s \) appears at least once and form \( V(s)=S(s)/N(s) \) by averaging the first-visit returns. This immediately tells us what we can estimate: if \( s \) is never visited, then \( N(s)=0 \) and we cannot evaluate \( v_\pi(s) \) from data. If \( s \) is rarely visited, the estimate has high variance and may be unreliable. If some states are unlikely under \( \pi \), or returns have high variance we do not care much about them, because we mainly care about the states we actually encounter under \( \pi \) (exploration or visiting more states becomes central in later chapters on control), and for any state that is visited infinitely often, the law of large numbers gives convergence:

$$
V(s)\to v_\pi(s)\quad \text{as}\quad N(s)\to\infty.
$$

Note that, we cannot stop the process once all the states have been visited once, as we still need the episode to terminate because the return \( G_{t^\star} \) for a state's first visit depends on rewards after that time step, all the way to the end of the episode.

## Every-Visit Monte Carlo policy evaluation

In every-visit MC, we estimate \( v_\pi(s) \) from complete episodes by averaging all returns observed after every visit to \( s \) (not just the first visit within an episode). This usually gives more samples per episode for visited states, at the cost of using correlated returns from the same trajectory.

### Procedure
Across episodes generated by following policy \( \pi \) to termination, we maintain for each state \( s \):

$$
N(s)\ \text{(number of visits to state s)},\qquad S(s)\ \text{(sum of returns)},\qquad V(s)=\frac{S(s)}{N(s)}.
$$

In one episode, for every time step \( t \) such that \( S_t=s \):

$$
N(s)\leftarrow N(s)+1,\qquad S(s)\leftarrow S(s)+G_t,\qquad V(s)\leftarrow \frac{S(s)}{N(s)}.
$$

Here, \( N(s) \) counts all visits to \( s \) (over all episodes and all time steps). As \( N(s)\to\infty \), we have \( V(s)\to v_\pi(s) \) under standard sampling assumptions.

### Incremental mean (online update)
To compute averages from a stream of data without storing all past samples (which is exactly what we need in MC), we use an incremental mean update. If we observe samples \( x_1,x_2,\dots \), the sample mean after \( k \) samples is

$$
\mu_k = \frac{1}{k}\sum_{j=1}^k x_j,
$$

and we can update it without storing history:

$$
\mu_k \leftarrow \mu_{k-1} + \frac{1}{k}\bigl(x_k-\mu_{k-1}\bigr).
$$

We can read this as an "error-correction" update:

$$
\mu_k \leftarrow \mu_{k-1} + \alpha_k\cdot \text{error},\qquad
\alpha_k=\frac{1}{k},\ \ \text{error}=x_k-\mu_{k-1}.
$$

Here, \( k \) is the number of samples seen so far. We start from an initial \( \mu_0 \) (often \( 0 \)) and update as samples arrive. This lets us update \( V(s) \) online across episodes without storing past returns: once an episode ends and we compute a new sample \( G \), we immediately fold it into \( V(s) \) via \( V(s)\leftarrow V(s)+\frac{1}{N(s)}(G-V(s)) \). So we store only \( (N(s),V(s)) \) per state, not the entire history.

**Incremental Monte Carlo value updates.** After the episode ends and all required returns are available:

- First-visit MC: for each state \( s \) that appears in the episode, take only its earliest occurrence \( t^\star \) and use the sample \( (s,\; G_{t^\star}) \) to update \( V(s) \) once.
- Every-visit MC: after the episode terminates, compute all \( G_t \) and update \( V(S_t) \) for each \( t=1,\dots,T-1 \).

**Forgetting old episodes.** If the environment is non-stationary (its dynamics or reward distribution can change over time), a long-run average may lag behind to what is currently happening. In that case, we use a constant step size \( \alpha \) instead of \( 1/N(s) \):

$$
V(s)\leftarrow V(s)+\alpha\bigl(G - V(s)\bigr).
$$

This creates an exponential moving average, so recent returns influence \( V(s) \) more than older ones.

## Temporal-Difference learning

Temporal-Difference (TD) learning is another model-free way to estimate \( v_\pi \) from experience. Again, we only need sampled interaction trajectories generated by following a fixed policy \( \pi \). Compared to Monte Carlo, the key difference is timing: TD updates during an episode, before termination, unlike MC which requires termination of episodes.

**The basic TD idea: learn from one transition.** At time step \( t \), we observe a single transition while following \( \pi \):

$$
S_t,\ A_t,\ R_{t+1},\ S_{t+1}.
$$

We then approximate the unknown "rest of the future" after \( S_{t+1} \) by our current estimate \( V(S_{t+1}) \). Therefore, instead of waiting for the full return \( G_t \), TD builds a one-step estimate of the return:

$$
\text{TD target} = R_{t+1}+\gamma V(S_{t+1}).
$$

This target is available immediately: \( R_{t+1} \) is observed after the transition, and \( V(S_{t+1}) \) is the current stored estimate for the observed next state \( (S_{t+1}) \), this allows the update before the episode terminates. We then move the current stored estimate at the present state, \( V(S_t) \), toward this target using step size \( \alpha \):

$$
V(S_t)\leftarrow V(S_t)+\alpha\Bigl(\delta_t\Bigr),
\qquad
\delta_t = R_{t+1}+\gamma V(S_{t+1})-V(S_t),
$$

where \( \delta_t \) is the TD error.

### Connection to the Bellman expectation equation
For a fixed policy \( \pi \), the Bellman expectation equation says:

$$
v_\pi(s)=\mathbb{E}_\pi\!\left[\,R_{t+1}+\gamma v_\pi(S_{t+1})\mid S_t=s\right].
$$

TD(0) is the sample-based version of this identity: we replace the expectation by the single observed reward \( R_{t+1} \) and replace the unknown \( v_\pi(S_{t+1}) \) with our current estimate \( V(S_{t+1}) \), then update \( V(S_t) \) toward that one-step target. This technique of updating is know as bootstrapping (discussed later in more details).

## MC vs. TD (1): Bias-variance tradeoff

Both Monte Carlo (MC) and Temporal Difference (TD) methods estimate the same object, \( v_\pi \), from experience under a fixed policy \( \pi \). The difference is the learning signal: MC uses a full-return target and updates after the episode ends, while TD updates during the episode using a bootstrapped one-step target.

### Bias-variance trade-off

**Unbiased targets (if we had the truth).**
For MC, the return is an unbiased sample of the value:

$$
\mathbb{E}_\pi\!\left[G_t\mid S_t\right]=v_\pi(S_t).
$$

Meanwhile for TD, the ideal one-step target would also be unbiased:

$$
\mathbb{E}_\pi\!\left[R_{t+1}+\gamma v_\pi(S_{t+1})\mid S_t\right]=v_\pi(S_t).
$$

But, in practice we do not know \( v_\pi \), so TD uses

$$
R_{t+1}+\gamma V(S_{t+1}),
$$

and this is generally a biased target because typically \( V(S_{t+1})\neq v_\pi(S_{t+1}) \).

## MC vs. TD (2): Certainty Equivalence in batch learning

So far, we assumed we can keep collecting new experience. With enough data, both MC and TD can converge to the true value:

$$
V(s)\to v_\pi(s)\qquad \text{as experience }\to\infty.
$$

In the batch setting, we instead get a fixed dataset of \( K \) episodes and we must learn only by reusing it:

$$
\mathcal{D}=\{\tau^{(1)},\dots,\tau^{(K)}\},\qquad
\tau^{(k)}=(S^{(k)}_1,A^{(k)}_1,R^{(k)}_2,\dots,S^{(k)}_{T_k}).
$$

We repeatedly sweep through \( \mathcal{D} \) (or sample episodes from it) and apply updates to the same data until the values stop changing.

### A-B example

**Environment and dataset.** We consider an episodic process with \( \gamma=1 \) and two non-terminal states \( A \) and \( B \). Our fixed dataset contains \( 8 \) episodes:

$$
(A,0,B,0),\quad (B,1),\quad (B,1),\quad (B,1),\quad (B,1),\quad (B,1),\quad (B,1),\quad (B,0).
$$

So:

- Episode 1 visits \( A\to B \) and then terminates with reward \( 0 \) from \( B \).
- Episodes 2--7 start at \( B \) and terminate with reward \( 1 \).
- Episode 8 starts at \( B \) and terminates with reward \( 0 \).

**Batch MC: empirical mean of observed returns.** Batch MC converges to the value function that best fits the observed Monte Carlo returns in the dataset, i.e., it treats each sampled return as a supervised target and solves a least-squares regression:

$$
V_{\text{MC}}
\in \arg\min_V \;\sum_{k=1}^{K}\sum_{t=1}^{T_k}\Bigl(G_t^{(k)} - V(s_t^{(k)})\Bigr)^2.
$$

Here the dataset consists of \( K \) recorded episodes (trajectories), indexed by \( k=1,\dots,K \). Episode \( k \) has length \( T_k \) time steps (i.e., it contains states \( s^{(k)}_1,\dots,s^{(k)}_{T_k} \)), so the inner sum ranges over all time indices within episode \( k \) and the outer sum aggregates the squared errors over all time steps in all episodes. In our tabular example, each state value \( V(s) \) is an independent parameter, so the objective separates by state. Group all terms with the same state \( s \):

$$
\sum_{k=1}^{K}\sum_{t=1}^{T_k}\bigl(G_t^{(k)}-V(s_t^{(k)})\bigr)^2
=\sum_{s}\sum_{i=1}^{n(s)}\bigl(G_i(s)-V(s)\bigr)^2 .
$$

Thus we can minimize each state’s sum of squares independently:

$$
V_{\text{MC}}(s)\in\arg\min_{v}\;\sum_{i=1}^{n(s)}\bigl(G_i(s)-v\bigr)^2 .
$$

Differentiate w.r.t. \( v \) and set to zero:

$$
\frac{d}{dv}\sum_{i=1}^{n(s)}(G_i(s)-v)^2
=-2\sum_{i=1}^{n(s)}(G_i(s)-v)=0
\;\Rightarrow\;
v=\frac{1}{n(s)}\sum_{i=1}^{n(s)}G_i(s).
$$

Therefore,

$$
V_{\text{MC}}(s)=\frac{1}{n(s)}\sum_{i=1}^{n(s)} G_i(s).
$$

where \( n(s) \) is the number of occurrences of state \( s \) in the dataset and \( G_i(s) \) are the corresponding sampled returns. For \( A \): \( n(A)=1 \) and the single observed return is \( G_1(A)=0 \), so

$$
V_{\text{MC}}(A)=\frac{1}{1}\bigl(0\bigr)=0.
$$

For \( B \): \( n(B)=8 \) and the observed returns are \( \{1,1,1,1,1,1,0,0\} \), so

$$
V_{\text{MC}}(B)=\frac{1}{8}\bigl(1+1+1+1+1+1+0+0\bigr)
=\frac{6}{8}=0.75.
$$

**Batch TD(0): fixed point induced by the dataset.** Batch TD(0) behaves differently: with a fixed dataset, it effectively treats the data as an empirical MDP and converges to the value function that satisfies the Bellman equations of that estimated model under \( \pi \). In particular, the dataset induces maximum-likelihood one-step estimates

$$
\hat{P}^{a}_{s,s'}=
\frac{\#\{(s_t=s,\ a_t=a,\ s_{t+1}=s')\}}{\#\{(s_t=s,\ a_t=a)\}},
\qquad
\hat{R}^{a}_{s}=
\frac{\sum_{t:\,s_t=s,\ a_t=a} r_{t+1}}{\#\{(s_t=s,\ a_t=a)\}},
$$

and TD(0) converges to the fixed point of the Bellman expectation equation for the estimated MDP
\( \langle \mathcal{S},\mathcal{A},\hat{P},\hat{R},\gamma\rangle \). In our dataset, \( A \) always transitions to \( B \) with reward \( 0 \), so

$$
V_{\text{TD}}(A)=0+\gamma V_{\text{TD}}(B),
$$

and from \( B \) the empirical mean terminal reward is \( 0.75 \) (with terminal value \( 0 \)), hence

$$
V_{\text{TD}}(B)=0.75.
$$

Therefore \( V_{\text{TD}}(A)=0.75 \) when \( \gamma=1 \) (and more generally \( V_{\text{TD}}(A)=\gamma\cdot 0.75 \)).

**Why the answers differ.** With a finite dataset, MC updates \( A \) using only the single observed return following \( A \) (here it happened to be \( 0 \)), so it keeps \( V(A)=0 \), whereas TD bootstraps through \( B \) and propagates the dataset's average outcome at \( B \) back through the observed transition \( A\!\to\!B \), yielding \( V(A)=V(B)=0.75 \). In batch learning (no new data), repeated passes over the same \( K \) episodes drive the methods to different stable solutions: MC converges to empirical mean returns, while TD converges to the fixed point induced by the dataset's empirical transition and reward estimates.

## MC vs. TD (3): The Markov property

In an MDP, the future is conditionally independent of the past given the current state \( S_t \), and TD(0) exploits this one-step Markov structure by updating from a single transition \( (S_t,R_{t+1},S_{t+1}) \) via

$$
V(S_t)\leftarrow V(S_t)+\alpha\bigl(R_{t+1}+\gamma V(S_{t+1})-V(S_t)\bigr),
$$

which lets information propagate efficiently along observed state-to-state transitions without waiting for complete outcomes. In contrast, MC uses the full return \( G_t \) as a black-box target that bundles the entire future into one number, this can work well, but it does not explicitly leverage the Markov conditional-independence structure and instead learns only from complete episode outcomes. As a rule of thumb, in standard Markov settings TD is often more data-efficient because it aligns with the environment's one-step dynamics, whereas with non-Markov signals (e.g. partial observability) MC can sometimes help because the full return may implicitly carry information from the episode history when the current state representation is incomplete.

## Method Summaries MC vs TD
**Monte Carlo (MC).**

- Unbiased target: MC uses the actual return \( G_t \), so

$$
\mathbb{E}_\pi\!\left[G_t\mid S_t=s\right]=v_\pi(s).
$$

- Higher variance: \( G_t \) aggregates many random future steps (actions, transitions, rewards).
- Often stable with approximation: MC looks like supervised learning on observed returns, which is usually numerically stable.
- Less sensitive to initialization: early errors in \( V \) do not affect the target (the target is \( G_t \)).
- Simple mental model: we "average returns" to estimate value.

**Temporal Difference (TD).**

- Bootstrapped (biased during learning): TD(0) uses the target

$$
R_{t+1}+\gamma V(S_{t+1}),
$$

which is biased when \( V(S_{t+1})\neq v_\pi(S_{t+1}) \).

- Lower variance: the target depends only on one transition plus a lookup of \( V(S_{t+1}) \).
- More sample-efficient: we update after each step and do not need to wait for termination.
- Tabular convergence: with a fixed \( \pi \) and suitable step sizes, TD(0) converges to \( v_\pi \).
- Can be unstable with approximation: bootstrapping + function approximation can cause divergence in some settings.
- More sensitive to initialization: poor initial \( V \) can propagate through the TD target.

### Convergence to the same value function?
In a stationary tabular setting where each relevant state is visited infinitely often under \( \pi \), MC with sample-mean updates and TD(0) with suitable step-size conditions both converge to the same value function \( v_\pi(s) \). In practice, with finite data, noisy returns, or function approximation, they can yield noticeably different estimates because their targets differ (full returns vs. bootstrapped) and they propagate information differently. An example of this can be seen in the next section.

## Bootstrapping and Sampling

Dynamic Programming, TD, and MC can be compared along two independent axes: bootstrapping (whether the update target uses the current estimate \( V \)) and sampling (whether expectations are approximated from data or computed exactly from a known model). MC is sampled but non-bootstrapped, using the complete return \( G_t \) as its target. TD is sampled and bootstrapped, using the one-step target \( R_{t+1}+\gamma V(S_{t+1}) \), and DP is bootstrapped but not sampled, using a full expected Bellman target such as \( \sum_{s'}P(s'|s,a)\bigl(R(s,a,s')+\gamma V(s')\bigr) \). Equivalently, all three are backups that move \( V(s) \) toward a target: DP performs a full expected backup using the model, while TD and MC perform sample backups using observed experience, TD uses a shallow one-step backup that propagates information via bootstrapping, whereas MC uses a deep backup to episode end with no bootstrapping. Many multi-step methods (e.g., TD(\( \lambda \))) interpolate between TD(0) and MC by trading off backup depth against the amount of bootstrapping.

## \( n \)-Step Prediction

\( n \)-step prediction provides a simple continuum between TD(0) and Monte-Carlo (MC). TD(0) uses a 1-step, bootstrapped target, updating immediately from \( (R_{t+1},S_{t+1}) \), while MC avoids bootstrapping by waiting for the episode to finish and using the full observed return. The idea of \( n \)-step methods is to look ahead for \( n \) steps: use \( n \) actual rewards, then bootstrap once at step \( n \). As \( n \) increases, the target depends less on the current value estimates and more on observed rewards, in the limit \( n\to\infty \) (for episodic tasks) the method reduces to MC.

**\( n \)-step return.**
For a trajectory generated under \( \pi \), define

$$
G_t^{(n)}=
\sum_{i=1}^{n}\gamma^{i-1}R_{t+i}
\;+\;
\gamma^{n} V(S_{t+n}).
$$

This is an \( n \)-step backup: accumulate \( n \) rewards, then terminate the target by bootstrapping from \( V \) at the state reached after \( n \) steps. Special cases recover familiar targets:

$$
G_t^{(1)} = R_{t+1} + \gamma V(S_{t+1}) \quad (\text{TD(0)}),\qquad
G_t^{(\infty)}=\sum_{i=1}^{T-t}\gamma^{i-1}R_{t+i}\quad (\text{MC}).
$$

If the episode terminates before \( t+n \), the bootstrap term is omitted and the sum truncates naturally at termination.

**\( n \)-step TD update.**
Using \( G_t^{(n)} \) yields the standard TD update form

$$
V(S_t)\leftarrow V(S_t)+\alpha\bigl(G_t^{(n)}-V(S_t)\bigr).
$$

**Choosing \( n \) (trade-off).**
Smaller \( n \) means heavier bootstrapping (typically higher bias but lower variance and more online-friendly updates), while larger \( n \) uses more real rewards (typically lower bias but higher variance and potentially delayed updates). Intermediate \( n \) often works well in practice by propagating multi-step information without the full variance of pure MC.

### Averaging \( n \)-Step Returns

Rather than choosing a single horizon \( n \), we can form a target by mixing multiple \( n \)-step returns: shorter backups tend to give lower-variance, more stable updates, while longer backups propagate credit farther through the episode. For instance,

$$
\frac{1}{2}G_t^{(2)}+\frac{1}{2}G_t^{(4)}
$$

blends a shallow (2-step) and a deeper (4-step) return. In general, we would like a principled mixture over many \( n \) values without explicitly computing every \( G_t^{(n)} \).

### \( \lambda \)-Return

The \( \lambda \)-return provides exactly such a mixture by averaging all \( n \)-step returns with geometric weights:

$$
G_t^{\lambda} \;=\; (1-\lambda)\sum_{n=1}^{\infty}\lambda^{n-1}G_t^{(n)}, \qquad \lambda\in[0,1].
$$

This yields a proper weighted average over backup depths and introduces a single interpolation parameter: \( \lambda=0 \) places all weight on \( n=1 \), giving \( G_t^{\lambda}=G_t^{(1)} \) (TD(0)), while \( \lambda\to 1 \) shifts mass to long horizons and approaches the MC return (less bootstrapping, more reliance on observed rewards).

## Eligibility Traces

The previous section introduced the \(\lambda\)-return as a way to assign credit across multiple time steps by blending \(n\)-step returns. The drawback is efficiency: computing it directly would require explicitly forming many \(G_t^{(n)}\). We therefore want an online procedure that achieves the same multi-step credit assignment without enumerating these returns and eligibility traces provide exactly this mechanism. Eligibility traces maintain a decaying "memory" of recently visited states. Each visit boosts a state's trace, which then fades over time while repeated visits reinforce it. When a TD error occurs, we distribute that error backward in proportion to the current trace, so recent and frequently visited states receive larger updates, while distant one-off states receive little.

**Rat example: frequency vs. recency.**

Consider the sequence \(\text{lever},\text{lever},\text{lever},\text{bell},\text{shock}\). A pure frequency rule would assign most blame to \(\text{lever}\) (it appears three times), while a pure recency rule would assign most blame to \(\text{bell}\) (it is closest to the shock). Eligibility traces do both: \(\text{bell}\) receives a large share because it is most recent, and \(\text{lever}\) can still receive substantial blame because its trace has been "topped up" multiple times despite decay.

**State eligibility trace.**

Formally, maintain a trace \(E_t(s)\) for each state \(s\). Each time step applies two operations: (\(i\)) decay all traces by a factor \(\gamma\lambda\), making older visits matter less, and (\(ii\)) increment the currently visited state by \(+1\), marking it as freshly visited. This yields the standard accumulating trace recursion

$$
E_0(s)=0,\qquad
E_t(s)=\gamma\lambda\,E_{t-1}(s) + \mathbf{1}(S_t=s).
$$

Here \(\gamma\) is the discount factor from the problem definition, and \(\lambda\in[0,1]\) controls how slowly traces decay: larger \(\lambda\) makes traces persist longer and allows credit to propagate farther into the past. Equivalently, the effective memory length is governed by the product \(\gamma\lambda\): if \(\gamma\lambda\) is small, traces vanish quickly, if it is near \(1\), traces remain significant for many steps.

### Backward view of TD(\(\lambda\))

The update above is called the backward view of TD(\(\lambda\)) because each one-step TD error \(\delta_t\) is computed locally from \((S_t,R_{t+1},S_{t+1})\) but its effect is propagated backward through time via the traces: states with larger \(e_t(s)\) receive a larger portion of the update \(\alpha\,\delta_t\,e_t(s)\), while states with small traces receive negligible updates. Thus, traces turn a local error signal into multi-step credit assignment.

### Forward view of TD(\(\lambda\))

The forward view describes TD(\(\lambda\)) in terms of the target each visited state is trying to match. If we could look ahead, then at time \(t\) we would form the \(\lambda\)-return

$$
G_t^\lambda \;=\; (1-\lambda)\sum_{n=1}^{T-t}\lambda^{n-1}G_t^{(n)} \;+\; \lambda^{T-t}G_t^{(T-t)},
$$

a geometrically weighted average of the \(n\)-step returns \(G_t^{(n)}\). The forward-view update is then simply

$$
V(S_t)\leftarrow V(S_t)+\alpha\bigl(G_t^{\lambda}-V(S_t)\bigr).
$$

This is conceptually clean, but it has its practical limitation: \(G_t^\lambda\) depends on future rewards and is therefore not directly available at time \(t\).

**Summary.**

Eligibility traces turn a one-step TD error into a multi-step learning update by keeping a decaying record of recently (and repeatedly) visited states, so “recent and frequent” states get the most credit.

## TD(1) as Monte Carlo, and the forward/backward equivalence

Consider an episodic trajectory \(S_0,R_1,S_1,\dots,S_T\) with terminal \(S_T\) and discount \(\gamma\in[0,1]\).
Let \(V(\cdot)\) be the current value estimate and define the (one-step) TD error

$$
\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t), \qquad t=0,\dots,T-1,
$$

with \(V(S_T)=0\) (no bootstrapping past the terminal state).

### Forward view: \(\lambda\)-return and the case \(\lambda=1\)

The Monte Carlo return from time \(t\) is

$$
G_t =\; R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{T-t-1}R_T=  \sum_{i=0}^{T-t-1} \gamma^i R_{t+1+i}.
$$

In the forward view, TD(\(\lambda\)) updates \(V(S_t)\) toward the \(\lambda\)-return \(G_t^\lambda\), a geometrically weighted
mixture of \(n\)-step returns.

$$
G_t^\lambda \;=\; (1-\lambda)\sum_{n=1}^{T-t}\lambda^{n-1}G_t^{(n)} \;+\; \lambda^{T-t}G_t^{(T-t)},
$$

Set \(\lambda=1\):

$$
G_t^{1}
= (1-1)\sum_{n=1}^{T-t}1^{\,n-1}G_t^{(n)} \;+\; 1^{\,T-t}G_t^{(T-t)}
= 0 \;+\; G_t^{(T-t)}
= G_t^{(T-t)}.
$$

Using \(G_t^{(n)}=\sum_{i=0}^{n-1}\gamma^i R_{t+1+i}+\gamma^n V(S_{t+n})\), set \(n=T-t\):

$$
G_t^{(T-t)}=\sum_{i=0}^{T-t-1}\gamma^i R_{t+1+i}+\gamma^{T-t}V(S_T)
=\sum_{i=0}^{T-t-1}\gamma^i R_{t+1+i}=G_t
$$

since \(S_T\) is terminal and \(V(S_T)=0\). Therefore, when \(\lambda=1\), the mixture places all its weight on the longest backup, so the \(\lambda\)-return reduces to the full
Monte Carlo return \(G_t^1=G_t^{(T-t)}=G_t\).
Thus the forward-view TD(\(1\)) update becomes the standard Monte Carlo update

$$
V(S_t) \leftarrow V(S_t) + \alpha\big(G_t - V(S_t)\big).
$$

### Backward view: eligibility traces and equivalence

The backward view implements the same overall effect online without explicitly forming \(G_t^\lambda\).
Using accumulating traces,

$$
E_t(s) = \gamma\lambda\,E_{t-1}(s) + \mathbf{1}\{S_t=s\}, \qquad E_{-1}(s)=0,
$$

each TD error is distributed to all states proportionally to their current eligibility:

$$
V(s) \leftarrow V(s) + \alpha\,\delta_t\,E_t(s).
$$

Summing these per-step increments over an episode gives the net change to state \(s\),

$$
\Delta V(s) \;=\; \sum_{t=0}^{T-1}\alpha\,\delta_t\,E_t(s).
$$

A standard way to see the equivalence is to rewrite the forward-view error in terms of TD errors and then swap the order of summation.

**Step 1: \(n\)-step error as a sum of TD errors (telescoping).**

Starting from the definition,

$$
G_t^{(n)} - V(S_t)
= \sum_{i=0}^{n-1}\gamma^i R_{t+1+i} + \gamma^n V(S_{t+n}) - V(S_t).
$$

Rewrite the rewards using the TD error identity

$$
\delta_{t+i}=R_{t+i+1}+\gamma V(S_{t+i+1})-V(S_{t+i})
\quad\Longrightarrow\quad
R_{t+i+1}=\delta_{t+i}-\gamma V(S_{t+i+1})+V(S_{t+i}).
$$

Substitute into the return:

$$
G_t^{(n)} - V(S_t)
= \sum_{i=0}^{n-1}\gamma^i\big(\delta_{t+i}-\gamma V(S_{t+i+1})+V(S_{t+i})\big)
    +\gamma^n V(S_{t+n})-V(S_t)
$$

$$
= \sum_{i=0}^{n-1}\gamma^i\delta_{t+i}
   \;-\;\sum_{i=0}^{n-1}\gamma^{i+1}V(S_{t+i+1})
   \;+\;\sum_{i=0}^{n-1}\gamma^i V(S_{t+i})
   \;+\;\gamma^n V(S_{t+n})-V(S_t).
$$

Now the value terms telescope: split the last two sums,

$$
\sum_{i=0}^{n-1}\gamma^i V(S_{t+i}) = V(S_t)+\sum_{i=1}^{n-1}\gamma^i V(S_{t+i}),
\qquad
\sum_{i=0}^{n-1}\gamma^{i+1}V(S_{t+i+1})=\sum_{i=1}^{n}\gamma^i V(S_{t+i}),
$$

Consider the value-only terms:

$$
-\sum_{i=0}^{n-1}\gamma^{i+1}V(S_{t+i+1})
+\sum_{i=0}^{n-1}\gamma^i V(S_{t+i})
+\gamma^n V(S_{t+n})-V(S_t).
$$

Substituting:

$$
-\sum_{i=1}^{n}\gamma^i V(S_{t+i}) + V(S_t)+\sum_{i=1}^{n-1}\gamma^i V(S_{t+i}) +\;\gamma^n V(S_{t+n})-V(S_t) =0
$$

Therefore,

$$
G_t^{(n)} - V(S_t)=\sum_{i=0}^{n-1}\gamma^i\delta_{t+i}.
$$

Finally, re-index with \(k=t+i\):

$$
\sum_{i=0}^{n-1}\gamma^i\delta_{t+i}
=\sum_{k=t}^{t+n-1}\gamma^{k-t}\delta_k,
$$

This gives the result:

$$
G_t^{(n)} - V(S_t) =\sum_{k=t}^{t+n-1}\gamma^{k-t}\delta_k.
$$

**Step 2: \(\lambda\)-return error becomes a discounted sum of TD errors.**

Plug the identity above into the definition of \(G_t^\lambda\):

$$
G_t^\lambda - V(S_t)
= \Bigl[(1-\lambda)\sum_{n=1}^{T-t}\lambda^{n-1}G_t^{(n)}+\lambda^{T-t}G_t^{(T-t)}\Bigr]-V(S_t)
$$

$$
= (1-\lambda)\sum_{n=1}^{T-t}\lambda^{n-1}\bigl(G_t^{(n)}-V(S_t)\bigr)
\;+\;\lambda^{T-t}\bigl(G_t^{(T-t)}-V(S_t)\bigr)
$$

$$
= (1-\lambda)\sum_{n=1}^{T-t}\lambda^{n-1}\sum_{k=t}^{t+n-1}\gamma^{k-t}\delta_k
\;+\;\lambda^{T-t}\sum_{k=t}^{T-1}\gamma^{k-t}\delta_k,
$$

where the last equality uses \(t+(T-t)-1=T-1\).
Fix \(k\ge t\) and collect the coefficient of \(\gamma^{k-t}\delta_k\).

In the first double-sum, \(\delta_k\) appears whenever \(k\le t+n-1\), i.e. \(n\ge k-t+1\), so its total weight there is

$$
(1-\lambda)\sum_{n=k-t+1}^{T-t}\lambda^{n-1}.
$$

Adding the second term contributes an additional \(\lambda^{T-t}\). Hence the total coefficient on \(\gamma^{k-t}\delta_k\) is

$$
(1-\lambda)\sum_{n=k-t+1}^{T-t}\lambda^{n-1}+\lambda^{T-t}.
$$

Evaluate the geometric sum:

$$
\sum_{n=k-t+1}^{T-t}\lambda^{n-1}
=\sum_{j=k-t}^{T-t-1}\lambda^{j}
=\lambda^{k-t}\frac{1-\lambda^{T-k}}{1-\lambda}.
$$

Therefore,

$$
(1-\lambda)\sum_{n=k-t+1}^{T-t}\lambda^{n-1}+\lambda^{T-t}
=\lambda^{k-t}(1-\lambda^{T-k})+\lambda^{T-t}
=\lambda^{k-t}.
$$

So the coefficient of \(\gamma^{k-t}\delta_k\) is \(\lambda^{k-t}\), and we obtain

$$
G_t^\lambda - V(S_t)
=\sum_{k=t}^{T-1}\gamma^{k-t}\lambda^{k-t}\delta_k
=\sum_{k=t}^{T-1}(\gamma\lambda)^{k-t}\delta_k.
$$

**Step 3: accumulate forward-view updates and swap sums.**

Sum the forward-view updates affecting a particular state \(s\):

$$
\sum_{t=0}^{T-1}\alpha\bigl(G_t^\lambda - V(S_t)\bigr)\mathbf{1}\{S_t=s\}
= \sum_{t=0}^{T-1}\alpha\,\mathbf{1}\{S_t=s\}\sum_{k=t}^{T-1}(\gamma\lambda)^{k-t}\delta_k
$$

$$
= \alpha\sum_{t=0}^{T-1}\sum_{k=t}^{T-1} \mathbf{1}\{S_t=s\}(\gamma\lambda)^{k-t}\delta_k
$$

Start with the double sum

$$
\alpha\sum_{t=0}^{T-1}\sum_{k=t}^{T-1} \mathbf{1}\{S_t=s\}(\gamma\lambda)^{k-t}\delta_k.
$$

The index set here is the triangle

$$
\{(t,k): 0\le t\le T-1,\; t\le k\le T-1\} \;=\; \{(t,k): 0\le k\le T-1,\; 0\le t\le k\}.
$$

So we can swap the order:

$$
\alpha\sum_{t=0}^{T-1}\sum_{k=t}^{T-1} (\cdots)
\;=\;
\sum_{k=0}^{T-1}\alpha\,\delta_k \sum_{t=0}^{k}(\gamma\lambda)^{k-t}\mathbf{1}\{S_t=s\}.
$$

**Step 4: the inner sum is exactly the trace.**

Unrolling the accumulating-trace recursion

$$
E_k(s)=\gamma\lambda E_{k-1}(s)+\mathbf{1}\{S_k=s\},\qquad E_{-1}(s)=0,
$$

gives

$$
E_k(s)=\sum_{t=0}^{k}(\gamma\lambda)^{k-t}\mathbf{1}\{S_t=s\}.
$$

Substitute this into the expression above to obtain

$$
\sum_{t=0}^{T-1}\alpha\bigl(G_t^\lambda - V(S_t)\bigr)\mathbf{1}\{S_t=s\}
\;=\;
\sum_{k=0}^{T-1}\alpha\,\delta_k\,E_k(s),
$$

which is the desired forward/backward equivalence.

Therefore the forward view explains what is being optimized (learning toward \(G_t^\lambda\)), while the backward view
explains how to realize it incrementally via traces. In particular, when \(\lambda=1\) we have \(G_t^\lambda = G_t\),
so the episode-level effect of TD(1) matches Monte Carlo.

### Special case: \(s\) is visited only once

Assume a state \(s\) appears exactly once, at time \(k\) (i.e., \(S_k=s\) and \(S_t\neq s\) for \(t\neq k\)). For TD(1),
the trace recursion becomes a pure geometric decay after the visit:

$$
E_t(s)=\gamma E_{t-1}(s)+\mathbf{1}(S_t=s)
\;=\;
\begin{cases}
0, & t<k,\\[2pt]
\gamma^{t-k}, & t\ge k.
\end{cases}
$$

Hence the total change to \(V(s)\) over the episode is a discounted sum of subsequent TD errors:

$$
\Delta V(s) \;=\; \alpha\sum_{t=k}^{T-1}\gamma^{t-k}\delta_t.
$$

This sum telescopes: the \(-V(S_t)\) term in \(\delta_t\) cancels with the \(+\gamma V(S_{t+1})\) term from the previous step
once we apply the discount powers. What remains is exactly “observed return minus current estimate”:

$$
\sum_{t=k}^{T-1}\gamma^{t-k}\delta_t =
\Big(\sum_{i=0}^{T-k-1}\gamma^i R_{k+1+i}\Big) - V(S_k) =
G_k - V(S_k).
$$

Therefore,

$$
\Delta V(s) \;=\; \alpha\big(G_k - V(S_k)\big),
$$

which is precisely the Monte Carlo update for the single visit at time \(k\).

## Telescoping in TD(1) (why it matches MC at the episode level)

Let's assume we are working in an episodic setting with discount \(\gamma\in[0,1]\), terminal time \(T\), and value estimate \(V\) with the terminal
convention \(V(S_T)=0\). Define the one-step TD error

$$
\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t), \qquad t=0,\dots,T-1.
$$

**A telescoping identity.** From the \(n\)-step derivation already shown earlier (the identity \(G_t^{(n)}-V(S_t)=\sum_{k=t}^{t+n-1}\gamma^{k-t}\delta_k\)),
take \(n=T-t\) to obtain

$$
\sum_{k=t}^{T-1}\gamma^{k-t}\delta_k \;=\; G_t^{(T-t)} - V(S_t).
$$

Since \(S_T\) is terminal and \(V(S_T)=0\), the \((T-t)\)-step return is exactly the Monte Carlo return:

$$
G_t^{(T-t)} \;=\; \sum_{i=0}^{T-t-1}\gamma^i R_{t+1+i} \;=\; G_t.
$$

Combining these gives the telescoping relation

$$
\delta_t + \gamma\delta_{t+1} + \cdots + \gamma^{T-1-t}\delta_{T-1}
\;=\;
G_t - V(S_t).
$$

(Note: the explicit term-by-term cancellation expansion is the same telescoping argument used in the earlier
\(n\)-step identity, so it is omitted here.)

### TD(1) is every-visit MC when updates are applied offline

In the backward view, TD(\(\lambda\)) applies per-step increments

$$
\Delta V(s)\!\mid_t \;=\; \alpha\,\delta_t\,E_t(s),
\qquad
E_t(s)=\gamma\lambda E_{t-1}(s)+\mathbf{1}\{S_t=s\},
$$

and the episode-level (offline) change is the sum of these increments:

$$
\Delta V(s)\;=\;\sum_{t=0}^{T-1}\alpha\,\delta_t\,E_t(s).
$$

For \(\lambda=1\), unrolling the trace recursion gives the explicit form

$$
E_u(s)\;=\;\sum_{t=0}^{u}\gamma^{u-t}\mathbf{1}\{S_t=s\},
$$

so

$$
\Delta V(s)
=\sum_{u=0}^{T-1}\alpha\,\delta_u\sum_{t=0}^{u}\gamma^{u-t}\mathbf{1}\{S_t=s\}
=\sum_{t=0}^{T-1}\alpha\,\mathbf{1}\{S_t=s\}\sum_{u=t}^{T-1}\gamma^{u-t}\delta_u,
$$

where we swapped the order of summation over the region \(0\le t\le u\le T-1\).

By the telescoping identity,

$$
\sum_{u=t}^{T-1}\gamma^{u-t}\delta_u \;=\; G_t - V(S_t),
$$

hence

$$
\Delta V(s)
=\sum_{t=0}^{T-1}\alpha\bigl(G_t - V(S_t)\bigr)\mathbf{1}\{S_t=s\},
$$

which is exactly the state-wise form of the every-visit Monte Carlo update.

**What differs in practice?**

The equality above is an episode-level statement (often presented for offline/accumulated updates).
Conceptually, MC computes \(G_t\) directly from the full reward sequence after the episode ends, while TD(1)
constructs the same quantity indirectly: it generates one-step TD errors online and uses traces to “route” those errors
back to earlier states. The end result matches, but the bookkeeping and when information is used are different.

## Telescoping in TD(\(\lambda\)) and the forward/backward match

Assume an episodic trajectory \(S_0,R_1,S_1,\dots,S_T\) with discount \(\gamma\in[0,1]\) and terminal convention \(V(S_T)=0\).
Define the one-step TD error

$$
\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t), \qquad t=0,\dots,T-1.
$$

**\(\lambda\)-return as a weighted sum of \(n\)-step targets.** The \(n\)-step return is

$$
G_t^{(n)} = \sum_{i=0}^{n-1}\gamma^i R_{t+1+i} + \gamma^n V(S_{t+n}),
$$

and the episodic \(\lambda\)-return can be written (equivalently) as a geometric mixture over horizons, with the remaining
probability mass placed on the full return:

$$
G_t^\lambda = (1-\lambda)\sum_{n=1}^{T-t-1}\lambda^{n-1}G_t^{(n)} \;+\; \lambda^{T-t-1}G_t.
$$

**TD errors telescope to the \(\lambda\)-error.** Using the identity proved earlier,

$$
G_t^{(n)} - V(S_t) \;=\; \sum_{k=t}^{t+n-1}\gamma^{k-t}\delta_k,
$$

one obtains (by substituting into the \(\lambda\)-return and collecting coefficients) the compact telescoping form

$$
G_t^\lambda - V(S_t)
\;=\;
\sum_{k=t}^{T-1}(\gamma\lambda)^{k-t}\delta_k
\;=\;
\delta_t + (\gamma\lambda)\delta_{t+1} + \cdots + (\gamma\lambda)^{T-1-t}\delta_{T-1}.
$$

(The term-by-term cancellation intuition is the same telescoping argument already used in the \(n\)-step case, so we omit a second expansion here.)

### Backward view: eligibility traces recover the same target

The backward view implements the update online without explicitly forming \(G_t^\lambda\). It maintains accumulating traces

$$
E_t(s) = \gamma\lambda\,E_{t-1}(s) + \mathbf{1}\{S_t=s\}, \qquad E_{-1}(s)=0,
$$

and applies the per-step increment

$$
\Delta V(s)\!\mid_t \;=\; \alpha\,\delta_t\,E_t(s).
$$

Thus each local TD error \(\delta_t\) is distributed across previously visited states, with geometric decay controlled by \(\gamma\lambda\). To see the link to the forward view without repeating the full derivation, unroll the trace:

$$
E_t(s) \;=\; \sum_{k=0}^{t}(\gamma\lambda)^{t-k}\mathbf{1}\{S_k=s\}.
$$

Accumulating increments over an episode (offline) gives

$$
\Delta V(s)
=\sum_{t=0}^{T-1}\alpha\,\delta_t\,E_t(s)
$$

$$
=\sum_{t=0}^{T-1}\alpha\,\delta_t\sum_{k=0}^{t}(\gamma\lambda)^{t-k}\mathbf{1}\{S_k=s\}
$$

$$
=\sum_{k=0}^{T-1}\alpha\,\mathbf{1}\{S_k=s\}\sum_{t=k}^{T-1}(\gamma\lambda)^{t-k}\delta_t,
$$

where the last step is just swapping sums over \(0\le k\le t\le T-1\). By the telescoping identity,

$$
\sum_{t=k}^{T-1}(\gamma\lambda)^{t-k}\delta_t \;=\; G_k^\lambda - V(S_k),
$$

so

$$
\Delta V(s)
=\sum_{k=0}^{T-1}\alpha\bigl(G_k^\lambda - V(S_k)\bigr)\mathbf{1}\{S_k=s\},
$$

which is exactly the episode-level (offline) forward-view update, written state-by-state.

## Online vs. Offline: when do forward and backward TD(\(\lambda\)) agree?

Fix an episodic trajectory \(S_0,R_1,S_1,\dots,S_T\) with discount \(\gamma\in[0,1]\) and terminal convention \(V(S_T)=0\).
Let

$$
\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t), \qquad t=0,\dots,T-1.
$$

**What is being compared?** The forward view specifies a target \(G_t^\lambda\) for each visit and updates \(V(S_t)\) toward it, the backward view produces
updates online by distributing each \(\delta_t\) through eligibility traces. The two views coincide only once we fix when
\(V\) is allowed to change.

### Offline (frozen-\(V\)) updates: exact equality

If the episode is generated with \(V\) held fixed and all changes are applied at the end, then the forward-view and backward-view
episode updates are identical:

$$
\sum_{t=0}^{T-1}\alpha\,\delta_t\,E_t(s)
\;=\;
\sum_{t=0}^{T-1}\alpha\bigl(G_t^\lambda - V(S_t)\bigr)\mathbf{1}\{S_t=s\}.
$$

This is exactly the equivalence derived in the previous section (it relies on using the same frozen \(V\) inside all bootstrapping
terms and TD errors throughout the episode). The endpoint cases discussed earlier follow immediately: \(\lambda=0\) gives TD(0), and
\(\lambda=1\) gives TD(1), whose episode-level update matches every-visit Monte Carlo via telescoping.

### Online updates: why equality can fail

With online learning, \(V\) is modified after each transition. Then the forward view becomes ambiguous because its target
\(G_t^\lambda\) contains bootstrapping terms \(V(S_{t+n})\): under online updates there is no single “\(V\)” to plug in—those values
depend on which intermediate version of \(V\) we use. The backward view, meanwhile, propagates each new \(\delta_t\) through traces
computed from past visits, but applies it using the current \(V\). Because both the TD errors and the bootstrapping values are
now tied to a moving function, the clean algebraic match from the frozen-\(V\) case no longer goes through in general. (An exception is \(\lambda=0\), where the target is purely one-step and the forward/backward views coincide in the usual way.)

**Restoring exact online equivalence: True Online TD(\(\lambda\)).** True Online TD(\(\lambda\)) modifies the trace-based update with a small correction so that, even when \(V\) changes during the episode, the backward-view recursion matches the corresponding online forward view exactly at each step.

## Summary of Forward and Backward TD(\(\lambda\))

| Offline updates | \(\lambda = 0\) | \(\lambda \in (0,1)\) | \(\lambda = 1\) |
|---|---|---|---|
| Backward view | \(\mathrm{TD}(0)\) | \(\mathrm{TD}(\lambda)\) | \(\mathrm{TD}(1)\) |
|  | \(\parallel\) | \(\parallel\) | \(\parallel\) |
| Forward view | \(\mathrm{TD}(0)\) | Forward \(\mathrm{TD}(\lambda)\) | \(\mathrm{MC}\) |
| **Online updates** | \(\lambda = 0\) | \(\lambda \in (0,1)\) | \(\lambda = 1\) |
| Backward view | \(\mathrm{TD}(0)\) | \(\mathrm{TD}(\lambda)\) | \(\mathrm{TD}(1)\) |
|  | \(\parallel\) | \(\nparallel\) | \(\nparallel\) |
| Forward view | \(\mathrm{TD}(0)\) | Forward \(\mathrm{TD}(\lambda)\) | \(\mathrm{MC}\) |
|  | \(\parallel\) | \(\parallel\) | \(\parallel\) |
| Exact Online | \(\mathrm{TD}(0)\) | Exact Online \(\mathrm{TD}(\lambda)\) | Exact Online \(\mathrm{TD}(1)\) |

\(\parallel\) here indicates equivalence in total update at end of episode.

## References

- https://github.com/zyxue/youtube_RL_course_by_David_Silver
