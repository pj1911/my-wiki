## Model-Free Prediction (Policy Evaluation)

### Recap
In the last chapter, we planned with dynamic programming to solve a known MDP. In this chapter, we move to model-free prediction, where we estimate a value function in an unknown MDP. In the next chapter, we study model-free control, where we optimize value and aim to find an optimal policy in an unknown MDP.

### Setting: unknown MDP, fixed policy
In model-free prediction, we do not know the MDP model (transitions and rewards are unknown), but we can sample experience by interacting with the environment.  
Here, we focus on policy evaluation: given a fixed policy \( \pi \), we estimate its value function. To estimate \( v_\pi \) in an unknown MDP, we mainly use:

- Monte Carlo (MC) prediction
- Temporal Difference (TD) prediction

## Monte Carlo Prediction

Monte Carlo (MC) prediction estimates the value function of a fixed policy \( \pi \) using sampled episodes of experience.  
We do not need the MDP model (transitions or rewards). Instead, we generate episodes by interacting with the environment, following policy \( \pi \) and learn from what actually happened. We compute returns \( G_t \) from visited states using rewards until termination. Therefore, this method fits episodic tasks, where each episode terminates at some time \( T \). If an episode never ends, the return is not naturally available in this form.

### Return and value under a fixed policy
Consider one episode generated by following policy \( \pi \):

$$
S_1, A_1, R_2, S_2, A_2, R_3, \dots, S_T.
$$

For a time step \( t \), we define the (discounted) return from that time as

$$
G_t = \sum_{k=0}^{T-t-1}\gamma^k R_{t+k+1}
      = R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{T-1-t}R_T,
$$

and the state-value function as the expected return when we start from state \( s \) and follow \( \pi \):

$$
v_\pi(s) = \mathbb{E}_\pi\!\left[G_t \mid S_t=s\right].
$$

Each time we visit a state \( s \) while following \( \pi \), we can compute a return sample starting from that visit.  
Let \( G^{(1)}(s), G^{(2)}(s), \dots, G^{(N(s))}(s) \) be the returns observed from \( N(s) \) number of visits to \( s \).  
Then MC estimates the value by the sample mean:

$$
V(s) = \frac{1}{N(s)}\sum_{i=1}^{N(s)} G^{(i)}(s)
\approx v_\pi(s).
$$

As \( N(s) \) grows, this average becomes a better estimate of the true expectation. In short, MC policy evaluation replaces the expected return in \( v_\pi(s) \) with an empirical mean computed from experience.

## First-Visit Monte Carlo Policy Evaluation

In first-visit MC, we update each state using the return from its first occurrence in an episode, and then average these returns across episodes to approximate \( v_\pi(s) \). For each state \( s \), we maintain:

$$
N(s)\ \#\{\text{episodes in which } s \text{ is visited at least once}\},\qquad S(s)\ \text{(sum of returns)},\qquad V(s)=\frac{S(s)}{N(s)}.
$$

### Procedure
For each episode generated by following \( \pi \) until termination,

$$
S_1, A_1, R_2, S_2, \dots, S_T,
$$

we do the following for every state \( s \) that appears in the episode:

1. Find the first time step \( t^\star \) in the episode such that \( S_{t^\star}=s \).
2. Compute the return from that first visit:

$$
G_{t^\star} = \sum_{k=0}^{T-t^\star-1}\gamma^k R_{t^\star+k+1}.
$$

3. Update totals and counts after each episode:

$$
N(s)\leftarrow N(s)+1,\qquad S(s)\leftarrow S(s)+G_{t^\star},\qquad V(s)\leftarrow \frac{S(s)}{N(s)}.
$$

**Practical notes**
Episodes start from an initial-state distribution, which is typically set by the environment reset rule (sometimes a fixed start state, sometimes random). During an episode, a state \( s \) may appear multiple times, and first-visit MC means we update \( s \) using only the first time it appears in that episode (we do not need to return to \( s \) again within the same episode). Across episodes, we count how often \( s \) appears at least once and form \( V(s)=S(s)/N(s) \) by averaging the first-visit returns. This immediately tells us what we can estimate: if \( s \) is never visited, then \( N(s)=0 \) and we cannot evaluate \( v_\pi(s) \) from data. If \( s \) is rarely visited, the estimate has high variance and may be unreliable. If some states are unlikely under \( \pi \), or returns have high variance we do not care much about them, because we mainly care about the states we actually encounter under \( \pi \) (exploration or visiting more states becomes central in later chapters on control), and for any state that is visited infinitely often, the law of large numbers gives convergence:

$$
V(s)\to v_\pi(s)\quad \text{as}\quad N(s)\to\infty.
$$

Note that, we cannot stop the process once all the states have been visited once, as we still need the episode to terminate because the return \( G_{t^\star} \) for a state's first visit depends on rewards after that time step, all the way to the end of the episode.

## Every-Visit Monte Carlo Policy Evaluation

In every-visit MC, we estimate \( v_\pi(s) \) from complete episodes by averaging all returns observed after every visit to \( s \) (not just the first visit within an episode). This usually gives more samples per episode for visited states, at the cost of using correlated returns from the same trajectory.

### Procedure
Across episodes generated by following policy \( \pi \) to termination, we maintain for each state \( s \):

$$
N(s)\ \text{(number of visits to state s)},\qquad S(s)\ \text{(sum of returns)},\qquad V(s)=\frac{S(s)}{N(s)}.
$$

In one episode, for every time step \( t \) such that \( S_t=s \):

$$
N(s)\leftarrow N(s)+1,\qquad S(s)\leftarrow S(s)+G_t,\qquad V(s)\leftarrow \frac{S(s)}{N(s)}.
$$

Here, \( N(s) \) counts all visits to \( s \) (over all episodes and all time steps). As \( N(s)\to\infty \), we have \( V(s)\to v_\pi(s) \) under standard sampling assumptions.

### Incremental mean (online update)
To compute averages from a stream of data without storing all past samples (which is exactly what we need in MC), we use an incremental mean update. If we observe samples \( x_1,x_2,\dots \), the sample mean after \( k \) samples is

$$
\mu_k = \frac{1}{k}\sum_{j=1}^k x_j,
$$

and we can update it without storing history:

$$
\mu_k \leftarrow \mu_{k-1} + \frac{1}{k}\bigl(x_k-\mu_{k-1}\bigr).
$$

We can read this as an "error-correction" update:

$$
\mu_k \leftarrow \mu_{k-1} + \alpha_k\cdot \text{error},\qquad
\alpha_k=\frac{1}{k},\ \ \text{error}=x_k-\mu_{k-1}.
$$

Here, \( k \) is the number of samples seen so far. We start from an initial \( \mu_0 \) (often \( 0 \)) and update as samples arrive. This lets us update \( V(s) \) online across episodes without storing past returns: once an episode ends and we compute a new sample \( G \), we immediately fold it into \( V(s) \) via \( V(s)\leftarrow V(s)+\frac{1}{N(s)}(G-V(s)) \). So we store only \( (N(s),V(s)) \) per state, not the entire history.

### Incremental Monte Carlo Value Updates
After the episode ends and all required returns are available:

- First-visit MC: for each state \( s \) that appears in the episode, take only its earliest occurrence \( t^\star \) and use the sample \( (s,\; G_{t^\star}) \) to update \( V(s) \) once.
- Every-visit MC: after the episode terminates, compute all \( G_t \) and update \( V(S_t) \) for each \( t=1,\dots,T-1 \).

### Forgetting Old Episodes
If the environment is non-stationary (its dynamics or reward distribution can change over time), a long-run average may lag behind to what is currently happening. In that case, we use a constant step size \( \alpha \) instead of \( 1/N(s) \):

$$
V(s)\leftarrow V(s)+\alpha\bigl(G - V(s)\bigr).
$$

This creates an exponential moving average, so recent returns influence \( V(s) \) more than older ones.

## Temporal-Difference Learning

Temporal-Difference (TD) learning is another model-free way to estimate \( v_\pi \) from experience. Again, we only need sampled interaction trajectories generated by following a fixed policy \( \pi \). Compared to Monte Carlo, the key difference is timing: TD updates during an episode, before termination, unlike MC which requires termination of episodes.

### The basic TD idea: learn from one transition
At time step \( t \), we observe a single transition while following \( \pi \):

$$
S_t,\ A_t,\ R_{t+1},\ S_{t+1}.
$$

We then approximate the unknown ``rest of the future'' after \( S_{t+1} \) by our current estimate \( V(S_{t+1}) \). Therefore, instead of waiting for the full return \( G_t \), TD builds a one-step estimate of the return:

$$
\text{TD target} = R_{t+1}+\gamma V(S_{t+1}).
$$

This target is available immediately: \( R_{t+1} \) is observed after the transition, and \( V(S_{t+1}) \) is the current stored estimate for the observed next state \( (S_{t+1}) \), this allows the update before the episode terminates. We then move the current stored estimate at the present state, \( V(S_t) \), toward this target using step size \( \alpha \):

$$
V(S_t)\leftarrow V(S_t)+\alpha\Bigl(\delta_t\Bigr),
\qquad
\delta_t = R_{t+1}+\gamma V(S_{t+1})-V(S_t),
$$

where \( \delta_t \) is the TD error.

### Connection to the Bellman expectation equation
For a fixed policy \( \pi \), the Bellman expectation equation says:

$$
v_\pi(s)=\mathbb{E}_\pi\!\left[\,R_{t+1}+\gamma v_\pi(S_{t+1})\mid S_t=s\right].
$$

TD(0) is the sample-based version of this identity: we replace the expectation by the single observed reward \( R_{t+1} \) and replace the unknown \( v_\pi(S_{t+1}) \) with our current estimate \( V(S_{t+1}) \), then update \( V(S_t) \) toward that one-step target. This technique of updating is know as bootstrapping (discussed later in more details).

## MC vs. TD (1): Bias-variance tradeoff

Both Monte Carlo (MC) and Temporal Difference (TD) methods estimate the same object, \( v_\pi \), from experience under a fixed policy \( \pi \). The difference is the learning signal: MC uses a full-return target and updates after the episode ends, while TD updates during the episode using a bootstrapped one-step target.

### Bias-variance trade-off

**Unbiased targets (if we had the truth).**
For MC, the return is an unbiased sample of the value:

$$
\mathbb{E}_\pi\!\left[G_t\mid S_t\right]=v_\pi(S_t).
$$

Meanwhile for TD, the ideal one-step target would also be unbiased:

$$
\mathbb{E}_\pi\!\left[R_{t+1}+\gamma v_\pi(S_{t+1})\mid S_t\right]=v_\pi(S_t).
$$

But, in practice we do not know \( v_\pi \), so TD uses

$$
R_{t+1}+\gamma V(S_{t+1}),
$$

and this is generally a biased target because typically \( V(S_{t+1})\neq v_\pi(S_{t+1}) \).

### Method Summaries MC vs TD
#### Monte Carlo (MC)
- Unbiased target: MC uses the actual return \( G_t \), so

$$
\mathbb{E}_\pi\!\left[G_t\mid S_t=s\right]=v_\pi(s).
$$

- Higher variance: \( G_t \) aggregates many random future steps (actions, transitions, rewards).
- Often stable with approximation: MC looks like supervised learning on observed returns, which is usually numerically stable.
- Less sensitive to initialization: early errors in \( V \) do not affect the target (the target is \( G_t \)).
- Simple mental model: we ``average returns'' to estimate value.

#### Temporal Difference (TD)
- Bootstrapped (biased during learning): TD(0) uses the target

$$
R_{t+1}+\gamma V(S_{t+1}),
$$

which is biased when \( V(S_{t+1})\neq v_\pi(S_{t+1}) \).

- Lower variance: the target depends only on one transition plus a lookup of \( V(S_{t+1}) \).
- More sample-efficient: we update after each step and do not need to wait for termination.
- Tabular convergence: with a fixed \( \pi \) and suitable step sizes, TD(0) converges to \( v_\pi \).
- Can be unstable with approximation: bootstrapping + function approximation can cause divergence in some settings.
- More sensitive to initialization: poor initial \( V \) can propagate through the TD target.

### Convergence to the same value function?
In a stationary tabular setting where each relevant state is visited infinitely often under \( \pi \), MC with sample-mean updates and TD(0) with suitable step-size conditions both converge to the same value function \( v_\pi(s) \). In practice, with finite data, noisy returns, or function approximation, they can yield noticeably different estimates because their targets differ (full returns vs. bootstrapped) and they propagate information differently. An example of this can be seen in the next section.

## MC vs. TD (2): Certainty Equivalence in Batch Learning

So far, we assumed we can keep collecting new experience. With enough data, both MC and TD can converge to the true value:

$$
V(s)\to v_\pi(s)\qquad \text{as experience }\to\infty.
$$

In the batch setting, we instead get a fixed dataset of \( K \) episodes and we must learn only by reusing it:

$$
\mathcal{D}=\{\tau^{(1)},\dots,\tau^{(K)}\},\qquad
\tau^{(k)}=(S^{(k)}_1,A^{(k)}_1,R^{(k)}_2,\dots,S^{(k)}_{T_k}).
$$

We repeatedly sweep through \( \mathcal{D} \) (or sample episodes from it) and apply updates to the same data until the values stop changing.

### A-B example

#### Environment and dataset
We consider an episodic process with \( \gamma=1 \) and two non-terminal states \( A \) and \( B \). Our fixed dataset contains \( 8 \) episodes:

$$
(A,0,B,0),\quad (B,1),\quad (B,1),\quad (B,1),\quad (B,1),\quad (B,1),\quad (B,1),\quad (B,0).
$$

So:

- Episode 1 visits \( A\to B \) and then terminates with reward \( 0 \) from \( B \).
- Episodes 2--7 start at \( B \) and terminate with reward \( 1 \).
- Episode 8 starts at \( B \) and terminates with reward \( 0 \).

#### Batch MC: empirical mean of observed returns
Batch MC converges to the value function that best fits the observed Monte Carlo returns in the dataset, i.e., it treats each sampled return as a supervised target and solves a least-squares regression:

$$
V_{\text{MC}}
\in \arg\min_V \;\sum_{k=1}^{K}\sum_{t=1}^{T_k}\Bigl(G_t^{(k)} - V(s_t^{(k)})\Bigr)^2.
$$

Here the dataset consists of \( K \) recorded episodes (trajectories), indexed by \( k=1,\dots,K \). Episode \( k \) has length \( T_k \) time steps (i.e., it contains states \( s^{(k)}_1,\dots,s^{(k)}_{T_k} \)), so the inner sum ranges over all time indices within episode \( k \) and the outer sum aggregates the squared errors over all time steps in all episodes. In our tabular example, each state value \( V(s) \) is an independent parameter, so the objective separates by state. Group all terms with the same state \( s \):

$$
\sum_{k=1}^{K}\sum_{t=1}^{T_k}\bigl(G_t^{(k)}-V(s_t^{(k)})\bigr)^2
=\sum_{s}\sum_{i=1}^{n(s)}\bigl(G_i(s)-V(s)\bigr)^2 .
$$

Thus we can minimize each stateâ€™s sum of squares independently:

$$
V_{\text{MC}}(s)\in\arg\min_{v}\;\sum_{i=1}^{n(s)}\bigl(G_i(s)-v\bigr)^2 .
$$

Differentiate w.r.t.\ \( v \) and set to zero:

$$
\frac{d}{dv}\sum_{i=1}^{n(s)}(G_i(s)-v)^2
=-2\sum_{i=1}^{n(s)}(G_i(s)-v)=0
\;\Rightarrow\;
v=\frac{1}{n(s)}\sum_{i=1}^{n(s)}G_i(s).
$$

Therefore,

$$
V_{\text{MC}}(s)=\frac{1}{n(s)}\sum_{i=1}^{n(s)} G_i(s).
$$

where \( n(s) \) is the number of occurrences of state \( s \) in the dataset and \( G_i(s) \) are the corresponding sampled returns. For \( A \): \( n(A)=1 \) and the single observed return is \( G_1(A)=0 \), so

$$
V_{\text{MC}}(A)=\frac{1}{1}\bigl(0\bigr)=0.
$$

For \( B \): \( n(B)=8 \) and the observed returns are \( \{1,1,1,1,1,1,0,0\} \), so

$$
V_{\text{MC}}(B)=\frac{1}{8}\bigl(1+1+1+1+1+1+0+0\bigr)
=\frac{6}{8}=0.75.
$$

#### Batch TD(0): fixed point induced by the dataset
Batch TD(0) behaves differently: with a fixed dataset, it effectively treats the data as an empirical MDP and converges to the value function that satisfies the Bellman equations of that estimated model under \( \pi \). In particular, the dataset induces maximum-likelihood one-step estimates

$$
\hat{P}^{a}_{s,s'}=
\frac{\#\{(s_t=s,\ a_t=a,\ s_{t+1}=s')\}}{\#\{(s_t=s,\ a_t=a)\}},
\qquad
\hat{R}^{a}_{s}=
\frac{\sum_{t:\,s_t=s,\ a_t=a} r_{t+1}}{\#\{(s_t=s,\ a_t=a)\}},
$$

and TD(0) converges to the fixed point of the Bellman expectation equation for the estimated MDP
\( \langle \mathcal{S},\mathcal{A},\hat{P},\hat{R},\gamma\rangle \). In our dataset, \( A \) always transitions to \( B \) with reward \( 0 \), so

$$
V_{\text{TD}}(A)=0+\gamma V_{\text{TD}}(B),
$$

and from \( B \) the empirical mean terminal reward is \( 0.75 \) (with terminal value \( 0 \)), hence

$$
V_{\text{TD}}(B)=0.75.
$$

Therefore \( V_{\text{TD}}(A)=0.75 \) when \( \gamma=1 \) (and more generally \( V_{\text{TD}}(A)=\gamma\cdot 0.75 \)).

#### Why the answers differ
With a finite dataset, MC updates \( A \) using only the single observed return following \( A \) (here it happened to be \( 0 \)), so it keeps \( V(A)=0 \), whereas TD bootstraps through \( B \) and propagates the dataset's average outcome at \( B \) back through the observed transition \( A\!\to\!B \), yielding \( V(A)=V(B)=0.75 \). In batch learning (no new data), repeated passes over the same \( K \) episodes drive the methods to different stable solutions: MC converges to empirical mean returns, while TD converges to the fixed point induced by the dataset's empirical transition and reward estimates.

## MC vs. TD (3): The Markov Property

In an MDP, the future is conditionally independent of the past given the current state \( S_t \), and TD(0) exploits this one-step Markov structure by updating from a single transition \( (S_t,R_{t+1},S_{t+1}) \) via

$$
V(S_t)\leftarrow V(S_t)+\alpha\bigl(R_{t+1}+\gamma V(S_{t+1})-V(S_t)\bigr),
$$

which lets information propagate efficiently along observed state-to-state transitions without waiting for complete outcomes. In contrast, MC uses the full return \( G_t \) as a black-box target that bundles the entire future into one number, this can work well, but it does not explicitly leverage the Markov conditional-independence structure and instead learns only from complete episode outcomes. As a rule of thumb, in standard Markov settings TD is often more data-efficient because it aligns with the environment's one-step dynamics, whereas with non-Markov signals (e.g.\ partial observability) MC can sometimes help because the full return may implicitly carry information from the episode history when the current state representation is incomplete.

## Bootstrapping and Sampling

Dynamic Programming, TD, and MC can be compared along two independent axes: bootstrapping (whether the update target uses the current estimate \( V \)) and sampling (whether expectations are approximated from data or computed exactly from a known model). MC is sampled but non-bootstrapped, using the complete return \( G_t \) as its target; TD is sampled and bootstrapped, using the one-step target \( R_{t+1}+\gamma V(S_{t+1}) \); and DP is bootstrapped but not sampled, using a full expected Bellman target such as \( \sum_{s'}P(s'|s,a)\bigl(R(s,a,s')+\gamma V(s')\bigr) \). Equivalently, all three are backups that move \( V(s) \) toward a target: DP performs a full expected backup using the model, while TD and MC perform sample backups using observed experience; TD uses a shallow one-step backup that propagates information via bootstrapping, whereas MC uses a deep backup to episode end with no bootstrapping. Many multi-step methods (e.g., TD(\( \lambda \))) interpolate between TD(0) and MC by trading off backup depth against the amount of bootstrapping.
